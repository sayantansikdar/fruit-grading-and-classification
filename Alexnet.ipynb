{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141,"status":"error","timestamp":1698409850374,"user":{"displayName":"Sayantan Sikdar","userId":"16660529842037260833"},"user_tz":-330},"id":"Uoq5ZeBxQIv0","outputId":"0048a5c3-63a5-48de-edb5-5afd4fc8933c"},"outputs":[{"name":"stdout","output_type":"stream","text":["11\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 54, 54, 96)        34944     \n","                                                                 \n"," activation (Activation)     (None, 54, 54, 96)        0         \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 27, 27, 96)        0         \n"," D)                                                              \n","                                                                 \n"," batch_normalization (Batch  (None, 27, 27, 96)        384       \n"," Normalization)                                                  \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 17, 17, 256)       2973952   \n","                                                                 \n"," activation_1 (Activation)   (None, 17, 17, 256)       0         \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 8, 8, 256)         0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 8, 8, 256)         1024      \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 6, 6, 384)         885120    \n","                                                                 \n"," activation_2 (Activation)   (None, 6, 6, 384)         0         \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 6, 6, 384)         1536      \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 4, 4, 384)         1327488   \n","                                                                 \n"," activation_3 (Activation)   (None, 4, 4, 384)         0         \n","                                                                 \n"," batch_normalization_3 (Bat  (None, 4, 4, 384)         1536      \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 2, 2, 256)         884992    \n","                                                                 \n"," activation_4 (Activation)   (None, 2, 2, 256)         0         \n","                                                                 \n"," max_pooling2d_2 (MaxPoolin  (None, 1, 1, 256)         0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_4 (Bat  (None, 1, 1, 256)         1024      \n"," chNormalization)                                                \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 4096)              1052672   \n","                                                                 \n"," activation_5 (Activation)   (None, 4096)              0         \n","                                                                 \n"," dropout (Dropout)           (None, 4096)              0         \n","                                                                 \n"," batch_normalization_5 (Bat  (None, 4096)              16384     \n"," chNormalization)                                                \n","                                                                 \n"," dense_1 (Dense)             (None, 4096)              16781312  \n","                                                                 \n"," activation_6 (Activation)   (None, 4096)              0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 4096)              0         \n","                                                                 \n"," batch_normalization_6 (Bat  (None, 4096)              16384     \n"," chNormalization)                                                \n","                                                                 \n"," dense_2 (Dense)             (None, 1000)              4097000   \n","                                                                 \n"," activation_7 (Activation)   (None, 1000)              0         \n","                                                                 \n"," dropout_2 (Dropout)         (None, 1000)              0         \n","                                                                 \n"," batch_normalization_7 (Bat  (None, 1000)              4000      \n"," chNormalization)                                                \n","                                                                 \n"," dense_3 (Dense)             (None, 100)               100100    \n","                                                                 \n"," activation_8 (Activation)   (None, 100)               0         \n","                                                                 \n"," dropout_3 (Dropout)         (None, 100)               0         \n","                                                                 \n"," batch_normalization_8 (Bat  (None, 100)               400       \n"," chNormalization)                                                \n","                                                                 \n"," dense_4 (Dense)             (None, 12)                1212      \n","                                                                 \n"," activation_9 (Activation)   (None, 12)                0         \n","                                                                 \n","=================================================================\n","Total params: 28181464 (107.50 MB)\n","Trainable params: 28160128 (107.42 MB)\n","Non-trainable params: 21336 (83.34 KB)\n","_________________________________________________________________\n","Epoch 1/50\n","787/787 [==============================] - ETA: 0s - loss: 2.0118 - accuracy: 0.3642\n","Epoch 1: val_accuracy improved from -inf to 0.42929, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n"]},{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["787/787 [==============================] - 522s 661ms/step - loss: 2.0118 - accuracy: 0.3642 - val_loss: 1.6549 - val_accuracy: 0.4293\n","Epoch 2/50\n","787/787 [==============================] - ETA: 0s - loss: 1.6748 - accuracy: 0.4224\n","Epoch 2: val_accuracy improved from 0.42929 to 0.50168, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 518s 658ms/step - loss: 1.6748 - accuracy: 0.4224 - val_loss: 1.4350 - val_accuracy: 0.5017\n","Epoch 3/50\n","787/787 [==============================] - ETA: 0s - loss: 1.5413 - accuracy: 0.4704\n","Epoch 3: val_accuracy improved from 0.50168 to 0.62346, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 531s 674ms/step - loss: 1.5413 - accuracy: 0.4704 - val_loss: 1.1675 - val_accuracy: 0.6235\n","Epoch 4/50\n","787/787 [==============================] - ETA: 0s - loss: 1.4508 - accuracy: 0.5101\n","Epoch 4: val_accuracy improved from 0.62346 to 0.63861, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 515s 654ms/step - loss: 1.4508 - accuracy: 0.5101 - val_loss: 1.1726 - val_accuracy: 0.6386\n","Epoch 5/50\n","787/787 [==============================] - ETA: 0s - loss: 1.3884 - accuracy: 0.5284\n","Epoch 5: val_accuracy improved from 0.63861 to 0.65937, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 514s 653ms/step - loss: 1.3884 - accuracy: 0.5284 - val_loss: 1.1055 - val_accuracy: 0.6594\n","Epoch 6/50\n","787/787 [==============================] - ETA: 0s - loss: 1.2966 - accuracy: 0.5682\n","Epoch 6: val_accuracy did not improve from 0.65937\n","787/787 [==============================] - 484s 614ms/step - loss: 1.2966 - accuracy: 0.5682 - val_loss: 1.1744 - val_accuracy: 0.6347\n","Epoch 7/50\n","787/787 [==============================] - ETA: 0s - loss: 1.3115 - accuracy: 0.5672\n","Epoch 7: val_accuracy did not improve from 0.65937\n","787/787 [==============================] - 521s 663ms/step - loss: 1.3115 - accuracy: 0.5672 - val_loss: 1.6550 - val_accuracy: 0.5842\n","Epoch 8/50\n","787/787 [==============================] - ETA: 0s - loss: 1.3146 - accuracy: 0.5552\n","Epoch 8: val_accuracy did not improve from 0.65937\n","787/787 [==============================] - 516s 656ms/step - loss: 1.3146 - accuracy: 0.5552 - val_loss: 1.1679 - val_accuracy: 0.6358\n","Epoch 9/50\n","787/787 [==============================] - ETA: 0s - loss: 1.4247 - accuracy: 0.5320\n","Epoch 9: val_accuracy improved from 0.65937 to 0.74299, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 516s 655ms/step - loss: 1.4247 - accuracy: 0.5320 - val_loss: 0.8324 - val_accuracy: 0.7430\n","Epoch 10/50\n","787/787 [==============================] - ETA: 0s - loss: 1.1941 - accuracy: 0.6037\n","Epoch 10: val_accuracy improved from 0.74299 to 0.75084, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 516s 656ms/step - loss: 1.1941 - accuracy: 0.6037 - val_loss: 0.7749 - val_accuracy: 0.7508\n","Epoch 11/50\n","787/787 [==============================] - ETA: 0s - loss: 1.0923 - accuracy: 0.6400\n","Epoch 11: val_accuracy did not improve from 0.75084\n","787/787 [==============================] - 508s 645ms/step - loss: 1.0923 - accuracy: 0.6400 - val_loss: 0.7626 - val_accuracy: 0.7435\n","Epoch 12/50\n","787/787 [==============================] - ETA: 0s - loss: 1.0313 - accuracy: 0.6648\n","Epoch 12: val_accuracy improved from 0.75084 to 0.78507, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 510s 648ms/step - loss: 1.0313 - accuracy: 0.6648 - val_loss: 0.6860 - val_accuracy: 0.7851\n","Epoch 13/50\n","787/787 [==============================] - ETA: 0s - loss: 1.1451 - accuracy: 0.6306\n","Epoch 13: val_accuracy did not improve from 0.78507\n","787/787 [==============================] - 519s 660ms/step - loss: 1.1451 - accuracy: 0.6306 - val_loss: 0.8991 - val_accuracy: 0.7329\n","Epoch 14/50\n","787/787 [==============================] - ETA: 0s - loss: 1.0075 - accuracy: 0.6757\n","Epoch 14: val_accuracy did not improve from 0.78507\n","787/787 [==============================] - 518s 658ms/step - loss: 1.0075 - accuracy: 0.6757 - val_loss: 0.8333 - val_accuracy: 0.7340\n","Epoch 15/50\n","787/787 [==============================] - ETA: 0s - loss: 1.0106 - accuracy: 0.6797\n","Epoch 15: val_accuracy did not improve from 0.78507\n","787/787 [==============================] - 558s 709ms/step - loss: 1.0106 - accuracy: 0.6797 - val_loss: 0.9400 - val_accuracy: 0.7413\n","Epoch 16/50\n","787/787 [==============================] - ETA: 0s - loss: 0.8799 - accuracy: 0.7231\n","Epoch 16: val_accuracy improved from 0.78507 to 0.83726, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 529s 672ms/step - loss: 0.8799 - accuracy: 0.7231 - val_loss: 0.5455 - val_accuracy: 0.8373\n","Epoch 17/50\n","787/787 [==============================] - ETA: 0s - loss: 0.9256 - accuracy: 0.7146\n","Epoch 17: val_accuracy did not improve from 0.83726\n","787/787 [==============================] - 542s 688ms/step - loss: 0.9256 - accuracy: 0.7146 - val_loss: 0.8079 - val_accuracy: 0.7452\n","Epoch 18/50\n","787/787 [==============================] - ETA: 0s - loss: 0.9065 - accuracy: 0.7177\n","Epoch 18: val_accuracy improved from 0.83726 to 0.85634, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 542s 688ms/step - loss: 0.9065 - accuracy: 0.7177 - val_loss: 0.5024 - val_accuracy: 0.8563\n","Epoch 19/50\n","787/787 [==============================] - ETA: 0s - loss: 0.7710 - accuracy: 0.7600\n","Epoch 19: val_accuracy improved from 0.85634 to 0.88047, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 531s 674ms/step - loss: 0.7710 - accuracy: 0.7600 - val_loss: 0.4236 - val_accuracy: 0.8805\n","Epoch 20/50\n","787/787 [==============================] - ETA: 0s - loss: 0.8217 - accuracy: 0.7445\n","Epoch 20: val_accuracy did not improve from 0.88047\n","787/787 [==============================] - 547s 696ms/step - loss: 0.8217 - accuracy: 0.7445 - val_loss: 0.5858 - val_accuracy: 0.8462\n","Epoch 21/50\n","787/787 [==============================] - ETA: 0s - loss: 0.8951 - accuracy: 0.7245\n","Epoch 21: val_accuracy did not improve from 0.88047\n","787/787 [==============================] - 600s 762ms/step - loss: 0.8951 - accuracy: 0.7245 - val_loss: 1.7146 - val_accuracy: 0.6072\n","Epoch 22/50\n","787/787 [==============================] - ETA: 0s - loss: 0.7560 - accuracy: 0.7723\n","Epoch 22: val_accuracy did not improve from 0.88047\n","787/787 [==============================] - 572s 727ms/step - loss: 0.7560 - accuracy: 0.7723 - val_loss: 0.5069 - val_accuracy: 0.8608\n","Epoch 23/50\n","787/787 [==============================] - ETA: 0s - loss: 0.7048 - accuracy: 0.7906\n","Epoch 23: val_accuracy did not improve from 0.88047\n","787/787 [==============================] - 523s 664ms/step - loss: 0.7048 - accuracy: 0.7906 - val_loss: 0.6124 - val_accuracy: 0.8294\n","Epoch 24/50\n","787/787 [==============================] - ETA: 0s - loss: 0.7921 - accuracy: 0.7563\n","Epoch 24: val_accuracy did not improve from 0.88047\n","787/787 [==============================] - 525s 667ms/step - loss: 0.7921 - accuracy: 0.7563 - val_loss: 1.4673 - val_accuracy: 0.7211\n","Epoch 25/50\n","787/787 [==============================] - ETA: 0s - loss: 0.6300 - accuracy: 0.8045\n","Epoch 25: val_accuracy improved from 0.88047 to 0.90348, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 535s 680ms/step - loss: 0.6300 - accuracy: 0.8045 - val_loss: 0.3509 - val_accuracy: 0.9035\n","Epoch 26/50\n","787/787 [==============================] - ETA: 0s - loss: 0.6245 - accuracy: 0.8185\n","Epoch 26: val_accuracy did not improve from 0.90348\n","787/787 [==============================] - 491s 623ms/step - loss: 0.6245 - accuracy: 0.8185 - val_loss: 0.8296 - val_accuracy: 0.7800\n","Epoch 27/50\n","787/787 [==============================] - ETA: 0s - loss: 0.5837 - accuracy: 0.8288\n","Epoch 27: val_accuracy improved from 0.90348 to 0.91975, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 516s 655ms/step - loss: 0.5837 - accuracy: 0.8288 - val_loss: 0.3084 - val_accuracy: 0.9198\n","Epoch 28/50\n","787/787 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.8328\n","Epoch 28: val_accuracy did not improve from 0.91975\n","787/787 [==============================] - 482s 613ms/step - loss: 0.5607 - accuracy: 0.8328 - val_loss: 0.3438 - val_accuracy: 0.8984\n","Epoch 29/50\n","787/787 [==============================] - ETA: 0s - loss: 0.5622 - accuracy: 0.8335\n","Epoch 29: val_accuracy did not improve from 0.91975\n","787/787 [==============================] - 481s 611ms/step - loss: 0.5622 - accuracy: 0.8335 - val_loss: 0.3090 - val_accuracy: 0.9164\n","Epoch 30/50\n","787/787 [==============================] - ETA: 0s - loss: 0.5314 - accuracy: 0.8444\n","Epoch 30: val_accuracy did not improve from 0.91975\n","787/787 [==============================] - 485s 617ms/step - loss: 0.5314 - accuracy: 0.8444 - val_loss: 0.2872 - val_accuracy: 0.9141\n","Epoch 31/50\n","787/787 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.8417\n","Epoch 31: val_accuracy did not improve from 0.91975\n","787/787 [==============================] - 514s 653ms/step - loss: 0.5467 - accuracy: 0.8417 - val_loss: 0.3090 - val_accuracy: 0.9141\n","Epoch 32/50\n","787/787 [==============================] - ETA: 0s - loss: 0.5451 - accuracy: 0.8370\n","Epoch 32: val_accuracy did not improve from 0.91975\n","787/787 [==============================] - 513s 652ms/step - loss: 0.5451 - accuracy: 0.8370 - val_loss: 0.3684 - val_accuracy: 0.9147\n","Epoch 33/50\n","787/787 [==============================] - ETA: 0s - loss: 0.4264 - accuracy: 0.8701\n","Epoch 33: val_accuracy did not improve from 0.91975\n","787/787 [==============================] - 513s 652ms/step - loss: 0.4264 - accuracy: 0.8701 - val_loss: 0.3099 - val_accuracy: 0.9175\n","Epoch 34/50\n","787/787 [==============================] - ETA: 0s - loss: 0.4876 - accuracy: 0.8595\n","Epoch 34: val_accuracy did not improve from 0.91975\n","787/787 [==============================] - 516s 656ms/step - loss: 0.4876 - accuracy: 0.8595 - val_loss: 0.2849 - val_accuracy: 0.9181\n","Epoch 35/50\n","787/787 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.8603\n","Epoch 35: val_accuracy did not improve from 0.91975\n","787/787 [==============================] - 528s 671ms/step - loss: 0.4830 - accuracy: 0.8603 - val_loss: 0.4031 - val_accuracy: 0.9040\n","Epoch 36/50\n","787/787 [==============================] - ETA: 0s - loss: 0.4334 - accuracy: 0.8753\n","Epoch 36: val_accuracy improved from 0.91975 to 0.93996, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 519s 659ms/step - loss: 0.4334 - accuracy: 0.8753 - val_loss: 0.2471 - val_accuracy: 0.9400\n","Epoch 37/50\n","787/787 [==============================] - ETA: 0s - loss: 0.4071 - accuracy: 0.8906\n","Epoch 37: val_accuracy did not improve from 0.93996\n","787/787 [==============================] - 528s 670ms/step - loss: 0.4071 - accuracy: 0.8906 - val_loss: 0.4170 - val_accuracy: 0.9024\n","Epoch 38/50\n","787/787 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.8950\n","Epoch 38: val_accuracy did not improve from 0.93996\n","787/787 [==============================] - 513s 652ms/step - loss: 0.3765 - accuracy: 0.8950 - val_loss: 0.2161 - val_accuracy: 0.9349\n","Epoch 39/50\n","787/787 [==============================] - ETA: 0s - loss: 0.4013 - accuracy: 0.8855\n","Epoch 39: val_accuracy improved from 0.93996 to 0.95062, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch50.h5\n","787/787 [==============================] - 520s 661ms/step - loss: 0.4013 - accuracy: 0.8855 - val_loss: 0.2069 - val_accuracy: 0.9506\n","Epoch 40/50\n","787/787 [==============================] - ETA: 0s - loss: 0.3260 - accuracy: 0.9097\n","Epoch 40: val_accuracy did not improve from 0.95062\n","787/787 [==============================] - 523s 665ms/step - loss: 0.3260 - accuracy: 0.9097 - val_loss: 0.2059 - val_accuracy: 0.9450\n","Epoch 41/50\n","787/787 [==============================] - ETA: 0s - loss: 0.4246 - accuracy: 0.8804\n","Epoch 41: val_accuracy did not improve from 0.95062\n","787/787 [==============================] - 509s 647ms/step - loss: 0.4246 - accuracy: 0.8804 - val_loss: 0.4182 - val_accuracy: 0.9057\n","Epoch 42/50\n","787/787 [==============================] - ETA: 0s - loss: 0.4307 - accuracy: 0.8777\n","Epoch 42: val_accuracy did not improve from 0.95062\n","787/787 [==============================] - 534s 679ms/step - loss: 0.4307 - accuracy: 0.8777 - val_loss: 0.3620 - val_accuracy: 0.9141\n","Epoch 43/50\n","787/787 [==============================] - ETA: 0s - loss: 0.3419 - accuracy: 0.9064\n","Epoch 43: val_accuracy did not improve from 0.95062\n","787/787 [==============================] - 526s 668ms/step - loss: 0.3419 - accuracy: 0.9064 - val_loss: 0.2993 - val_accuracy: 0.9254\n","Epoch 44/50\n","787/787 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.9159\n","Epoch 44: val_accuracy did not improve from 0.95062\n","787/787 [==============================] - 538s 683ms/step - loss: 0.3126 - accuracy: 0.9159 - val_loss: 0.2647 - val_accuracy: 0.9355\n","Epoch 45/50\n","787/787 [==============================] - ETA: 0s - loss: 0.3273 - accuracy: 0.9117\n","Epoch 45: val_accuracy did not improve from 0.95062\n","787/787 [==============================] - 547s 695ms/step - loss: 0.3273 - accuracy: 0.9117 - val_loss: 0.2614 - val_accuracy: 0.9265\n","Epoch 46/50\n","787/787 [==============================] - ETA: 0s - loss: 0.3927 - accuracy: 0.8867\n","Epoch 46: val_accuracy did not improve from 0.95062\n","787/787 [==============================] - 564s 716ms/step - loss: 0.3927 - accuracy: 0.8867 - val_loss: 0.3673 - val_accuracy: 0.9063\n","Epoch 47/50\n","787/787 [==============================] - ETA: 0s - loss: 0.2947 - accuracy: 0.9186\n","Epoch 47: val_accuracy did not improve from 0.95062\n","787/787 [==============================] - 546s 694ms/step - loss: 0.2947 - accuracy: 0.9186 - val_loss: 0.2023 - val_accuracy: 0.9478\n","Epoch 48/50\n","787/787 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.9015\n","Epoch 48: val_accuracy did not improve from 0.95062\n","787/787 [==============================] - 505s 642ms/step - loss: 0.3503 - accuracy: 0.9015 - val_loss: 0.2883 - val_accuracy: 0.9327\n","Epoch 49/50\n","787/787 [==============================] - ETA: 0s - loss: 0.3584 - accuracy: 0.8972\n","Epoch 49: val_accuracy did not improve from 0.95062\n","787/787 [==============================] - 494s 628ms/step - loss: 0.3584 - accuracy: 0.8972 - val_loss: 0.6631 - val_accuracy: 0.9248\n","Epoch 50/50\n","787/787 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.9241\n","Epoch 50: val_accuracy did not improve from 0.95062\n","787/787 [==============================] - 494s 628ms/step - loss: 0.2721 - accuracy: 0.9241 - val_loss: 0.6182 - val_accuracy: 0.9422\n","66/66 [==============================] - 30s 449ms/step\n","[[9.9999714e-01 3.0334771e-08 1.9194655e-12 ... 7.4195872e-10\n","  2.8533514e-06 3.1696763e-09]\n"," [3.4464759e-19 1.2524117e-11 1.8128230e-18 ... 3.8356745e-11\n","  3.6750530e-14 1.5474657e-38]\n"," [6.9260541e-03 4.5906560e-04 7.7230281e-05 ... 1.1701200e-03\n","  2.7136155e-04 9.6670526e-01]\n"," ...\n"," [9.9999499e-01 6.2951138e-08 1.0114820e-14 ... 2.8682219e-08\n","  4.8793545e-06 2.3314487e-11]\n"," [4.8386064e-04 1.2478080e-02 2.7647393e-04 ... 2.5963914e-03\n","  3.7053307e-03 9.5041460e-01]\n"," [1.2760191e-07 2.7765432e-06 5.5365792e-08 ... 9.3291890e-14\n","  9.9999714e-01 0.0000000e+00]]\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Alexnet_code.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1ST_yrafE2p_4JHcbvSpBBYMW1lWIw5hD\n","\"\"\"\n","\n","from __future__ import print_function\n","import keras\n","import pandas as pd\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import os\n","import time\n","from openpyxl import load_workbook\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n","from keras.layers import BatchNormalization\n","from PIL import Image\n","import tensorflow as tf\n","\n","import xlsxwriter\n","import cv2\n","from random import shuffle\n","\n","\n","\n","\n","start1= time.time()\n","\n","# Training parameters\n","batch_size = 10  # orig paper trained all networks with batch_size=128\n","epochs = 50\n","data_augmentation = False\n","num_classes = 12\n","test_img_num =2083\n","result_excel_link = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Result/fruit_batch10_epoch50.xlsx'\n","Train_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TRAIN'\n","TEST_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TEST'\n","label_link= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL_encoded_file.xlsx'\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 2\n","n = 2\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'AlexNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","##(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","\n","\n","def label_img(name):\n","    # print(name)\n","    filename1 = os.fsdecode(name)\n","    # print(filename1)\n","    s=[]\n","    s= filename1.split('_')\n","    # print(s)\n","    # print(int(s[0]))\n","    #word_label = name.split('-')[0]\n","    #if word_label == 'golden_retriever': return np.array([1, 0])\n","    #elif word_label == 'shetland_sheepdog' : return np.array([0, 1])\n","    wb = load_workbook(label_link)\n","    ws = wb.active\n","    #for cell in ws.columns[1]:  #here column 9 is value is what i am testing which is Column X of my example.\n","    for x in range(2, 18454):\n","        #for y in range(1,):\n","        cell= ws.cell(row=x, column=1)\n","        # print(cell.value)\n","        # print(cell.value)\n","        if cell.value == int(s[0]) :\n","               #print ws.cell(column=12).value\n","                #a= ws.columns(row).value\n","                #b= ws.columns(col).value\n","                cell2= ws.cell(row=x, column=2)\n","\n","                label1= cell2.value\n","                # print(label1)\n","                break\n","        #else:\n","                #print('hi')\n","    return label1\n","\n","def load_training_data():\n","    train_data = []\n","    for img in os.listdir(Train_DIR):\n","        #print(img)\n","        temp2 = img.replace('.png','')\n","        label = label_img(temp2)\n","        #print(label)\n","        path = os.path.join(Train_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        # print(path)\n","        # print(label)\n","        img2= cv2.imread(path)\n","        # cv2_imshow(img2)\n","        #img = img.convert('L')\n","        # img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        #path3= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/training_441_segmented_polar_A1_A2_A4'\n","        resized_img1 = cv2.resize(img2, (224,224), interpolation = cv2.INTER_AREA)\n","        #cv2.imwrite(path3+ '/' + temp2 , resized_img1 )\n","        train_data.append([np.array(resized_img1), label])\n","\n","    shuffle(train_data)\n","    return train_data\n","\n","\n","train_data = load_training_data()\n","\n","# s = [i[1] for i in train_data]\n","# print(train_data)\n","\n","# df2 = pd.DataFrame(np.array(df2),columns=['Label'])\n","# df2['Label'] = pd.Categorical(df2.Label)\n","IMG_SIZE = 224\n","# trainImages = np.array([i[0] for i in train_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","trainImages = np.array([i[0] for i in train_data])\n","# df = pd.read_csv()\n","# trainLabels = tf.convert_to_tensor(np.array([i[1] for i in train_data]).astype(np.int_))\n","trainLabels = np.array([i[1] for i in train_data])\n","print(trainLabels[2]) #successful till here\n","#print(trainLabels)\n","# trainLabels = to_categorical(trainLabels)                                                          #problem here\n","trainLabels = keras.utils.to_categorical(trainLabels, num_classes)\n","print(trainLabels[2])\n","\n","\n","\n","def load_test_data():\n","    test_data = []\n","    for img in os.listdir(TEST_DIR):\n","        temp1 = img.replace('.png','')\n","        # print(temp1)\n","        label = label_img(temp1)\n","        path = os.path.join(TEST_DIR, img)\n","        # temp1 = img\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img1= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        resized_img2 = cv2.resize(img1, (224,224), interpolation = cv2.INTER_AREA)\n","        #path2= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/testing folder_segmented_resized'\n","        #cv2.imwrite(path2+ '/' + temp1 , resized_img2 )\n","        test_data.append([np.array(resized_img2), label,temp1])\n","        # print(label)\n","\n","    shuffle(test_data)\n","    return test_data\n","\n","\n","test_data = load_test_data()\n","#plt.imshow(test_data[10][0], cmap = 'gist_gray')\n","\n","\n","# # convert it to tensorflow              ########################\n","# tensor1 = tf.convert_to_tensor(numpy_array)\n","# print(tensor1)\n","\n","# testImages = np.array([i[0] for i in test_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","# testLabels = tf.convert_to_tensor(np.array([i[1] for i in test_data]).astype(np.int_))\n","\n","testImages = np.array([i[0] for i in test_data])\n","testLabels = np.array([i[1] for i in test_data])\n","test_img_name = np.array([i[2] for i in test_data])\n","\n","\n","#testLabels = keras.utils.to_categorical(testLabels, num_classes)\n","\n","# Input image dimensions.\n","input_shape = trainImages.shape[1:]\n","\n","# Normalize data.\n","##trainImages = trainImages.astype('float32') / 255\n","##testImages = testImages.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","##if subtract_pixel_mean:\n","    ##trainImages_mean = np.mean(trainImages, axis=0)\n","    ##trainImages -= trainImages_mean\n","    ##testImages -= trainImages_mean\n","\n","#print('x_train shape:', x_train.shape)\n","#print(x_train.shape[0], 'train samples')\n","#print(x_test.shape[0], 'test samples')\n","#print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","#y_train = keras.utils.to_categorical(y_train, num_classes)\n","#y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","model = Sequential()\n","\n","# 1st Convolutional Layer\n","model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),strides=(4,4), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation before passing it to the next layer\n","model.add(BatchNormalization())\n","\n","# 2nd Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 3rd Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 5th Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# Passing it to a dense layer\n","model.add(Flatten())\n","# 1st Dense Layer\n","model.add(Dense(4096, input_shape=(224*224*3,)))\n","model.add(Activation('relu'))\n","# Add Dropout to prevent overfitting\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","# 2nd Dense Layer\n","model.add(Dense(4096))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","# 3rd Dense Layer\n","model.add(Dense(1000))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","# 4th Dense Layer\n","model.add(Dense(100))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","# Output Layer\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))\n","\n","model.summary()\n","model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","\n","# Prepare model model saving directory.\n","save_dir = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model'\n","model_name = 'Alexnet_model_fNet_batch10_epoch50.h5'\n","saved_model= save_dir + '/' + model_name\n","\n","# saved_model= save_dir + '/' + model_name\n","# saved_model_final = load_model(saved_model)\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200) #EPOCH CHANGING HERE\n","mc = ModelCheckpoint(saved_model , monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","\n","# print(type(trainImages))\n","# print(type(trainLabels))\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainLabels= np.asarray(trainLabels).astype(np.float32)\n","model.fit(trainImages, trainLabels, batch_size= batch_size, epochs = epochs, verbose=1, validation_split=0.1846, shuffle=True, callbacks=[es, mc])\n","\n","saved_model_final = load_model(saved_model)\n","\n","\n","# Score trained model.\n","preds = saved_model_final.predict(testImages)\n","print(preds)\n","\n","end1= time.time()\n","time_taken= end1- start1\n","\n","workbook = xlsxwriter.Workbook(result_excel_link)\n","worksheet = workbook.add_worksheet()\n","\n","for i in range(0,test_img_num):\n","        #wb1 = load_workbook('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        #ws1 = wb1.active\n","        #a = ws1.cell(row= i+1, column=1)\n","        #a.value = testLabels[i]\n","        #b = ws1.cell(row= i+1, column=2)\n","        #b.value = np.argmax(preds [i])\n","        #ws1.save('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        name = test_img_name[i]\n","        worksheet.write(i+1, 0 , name)\n","        a = testLabels[i]\n","        worksheet.write(i+1, 1 , a)\n","        b = np.argmax(preds[i])\n","        worksheet.write(i+1, 2 , b)\n","        if a==b:\n","             worksheet.write(i+1, 3 , 1)\n","        else:\n","             worksheet.write(i+1, 3 , 0)\n","\n","# worksheet.write(1, 4 , time_taken)\n","\n","workbook.close()\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4\n","[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_5 (Conv2D)           (None, 54, 54, 96)        34944     \n","                                                                 \n"," activation_10 (Activation)  (None, 54, 54, 96)        0         \n","                                                                 \n"," max_pooling2d_3 (MaxPoolin  (None, 27, 27, 96)        0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_9 (Bat  (None, 27, 27, 96)        384       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 17, 17, 256)       2973952   \n","                                                                 \n"," activation_11 (Activation)  (None, 17, 17, 256)       0         \n","                                                                 \n"," max_pooling2d_4 (MaxPoolin  (None, 8, 8, 256)         0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_10 (Ba  (None, 8, 8, 256)         1024      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 6, 6, 384)         885120    \n","                                                                 \n"," activation_12 (Activation)  (None, 6, 6, 384)         0         \n","                                                                 \n"," batch_normalization_11 (Ba  (None, 6, 6, 384)         1536      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 4, 4, 384)         1327488   \n","                                                                 \n"," activation_13 (Activation)  (None, 4, 4, 384)         0         \n","                                                                 \n"," batch_normalization_12 (Ba  (None, 4, 4, 384)         1536      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 2, 2, 256)         884992    \n","                                                                 \n"," activation_14 (Activation)  (None, 2, 2, 256)         0         \n","                                                                 \n"," max_pooling2d_5 (MaxPoolin  (None, 1, 1, 256)         0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_13 (Ba  (None, 1, 1, 256)         1024      \n"," tchNormalization)                                               \n","                                                                 \n"," flatten_1 (Flatten)         (None, 256)               0         \n","                                                                 \n"," dense_5 (Dense)             (None, 4096)              1052672   \n","                                                                 \n"," activation_15 (Activation)  (None, 4096)              0         \n","                                                                 \n"," dropout_4 (Dropout)         (None, 4096)              0         \n","                                                                 \n"," batch_normalization_14 (Ba  (None, 4096)              16384     \n"," tchNormalization)                                               \n","                                                                 \n"," dense_6 (Dense)             (None, 4096)              16781312  \n","                                                                 \n"," activation_16 (Activation)  (None, 4096)              0         \n","                                                                 \n"," dropout_5 (Dropout)         (None, 4096)              0         \n","                                                                 \n"," batch_normalization_15 (Ba  (None, 4096)              16384     \n"," tchNormalization)                                               \n","                                                                 \n"," dense_7 (Dense)             (None, 1000)              4097000   \n","                                                                 \n"," activation_17 (Activation)  (None, 1000)              0         \n","                                                                 \n"," dropout_6 (Dropout)         (None, 1000)              0         \n","                                                                 \n"," batch_normalization_16 (Ba  (None, 1000)              4000      \n"," tchNormalization)                                               \n","                                                                 \n"," dense_8 (Dense)             (None, 100)               100100    \n","                                                                 \n"," activation_18 (Activation)  (None, 100)               0         \n","                                                                 \n"," dropout_7 (Dropout)         (None, 100)               0         \n","                                                                 \n"," batch_normalization_17 (Ba  (None, 100)               400       \n"," tchNormalization)                                               \n","                                                                 \n"," dense_9 (Dense)             (None, 12)                1212      \n","                                                                 \n"," activation_19 (Activation)  (None, 12)                0         \n","                                                                 \n","=================================================================\n","Total params: 28181464 (107.50 MB)\n","Trainable params: 28160128 (107.42 MB)\n","Non-trainable params: 21336 (83.34 KB)\n","_________________________________________________________________\n","Epoch 1/75\n","787/787 [==============================] - ETA: 0s - loss: 1.9211 - accuracy: 0.3907\n","Epoch 1: val_accuracy improved from -inf to 0.53367, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 525s 665ms/step - loss: 1.9211 - accuracy: 0.3907 - val_loss: 1.4363 - val_accuracy: 0.5337\n","Epoch 2/75\n","787/787 [==============================] - ETA: 0s - loss: 1.4960 - accuracy: 0.5011\n","Epoch 2: val_accuracy improved from 0.53367 to 0.62346, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 529s 672ms/step - loss: 1.4960 - accuracy: 0.5011 - val_loss: 1.2781 - val_accuracy: 0.6235\n","Epoch 3/75\n","787/787 [==============================] - ETA: 0s - loss: 1.5414 - accuracy: 0.4880\n","Epoch 3: val_accuracy did not improve from 0.62346\n","787/787 [==============================] - 516s 656ms/step - loss: 1.5414 - accuracy: 0.4880 - val_loss: 1.2038 - val_accuracy: 0.5976\n","Epoch 4/75\n","787/787 [==============================] - ETA: 0s - loss: 1.3695 - accuracy: 0.5416\n","Epoch 4: val_accuracy did not improve from 0.62346\n","787/787 [==============================] - 523s 664ms/step - loss: 1.3695 - accuracy: 0.5416 - val_loss: 1.5875 - val_accuracy: 0.5561\n","Epoch 5/75\n","787/787 [==============================] - ETA: 0s - loss: 1.2928 - accuracy: 0.5752\n","Epoch 5: val_accuracy improved from 0.62346 to 0.64198, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 524s 665ms/step - loss: 1.2928 - accuracy: 0.5752 - val_loss: 1.1239 - val_accuracy: 0.6420\n","Epoch 6/75\n","787/787 [==============================] - ETA: 0s - loss: 1.2099 - accuracy: 0.6005\n","Epoch 6: val_accuracy did not improve from 0.64198\n","787/787 [==============================] - 518s 658ms/step - loss: 1.2099 - accuracy: 0.6005 - val_loss: 3.1835 - val_accuracy: 0.3670\n","Epoch 7/75\n","787/787 [==============================] - ETA: 0s - loss: 1.1866 - accuracy: 0.6070\n","Epoch 7: val_accuracy did not improve from 0.64198\n","787/787 [==============================] - 526s 669ms/step - loss: 1.1866 - accuracy: 0.6070 - val_loss: 1.7704 - val_accuracy: 0.5163\n","Epoch 8/75\n","787/787 [==============================] - ETA: 0s - loss: 1.2741 - accuracy: 0.5786\n","Epoch 8: val_accuracy improved from 0.64198 to 0.71437, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 515s 654ms/step - loss: 1.2741 - accuracy: 0.5786 - val_loss: 0.9194 - val_accuracy: 0.7144\n","Epoch 9/75\n","787/787 [==============================] - ETA: 0s - loss: 1.1717 - accuracy: 0.6096\n","Epoch 9: val_accuracy improved from 0.71437 to 0.73793, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 512s 650ms/step - loss: 1.1717 - accuracy: 0.6096 - val_loss: 0.8619 - val_accuracy: 0.7379\n","Epoch 10/75\n","787/787 [==============================] - ETA: 0s - loss: 1.1501 - accuracy: 0.6240\n","Epoch 10: val_accuracy did not improve from 0.73793\n","787/787 [==============================] - 475s 603ms/step - loss: 1.1501 - accuracy: 0.6240 - val_loss: 1.5894 - val_accuracy: 0.5505\n","Epoch 11/75\n","787/787 [==============================] - ETA: 0s - loss: 1.1215 - accuracy: 0.6337\n","Epoch 11: val_accuracy did not improve from 0.73793\n","787/787 [==============================] - 508s 645ms/step - loss: 1.1215 - accuracy: 0.6337 - val_loss: 1.4784 - val_accuracy: 0.6016\n","Epoch 12/75\n","787/787 [==============================] - ETA: 0s - loss: 1.0210 - accuracy: 0.6732\n","Epoch 12: val_accuracy improved from 0.73793 to 0.80471, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 515s 654ms/step - loss: 1.0210 - accuracy: 0.6732 - val_loss: 0.6961 - val_accuracy: 0.8047\n","Epoch 13/75\n","787/787 [==============================] - ETA: 0s - loss: 0.9649 - accuracy: 0.6954\n","Epoch 13: val_accuracy improved from 0.80471 to 0.83558, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 490s 623ms/step - loss: 0.9649 - accuracy: 0.6954 - val_loss: 0.5802 - val_accuracy: 0.8356\n","Epoch 14/75\n","787/787 [==============================] - ETA: 0s - loss: 0.9579 - accuracy: 0.7045\n","Epoch 14: val_accuracy did not improve from 0.83558\n","787/787 [==============================] - 514s 654ms/step - loss: 0.9579 - accuracy: 0.7045 - val_loss: 0.8631 - val_accuracy: 0.7671\n","Epoch 15/75\n","787/787 [==============================] - ETA: 0s - loss: 0.9431 - accuracy: 0.7027\n","Epoch 15: val_accuracy did not improve from 0.83558\n","787/787 [==============================] - 519s 660ms/step - loss: 0.9431 - accuracy: 0.7027 - val_loss: 0.6200 - val_accuracy: 0.7924\n","Epoch 16/75\n","787/787 [==============================] - ETA: 0s - loss: 0.7829 - accuracy: 0.7573\n","Epoch 16: val_accuracy improved from 0.83558 to 0.84287, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 517s 657ms/step - loss: 0.7829 - accuracy: 0.7573 - val_loss: 0.5849 - val_accuracy: 0.8429\n","Epoch 17/75\n","787/787 [==============================] - ETA: 0s - loss: 0.7795 - accuracy: 0.7650\n","Epoch 17: val_accuracy improved from 0.84287 to 0.88496, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 526s 668ms/step - loss: 0.7795 - accuracy: 0.7650 - val_loss: 0.4157 - val_accuracy: 0.8850\n","Epoch 18/75\n","787/787 [==============================] - ETA: 0s - loss: 0.6760 - accuracy: 0.7978\n","Epoch 18: val_accuracy improved from 0.88496 to 0.88552, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 495s 629ms/step - loss: 0.6760 - accuracy: 0.7978 - val_loss: 0.4101 - val_accuracy: 0.8855\n","Epoch 19/75\n","787/787 [==============================] - ETA: 0s - loss: 0.7184 - accuracy: 0.7854\n","Epoch 19: val_accuracy improved from 0.88552 to 0.89338, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 523s 664ms/step - loss: 0.7184 - accuracy: 0.7854 - val_loss: 0.3897 - val_accuracy: 0.8934\n","Epoch 20/75\n","787/787 [==============================] - ETA: 0s - loss: 0.6666 - accuracy: 0.7955\n","Epoch 20: val_accuracy did not improve from 0.89338\n","787/787 [==============================] - 491s 624ms/step - loss: 0.6666 - accuracy: 0.7955 - val_loss: 0.5873 - val_accuracy: 0.8586\n","Epoch 21/75\n","787/787 [==============================] - ETA: 0s - loss: 0.6276 - accuracy: 0.8100\n","Epoch 21: val_accuracy did not improve from 0.89338\n","787/787 [==============================] - 516s 656ms/step - loss: 0.6276 - accuracy: 0.8100 - val_loss: 0.4336 - val_accuracy: 0.8911\n","Epoch 22/75\n","787/787 [==============================] - ETA: 0s - loss: 0.5275 - accuracy: 0.8453\n","Epoch 22: val_accuracy improved from 0.89338 to 0.91470, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 506s 643ms/step - loss: 0.5275 - accuracy: 0.8453 - val_loss: 0.3044 - val_accuracy: 0.9147\n","Epoch 23/75\n","787/787 [==============================] - ETA: 0s - loss: 0.5482 - accuracy: 0.8368\n","Epoch 23: val_accuracy improved from 0.91470 to 0.91695, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 493s 626ms/step - loss: 0.5482 - accuracy: 0.8368 - val_loss: 0.3550 - val_accuracy: 0.9169\n","Epoch 24/75\n","787/787 [==============================] - ETA: 0s - loss: 0.5074 - accuracy: 0.8576\n","Epoch 24: val_accuracy improved from 0.91695 to 0.92480, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 509s 647ms/step - loss: 0.5074 - accuracy: 0.8576 - val_loss: 0.3151 - val_accuracy: 0.9248\n","Epoch 25/75\n","787/787 [==============================] - ETA: 0s - loss: 0.4410 - accuracy: 0.8717\n","Epoch 25: val_accuracy did not improve from 0.92480\n","787/787 [==============================] - 522s 663ms/step - loss: 0.4410 - accuracy: 0.8717 - val_loss: 6.4405 - val_accuracy: 0.3502\n","Epoch 26/75\n","787/787 [==============================] - ETA: 0s - loss: 0.5492 - accuracy: 0.8362\n","Epoch 26: val_accuracy improved from 0.92480 to 0.93659, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 508s 645ms/step - loss: 0.5492 - accuracy: 0.8362 - val_loss: 0.3492 - val_accuracy: 0.9366\n","Epoch 27/75\n","787/787 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.8528\n","Epoch 27: val_accuracy did not improve from 0.93659\n","787/787 [==============================] - 512s 651ms/step - loss: 0.5101 - accuracy: 0.8528 - val_loss: 0.3167 - val_accuracy: 0.9175\n","Epoch 28/75\n","787/787 [==============================] - ETA: 0s - loss: 0.4139 - accuracy: 0.8819\n","Epoch 28: val_accuracy did not improve from 0.93659\n","787/787 [==============================] - 514s 653ms/step - loss: 0.4139 - accuracy: 0.8819 - val_loss: 0.2983 - val_accuracy: 0.9209\n","Epoch 29/75\n","787/787 [==============================] - ETA: 0s - loss: 0.3924 - accuracy: 0.8892\n","Epoch 29: val_accuracy did not improve from 0.93659\n","787/787 [==============================] - 514s 653ms/step - loss: 0.3924 - accuracy: 0.8892 - val_loss: 0.5085 - val_accuracy: 0.9203\n","Epoch 30/75\n","787/787 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.8749\n","Epoch 30: val_accuracy did not improve from 0.93659\n","787/787 [==============================] - 525s 667ms/step - loss: 0.4291 - accuracy: 0.8749 - val_loss: 0.5603 - val_accuracy: 0.9141\n","Epoch 31/75\n","787/787 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.8729\n","Epoch 31: val_accuracy did not improve from 0.93659\n","787/787 [==============================] - 540s 686ms/step - loss: 0.4380 - accuracy: 0.8729 - val_loss: 0.8735 - val_accuracy: 0.8389\n","Epoch 32/75\n","787/787 [==============================] - ETA: 0s - loss: 0.4237 - accuracy: 0.8762\n","Epoch 32: val_accuracy improved from 0.93659 to 0.94164, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 543s 690ms/step - loss: 0.4237 - accuracy: 0.8762 - val_loss: 0.2916 - val_accuracy: 0.9416\n","Epoch 33/75\n","787/787 [==============================] - ETA: 0s - loss: 0.3359 - accuracy: 0.9062\n","Epoch 33: val_accuracy did not improve from 0.94164\n","787/787 [==============================] - 502s 637ms/step - loss: 0.3359 - accuracy: 0.9062 - val_loss: 0.5512 - val_accuracy: 0.9343\n","Epoch 34/75\n","787/787 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.9129\n","Epoch 34: val_accuracy improved from 0.94164 to 0.95398, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 534s 679ms/step - loss: 0.3202 - accuracy: 0.9129 - val_loss: 0.3586 - val_accuracy: 0.9540\n","Epoch 35/75\n","787/787 [==============================] - ETA: 0s - loss: 0.3350 - accuracy: 0.9034\n","Epoch 35: val_accuracy did not improve from 0.95398\n","787/787 [==============================] - 496s 630ms/step - loss: 0.3350 - accuracy: 0.9034 - val_loss: 0.2143 - val_accuracy: 0.9495\n","Epoch 36/75\n","787/787 [==============================] - ETA: 0s - loss: 0.3492 - accuracy: 0.9082\n","Epoch 36: val_accuracy did not improve from 0.95398\n","787/787 [==============================] - 529s 673ms/step - loss: 0.3492 - accuracy: 0.9082 - val_loss: 0.3608 - val_accuracy: 0.9400\n","Epoch 37/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.9214\n","Epoch 37: val_accuracy did not improve from 0.95398\n","787/787 [==============================] - 524s 666ms/step - loss: 0.2717 - accuracy: 0.9214 - val_loss: 1.1771 - val_accuracy: 0.8479\n","Epoch 38/75\n","787/787 [==============================] - ETA: 0s - loss: 0.3398 - accuracy: 0.9064\n","Epoch 38: val_accuracy did not improve from 0.95398\n","787/787 [==============================] - 519s 659ms/step - loss: 0.3398 - accuracy: 0.9064 - val_loss: 0.5134 - val_accuracy: 0.9327\n","Epoch 39/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.9301\n","Epoch 39: val_accuracy did not improve from 0.95398\n","787/787 [==============================] - 537s 682ms/step - loss: 0.2465 - accuracy: 0.9301 - val_loss: 0.2657 - val_accuracy: 0.9484\n","Epoch 40/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2319 - accuracy: 0.9354\n","Epoch 40: val_accuracy did not improve from 0.95398\n","787/787 [==============================] - 530s 673ms/step - loss: 0.2319 - accuracy: 0.9354 - val_loss: 0.3038 - val_accuracy: 0.9473\n","Epoch 41/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2388 - accuracy: 0.9335\n","Epoch 41: val_accuracy did not improve from 0.95398\n","787/787 [==============================] - 527s 670ms/step - loss: 0.2388 - accuracy: 0.9335 - val_loss: 0.3853 - val_accuracy: 0.9512\n","Epoch 42/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.9368\n","Epoch 42: val_accuracy did not improve from 0.95398\n","787/787 [==============================] - 535s 680ms/step - loss: 0.2399 - accuracy: 0.9368 - val_loss: 0.2647 - val_accuracy: 0.9405\n","Epoch 43/75\n","787/787 [==============================] - ETA: 0s - loss: 0.3248 - accuracy: 0.9139\n","Epoch 43: val_accuracy improved from 0.95398 to 0.96016, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 503s 640ms/step - loss: 0.3248 - accuracy: 0.9139 - val_loss: 0.1614 - val_accuracy: 0.9602\n","Epoch 44/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9450\n","Epoch 44: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 523s 665ms/step - loss: 0.2024 - accuracy: 0.9450 - val_loss: 0.2280 - val_accuracy: 0.9478\n","Epoch 45/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9475\n","Epoch 45: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 528s 671ms/step - loss: 0.1903 - accuracy: 0.9475 - val_loss: 0.2795 - val_accuracy: 0.9461\n","Epoch 46/75\n","787/787 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.8931\n","Epoch 46: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 537s 682ms/step - loss: 0.3848 - accuracy: 0.8931 - val_loss: 0.3419 - val_accuracy: 0.9198\n","Epoch 47/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2098 - accuracy: 0.9411\n","Epoch 47: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 536s 681ms/step - loss: 0.2098 - accuracy: 0.9411 - val_loss: 0.2298 - val_accuracy: 0.9484\n","Epoch 48/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2312 - accuracy: 0.9377\n","Epoch 48: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 516s 655ms/step - loss: 0.2312 - accuracy: 0.9377 - val_loss: 0.2921 - val_accuracy: 0.9383\n","Epoch 49/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9513\n","Epoch 49: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 537s 683ms/step - loss: 0.1773 - accuracy: 0.9513 - val_loss: 0.2356 - val_accuracy: 0.9540\n","Epoch 50/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2275 - accuracy: 0.9419\n","Epoch 50: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 520s 661ms/step - loss: 0.2275 - accuracy: 0.9419 - val_loss: 0.1932 - val_accuracy: 0.9574\n","Epoch 51/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9605\n","Epoch 51: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 522s 664ms/step - loss: 0.1610 - accuracy: 0.9605 - val_loss: 0.3279 - val_accuracy: 0.9332\n","Epoch 52/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2080 - accuracy: 0.9457\n","Epoch 52: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 536s 681ms/step - loss: 0.2080 - accuracy: 0.9457 - val_loss: 0.2265 - val_accuracy: 0.9478\n","Epoch 53/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.9361\n","Epoch 53: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 530s 674ms/step - loss: 0.2435 - accuracy: 0.9361 - val_loss: 1.3613 - val_accuracy: 0.7424\n","Epoch 54/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.9296\n","Epoch 54: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 538s 683ms/step - loss: 0.2387 - accuracy: 0.9296 - val_loss: 0.3943 - val_accuracy: 0.9557\n","Epoch 55/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1738 - accuracy: 0.9544\n","Epoch 55: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 519s 660ms/step - loss: 0.1738 - accuracy: 0.9544 - val_loss: 0.2888 - val_accuracy: 0.9495\n","Epoch 56/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1905 - accuracy: 0.9516\n","Epoch 56: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 538s 683ms/step - loss: 0.1905 - accuracy: 0.9516 - val_loss: 0.2845 - val_accuracy: 0.9383\n","Epoch 57/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2165 - accuracy: 0.9411\n","Epoch 57: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 518s 658ms/step - loss: 0.2165 - accuracy: 0.9411 - val_loss: 0.2378 - val_accuracy: 0.9467\n","Epoch 58/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1421 - accuracy: 0.9620\n","Epoch 58: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 546s 694ms/step - loss: 0.1421 - accuracy: 0.9620 - val_loss: 0.3164 - val_accuracy: 0.9495\n","Epoch 59/75\n","787/787 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.9218\n","Epoch 59: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 551s 700ms/step - loss: 0.3138 - accuracy: 0.9218 - val_loss: 0.4674 - val_accuracy: 0.9371\n","Epoch 60/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9521\n","Epoch 60: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 522s 663ms/step - loss: 0.1865 - accuracy: 0.9521 - val_loss: 0.4901 - val_accuracy: 0.9506\n","Epoch 61/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2832 - accuracy: 0.9277\n","Epoch 61: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 537s 681ms/step - loss: 0.2832 - accuracy: 0.9277 - val_loss: 0.3359 - val_accuracy: 0.9315\n","Epoch 62/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.9530\n","Epoch 62: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 525s 667ms/step - loss: 0.1766 - accuracy: 0.9530 - val_loss: 0.2606 - val_accuracy: 0.9473\n","Epoch 63/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.9481\n","Epoch 63: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 534s 679ms/step - loss: 0.1872 - accuracy: 0.9481 - val_loss: 0.1892 - val_accuracy: 0.9517\n","Epoch 64/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2377 - accuracy: 0.9343\n","Epoch 64: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 530s 674ms/step - loss: 0.2377 - accuracy: 0.9343 - val_loss: 0.1697 - val_accuracy: 0.9574\n","Epoch 65/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1709 - accuracy: 0.9561\n","Epoch 65: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 523s 665ms/step - loss: 0.1709 - accuracy: 0.9561 - val_loss: 1.3408 - val_accuracy: 0.7379\n","Epoch 66/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.9370\n","Epoch 66: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 545s 693ms/step - loss: 0.2522 - accuracy: 0.9370 - val_loss: 0.1873 - val_accuracy: 0.9517\n","Epoch 67/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9588\n","Epoch 67: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 524s 666ms/step - loss: 0.1650 - accuracy: 0.9588 - val_loss: 0.2133 - val_accuracy: 0.9574\n","Epoch 68/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.9469\n","Epoch 68: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 544s 691ms/step - loss: 0.1877 - accuracy: 0.9469 - val_loss: 0.2363 - val_accuracy: 0.9444\n","Epoch 69/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9605\n","Epoch 69: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 537s 682ms/step - loss: 0.1614 - accuracy: 0.9605 - val_loss: 0.3412 - val_accuracy: 0.9388\n","Epoch 70/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9500\n","Epoch 70: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 503s 639ms/step - loss: 0.1917 - accuracy: 0.9500 - val_loss: 0.2742 - val_accuracy: 0.9439\n","Epoch 71/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1580 - accuracy: 0.9593\n","Epoch 71: val_accuracy improved from 0.96016 to 0.96184, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch75.h5\n","787/787 [==============================] - 547s 696ms/step - loss: 0.1580 - accuracy: 0.9593 - val_loss: 0.1940 - val_accuracy: 0.9618\n","Epoch 72/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2243 - accuracy: 0.9404\n","Epoch 72: val_accuracy did not improve from 0.96184\n","787/787 [==============================] - 537s 682ms/step - loss: 0.2243 - accuracy: 0.9404 - val_loss: 0.2001 - val_accuracy: 0.9467\n","Epoch 73/75\n","787/787 [==============================] - ETA: 0s - loss: 0.2136 - accuracy: 0.9429\n","Epoch 73: val_accuracy did not improve from 0.96184\n","787/787 [==============================] - 534s 679ms/step - loss: 0.2136 - accuracy: 0.9429 - val_loss: 0.2532 - val_accuracy: 0.9428\n","Epoch 74/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.9521\n","Epoch 74: val_accuracy did not improve from 0.96184\n","787/787 [==============================] - 535s 679ms/step - loss: 0.1769 - accuracy: 0.9521 - val_loss: 0.1722 - val_accuracy: 0.9562\n","Epoch 75/75\n","787/787 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9650\n","Epoch 75: val_accuracy did not improve from 0.96184\n","787/787 [==============================] - 542s 688ms/step - loss: 0.1397 - accuracy: 0.9650 - val_loss: 0.4216 - val_accuracy: 0.9310\n","66/66 [==============================] - 30s 448ms/step\n","[[3.5736309e-03 1.1956633e-02 2.3342222e-03 ... 7.9936406e-04\n","  2.0759469e-03 9.0180790e-01]\n"," [1.8507873e-15 9.2714045e-16 1.0972499e-11 ... 1.8099825e-09\n","  8.1036609e-09 0.0000000e+00]\n"," [2.6934284e-05 9.9836987e-01 3.6563256e-11 ... 7.4481650e-05\n","  5.5441324e-04 9.7286311e-04]\n"," ...\n"," [9.2858664e-04 7.6387464e-03 8.3768973e-04 ... 6.6386827e-04\n","  7.9992088e-04 9.6183264e-01]\n"," [1.0782072e-03 1.5317647e-03 2.5216457e-05 ... 3.9897803e-02\n","  3.0241170e-05 1.1580097e-02]\n"," [1.9521838e-05 7.6339441e-09 8.6110970e-07 ... 4.6452504e-05\n","  1.4616340e-09 5.9193820e-07]]\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Alexnet_code.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1ST_yrafE2p_4JHcbvSpBBYMW1lWIw5hD\n","\"\"\"\n","\n","from __future__ import print_function\n","import keras\n","import pandas as pd\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import os\n","import time\n","from openpyxl import load_workbook\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n","from keras.layers import BatchNormalization\n","from PIL import Image\n","import tensorflow as tf\n","\n","import xlsxwriter\n","import cv2\n","from random import shuffle\n","\n","\n","\n","\n","start1= time.time()\n","\n","# Training parameters\n","batch_size = 10  # orig paper trained all networks with batch_size=128\n","epochs = 75\n","data_augmentation = False\n","num_classes = 12\n","test_img_num = 2083\n","result_excel_link = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Result/fruit_batch10_epoch75.xlsx'\n","Train_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TRAIN'\n","TEST_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TEST'\n","label_link= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL_encoded_file.xlsx'\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 2\n","n = 2\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'AlexNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","##(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","\n","\n","def label_img(name):\n","    # print(name)\n","    filename1 = os.fsdecode(name)\n","    # print(filename1)\n","    s=[]\n","    s= filename1.split('_')\n","    # print(s)\n","    # print(int(s[0]))\n","    #word_label = name.split('-')[0]\n","    #if word_label == 'golden_retriever': return np.array([1, 0])\n","    #elif word_label == 'shetland_sheepdog' : return np.array([0, 1])\n","    wb = load_workbook(label_link)\n","    ws = wb.active\n","    #for cell in ws.columns[1]:  #here column 9 is value is what i am testing which is Column X of my example.\n","    for x in range(2, 18454):\n","        #for y in range(1,):\n","        cell= ws.cell(row=x, column=1)\n","        # print(cell.value)\n","        # print(cell.value)\n","        if cell.value == int(s[0]) :\n","               #print ws.cell(column=12).value\n","                #a= ws.columns(row).value\n","                #b= ws.columns(col).value\n","                cell2= ws.cell(row=x, column=2)\n","\n","                label1= cell2.value\n","                # print(label1)\n","                break\n","        #else:\n","                #print('hi')\n","    return label1\n","\n","def load_training_data():\n","    train_data = []\n","    for img in os.listdir(Train_DIR):\n","        #print(img)\n","        temp2 = img.replace('.png','')\n","        label = label_img(temp2)\n","        #print(label)\n","        path = os.path.join(Train_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        # print(path)\n","        # print(label)\n","        img2= cv2.imread(path)\n","        # cv2_imshow(img2)\n","        #img = img.convert('L')\n","        # img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        #path3= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/training_441_segmented_polar_A1_A2_A4'\n","        resized_img1 = cv2.resize(img2, (224,224), interpolation = cv2.INTER_AREA)\n","        #cv2.imwrite(path3+ '/' + temp2 , resized_img1 )\n","        train_data.append([np.array(resized_img1), label])\n","\n","    shuffle(train_data)\n","    return train_data\n","\n","\n","train_data = load_training_data()\n","\n","# s = [i[1] for i in train_data]\n","# print(train_data)\n","\n","# df2 = pd.DataFrame(np.array(df2),columns=['Label'])\n","# df2['Label'] = pd.Categorical(df2.Label)\n","IMG_SIZE = 224\n","# trainImages = np.array([i[0] for i in train_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","trainImages = np.array([i[0] for i in train_data])\n","# df = pd.read_csv()\n","# trainLabels = tf.convert_to_tensor(np.array([i[1] for i in train_data]).astype(np.int_))\n","trainLabels = np.array([i[1] for i in train_data])\n","print(trainLabels[2]) #successful till here\n","#print(trainLabels)\n","# trainLabels = to_categorical(trainLabels)                                                          #problem here\n","trainLabels = keras.utils.to_categorical(trainLabels, num_classes)\n","print(trainLabels[2])\n","\n","\n","\n","def load_test_data():\n","    test_data = []\n","    for img in os.listdir(TEST_DIR):\n","        temp1 = img.replace('.png','')\n","        # print(temp1)\n","        label = label_img(temp1)\n","        path = os.path.join(TEST_DIR, img)\n","        # temp1 = img\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img1= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        resized_img2 = cv2.resize(img1, (224,224), interpolation = cv2.INTER_AREA)\n","        #path2= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/testing folder_segmented_resized'\n","        #cv2.imwrite(path2+ '/' + temp1 , resized_img2 )\n","        test_data.append([np.array(resized_img2), label,temp1])\n","        # print(label)\n","\n","    shuffle(test_data)\n","    return test_data\n","\n","\n","test_data = load_test_data()\n","#plt.imshow(test_data[10][0], cmap = 'gist_gray')\n","\n","\n","# # convert it to tensorflow              ########################\n","# tensor1 = tf.convert_to_tensor(numpy_array)\n","# print(tensor1)\n","\n","# testImages = np.array([i[0] for i in test_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","# testLabels = tf.convert_to_tensor(np.array([i[1] for i in test_data]).astype(np.int_))\n","\n","testImages = np.array([i[0] for i in test_data])\n","testLabels = np.array([i[1] for i in test_data])\n","test_img_name = np.array([i[2] for i in test_data])\n","\n","\n","#testLabels = keras.utils.to_categorical(testLabels, num_classes)\n","\n","# Input image dimensions.\n","input_shape = trainImages.shape[1:]\n","\n","# Normalize data.\n","##trainImages = trainImages.astype('float32') / 255\n","##testImages = testImages.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","##if subtract_pixel_mean:\n","    ##trainImages_mean = np.mean(trainImages, axis=0)\n","    ##trainImages -= trainImages_mean\n","    ##testImages -= trainImages_mean\n","\n","#print('x_train shape:', x_train.shape)\n","#print(x_train.shape[0], 'train samples')\n","#print(x_test.shape[0], 'test samples')\n","#print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","#y_train = keras.utils.to_categorical(y_train, num_classes)\n","#y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","model = Sequential()\n","\n","# 1st Convolutional Layer\n","model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),strides=(4,4), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation before passing it to the next layer\n","model.add(BatchNormalization())\n","\n","# 2nd Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 3rd Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 5th Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# Passing it to a dense layer\n","model.add(Flatten())\n","# 1st Dense Layer\n","model.add(Dense(4096, input_shape=(224*224*3,)))\n","model.add(Activation('relu'))\n","# Add Dropout to prevent overfitting\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 2nd Dense Layer\n","model.add(Dense(4096))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 3rd Dense Layer\n","model.add(Dense(1000))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Dense Layer\n","model.add(Dense(100))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","\n","# Output Layer\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))\n","\n","model.summary()\n","model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","\n","# Prepare model model saving directory.\n","save_dir = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model'\n","model_name = 'Alexnet_model_fNet_batch10_epoch75.h5'\n","saved_model= save_dir + '/' + model_name\n","\n","# saved_model= save_dir + '/' + model_name\n","# saved_model_final = load_model(saved_model)\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200) #EPOCH CHANGING HERE\n","mc = ModelCheckpoint(saved_model , monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","\n","# print(type(trainImages))\n","# print(type(trainLabels))\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainLabels= np.asarray(trainLabels).astype(np.float32)\n","model.fit(trainImages, trainLabels, batch_size= batch_size, epochs = epochs, verbose=1, validation_split=0.1846, shuffle=True, callbacks=[es, mc])\n","\n","saved_model_final = load_model(saved_model)\n","\n","\n","# Score trained model.\n","preds = saved_model_final.predict(testImages)\n","print(preds)\n","\n","end1= time.time()\n","time_taken= end1- start1\n","\n","workbook = xlsxwriter.Workbook(result_excel_link)\n","worksheet = workbook.add_worksheet()\n","\n","for i in range(0,test_img_num):\n","        #wb1 = load_workbook('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        #ws1 = wb1.active\n","        #a = ws1.cell(row= i+1, column=1)\n","        #a.value = testLabels[i]\n","        #b = ws1.cell(row= i+1, column=2)\n","        #b.value = np.argmax(preds [i])\n","        #ws1.save('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        name = test_img_name[i]\n","        worksheet.write(i+1, 0 , name)\n","        a = testLabels[i]\n","        worksheet.write(i+1, 1 , a)\n","        b = np.argmax(preds[i])\n","        worksheet.write(i+1, 2 , b)\n","        if a==b:\n","             worksheet.write(i+1, 3 , 1)\n","        else:\n","             worksheet.write(i+1, 3 , 0)\n","\n","# worksheet.write(1, 4 , time_taken)\n","\n","workbook.close()\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["11\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_10 (Conv2D)          (None, 54, 54, 96)        34944     \n","                                                                 \n"," activation_20 (Activation)  (None, 54, 54, 96)        0         \n","                                                                 \n"," max_pooling2d_6 (MaxPoolin  (None, 27, 27, 96)        0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_18 (Ba  (None, 27, 27, 96)        384       \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 17, 17, 256)       2973952   \n","                                                                 \n"," activation_21 (Activation)  (None, 17, 17, 256)       0         \n","                                                                 \n"," max_pooling2d_7 (MaxPoolin  (None, 8, 8, 256)         0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_19 (Ba  (None, 8, 8, 256)         1024      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 6, 6, 384)         885120    \n","                                                                 \n"," activation_22 (Activation)  (None, 6, 6, 384)         0         \n","                                                                 \n"," batch_normalization_20 (Ba  (None, 6, 6, 384)         1536      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 4, 4, 384)         1327488   \n","                                                                 \n"," activation_23 (Activation)  (None, 4, 4, 384)         0         \n","                                                                 \n"," batch_normalization_21 (Ba  (None, 4, 4, 384)         1536      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 2, 2, 256)         884992    \n","                                                                 \n"," activation_24 (Activation)  (None, 2, 2, 256)         0         \n","                                                                 \n"," max_pooling2d_8 (MaxPoolin  (None, 1, 1, 256)         0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_22 (Ba  (None, 1, 1, 256)         1024      \n"," tchNormalization)                                               \n","                                                                 \n"," flatten_2 (Flatten)         (None, 256)               0         \n","                                                                 \n"," dense_10 (Dense)            (None, 4096)              1052672   \n","                                                                 \n"," activation_25 (Activation)  (None, 4096)              0         \n","                                                                 \n"," dropout_8 (Dropout)         (None, 4096)              0         \n","                                                                 \n"," batch_normalization_23 (Ba  (None, 4096)              16384     \n"," tchNormalization)                                               \n","                                                                 \n"," dense_11 (Dense)            (None, 4096)              16781312  \n","                                                                 \n"," activation_26 (Activation)  (None, 4096)              0         \n","                                                                 \n"," dropout_9 (Dropout)         (None, 4096)              0         \n","                                                                 \n"," batch_normalization_24 (Ba  (None, 4096)              16384     \n"," tchNormalization)                                               \n","                                                                 \n"," dense_12 (Dense)            (None, 1000)              4097000   \n","                                                                 \n"," activation_27 (Activation)  (None, 1000)              0         \n","                                                                 \n"," dropout_10 (Dropout)        (None, 1000)              0         \n","                                                                 \n"," batch_normalization_25 (Ba  (None, 1000)              4000      \n"," tchNormalization)                                               \n","                                                                 \n"," dense_13 (Dense)            (None, 100)               100100    \n","                                                                 \n"," activation_28 (Activation)  (None, 100)               0         \n","                                                                 \n"," dropout_11 (Dropout)        (None, 100)               0         \n","                                                                 \n"," batch_normalization_26 (Ba  (None, 100)               400       \n"," tchNormalization)                                               \n","                                                                 \n"," dense_14 (Dense)            (None, 12)                1212      \n","                                                                 \n"," activation_29 (Activation)  (None, 12)                0         \n","                                                                 \n","=================================================================\n","Total params: 28181464 (107.50 MB)\n","Trainable params: 28160128 (107.42 MB)\n","Non-trainable params: 21336 (83.34 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","787/787 [==============================] - ETA: 0s - loss: 1.9854 - accuracy: 0.3777\n","Epoch 1: val_accuracy improved from -inf to 0.47643, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 570s 722ms/step - loss: 1.9854 - accuracy: 0.3777 - val_loss: 1.6280 - val_accuracy: 0.4764\n","Epoch 2/100\n","787/787 [==============================] - ETA: 0s - loss: 1.7277 - accuracy: 0.4257\n","Epoch 2: val_accuracy improved from 0.47643 to 0.60045, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 542s 689ms/step - loss: 1.7277 - accuracy: 0.4257 - val_loss: 1.2634 - val_accuracy: 0.6004\n","Epoch 3/100\n","787/787 [==============================] - ETA: 0s - loss: 1.5124 - accuracy: 0.4905\n","Epoch 3: val_accuracy did not improve from 0.60045\n","787/787 [==============================] - 558s 708ms/step - loss: 1.5124 - accuracy: 0.4905 - val_loss: 1.2779 - val_accuracy: 0.5701\n","Epoch 4/100\n","787/787 [==============================] - ETA: 0s - loss: 1.4239 - accuracy: 0.5288\n","Epoch 4: val_accuracy did not improve from 0.60045\n","787/787 [==============================] - 545s 692ms/step - loss: 1.4239 - accuracy: 0.5288 - val_loss: 1.4563 - val_accuracy: 0.5774\n","Epoch 5/100\n","787/787 [==============================] - ETA: 0s - loss: 1.3356 - accuracy: 0.5660\n","Epoch 5: val_accuracy did not improve from 0.60045\n","787/787 [==============================] - 560s 712ms/step - loss: 1.3356 - accuracy: 0.5660 - val_loss: 1.7551 - val_accuracy: 0.5354\n","Epoch 6/100\n","787/787 [==============================] - ETA: 0s - loss: 1.3088 - accuracy: 0.5737\n","Epoch 6: val_accuracy improved from 0.60045 to 0.63131, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 581s 739ms/step - loss: 1.3088 - accuracy: 0.5737 - val_loss: 1.1424 - val_accuracy: 0.6313\n","Epoch 7/100\n","787/787 [==============================] - ETA: 0s - loss: 1.3141 - accuracy: 0.5662\n","Epoch 7: val_accuracy improved from 0.63131 to 0.64815, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 489s 621ms/step - loss: 1.3141 - accuracy: 0.5662 - val_loss: 0.9456 - val_accuracy: 0.6481\n","Epoch 8/100\n","787/787 [==============================] - ETA: 0s - loss: 1.2053 - accuracy: 0.6090\n","Epoch 8: val_accuracy did not improve from 0.64815\n","787/787 [==============================] - 515s 654ms/step - loss: 1.2053 - accuracy: 0.6090 - val_loss: 1.8304 - val_accuracy: 0.5600\n","Epoch 9/100\n","787/787 [==============================] - ETA: 0s - loss: 1.3861 - accuracy: 0.5448\n","Epoch 9: val_accuracy did not improve from 0.64815\n","787/787 [==============================] - 545s 692ms/step - loss: 1.3861 - accuracy: 0.5448 - val_loss: 2.1416 - val_accuracy: 0.3895\n","Epoch 10/100\n","787/787 [==============================] - ETA: 0s - loss: 1.2703 - accuracy: 0.5803\n","Epoch 10: val_accuracy did not improve from 0.64815\n","787/787 [==============================] - 493s 627ms/step - loss: 1.2703 - accuracy: 0.5803 - val_loss: 1.2441 - val_accuracy: 0.6268\n","Epoch 11/100\n","787/787 [==============================] - ETA: 0s - loss: 1.1799 - accuracy: 0.6142\n","Epoch 11: val_accuracy improved from 0.64815 to 0.66218, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 471s 599ms/step - loss: 1.1799 - accuracy: 0.6142 - val_loss: 1.0725 - val_accuracy: 0.6622\n","Epoch 12/100\n","787/787 [==============================] - ETA: 0s - loss: 1.0760 - accuracy: 0.6501\n","Epoch 12: val_accuracy improved from 0.66218 to 0.69192, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 543s 690ms/step - loss: 1.0760 - accuracy: 0.6501 - val_loss: 1.0614 - val_accuracy: 0.6919\n","Epoch 13/100\n","787/787 [==============================] - ETA: 0s - loss: 1.0287 - accuracy: 0.6677\n","Epoch 13: val_accuracy improved from 0.69192 to 0.76824, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 558s 709ms/step - loss: 1.0287 - accuracy: 0.6677 - val_loss: 0.6722 - val_accuracy: 0.7682\n","Epoch 14/100\n","787/787 [==============================] - ETA: 0s - loss: 0.9556 - accuracy: 0.6968\n","Epoch 14: val_accuracy did not improve from 0.76824\n","787/787 [==============================] - 544s 691ms/step - loss: 0.9556 - accuracy: 0.6968 - val_loss: 0.9507 - val_accuracy: 0.7363\n","Epoch 15/100\n","787/787 [==============================] - ETA: 0s - loss: 1.0102 - accuracy: 0.6823\n","Epoch 15: val_accuracy did not improve from 0.76824\n","787/787 [==============================] - 547s 695ms/step - loss: 1.0102 - accuracy: 0.6823 - val_loss: 1.0480 - val_accuracy: 0.7172\n","Epoch 16/100\n","787/787 [==============================] - ETA: 0s - loss: 0.9159 - accuracy: 0.7078\n","Epoch 16: val_accuracy improved from 0.76824 to 0.82492, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 552s 701ms/step - loss: 0.9159 - accuracy: 0.7078 - val_loss: 0.5646 - val_accuracy: 0.8249\n","Epoch 17/100\n","787/787 [==============================] - ETA: 0s - loss: 0.9751 - accuracy: 0.6928\n","Epoch 17: val_accuracy did not improve from 0.82492\n","787/787 [==============================] - 534s 679ms/step - loss: 0.9751 - accuracy: 0.6928 - val_loss: 0.8382 - val_accuracy: 0.7963\n","Epoch 18/100\n","787/787 [==============================] - ETA: 0s - loss: 0.8971 - accuracy: 0.7144\n","Epoch 18: val_accuracy did not improve from 0.82492\n","787/787 [==============================] - 552s 701ms/step - loss: 0.8971 - accuracy: 0.7144 - val_loss: 0.6577 - val_accuracy: 0.8025\n","Epoch 19/100\n","787/787 [==============================] - ETA: 0s - loss: 0.8254 - accuracy: 0.7458\n","Epoch 19: val_accuracy did not improve from 0.82492\n","787/787 [==============================] - 544s 691ms/step - loss: 0.8254 - accuracy: 0.7458 - val_loss: 0.5944 - val_accuracy: 0.8171\n","Epoch 20/100\n","787/787 [==============================] - ETA: 0s - loss: 0.7210 - accuracy: 0.7797\n","Epoch 20: val_accuracy improved from 0.82492 to 0.86756, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 559s 710ms/step - loss: 0.7210 - accuracy: 0.7797 - val_loss: 0.4635 - val_accuracy: 0.8676\n","Epoch 21/100\n","787/787 [==============================] - ETA: 0s - loss: 0.7354 - accuracy: 0.7750\n","Epoch 21: val_accuracy did not improve from 0.86756\n","787/787 [==============================] - 528s 670ms/step - loss: 0.7354 - accuracy: 0.7750 - val_loss: 0.4697 - val_accuracy: 0.8547\n","Epoch 22/100\n","787/787 [==============================] - ETA: 0s - loss: 0.6824 - accuracy: 0.7988\n","Epoch 22: val_accuracy did not improve from 0.86756\n","787/787 [==============================] - 544s 691ms/step - loss: 0.6824 - accuracy: 0.7988 - val_loss: 0.8977 - val_accuracy: 0.7778\n","Epoch 23/100\n","787/787 [==============================] - ETA: 0s - loss: 0.7079 - accuracy: 0.7884\n","Epoch 23: val_accuracy did not improve from 0.86756\n","787/787 [==============================] - 555s 705ms/step - loss: 0.7079 - accuracy: 0.7884 - val_loss: 0.5921 - val_accuracy: 0.8373\n","Epoch 24/100\n","787/787 [==============================] - ETA: 0s - loss: 0.6509 - accuracy: 0.8088\n","Epoch 24: val_accuracy improved from 0.86756 to 0.92200, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 543s 690ms/step - loss: 0.6509 - accuracy: 0.8088 - val_loss: 0.3239 - val_accuracy: 0.9220\n","Epoch 25/100\n","787/787 [==============================] - ETA: 0s - loss: 0.7167 - accuracy: 0.7776\n","Epoch 25: val_accuracy did not improve from 0.92200\n","787/787 [==============================] - 550s 699ms/step - loss: 0.7167 - accuracy: 0.7776 - val_loss: 0.4236 - val_accuracy: 0.8855\n","Epoch 26/100\n","787/787 [==============================] - ETA: 0s - loss: 0.6707 - accuracy: 0.7985\n","Epoch 26: val_accuracy did not improve from 0.92200\n","787/787 [==============================] - 550s 699ms/step - loss: 0.6707 - accuracy: 0.7985 - val_loss: 0.3320 - val_accuracy: 0.8979\n","Epoch 27/100\n","787/787 [==============================] - ETA: 0s - loss: 0.5937 - accuracy: 0.8335\n","Epoch 27: val_accuracy did not improve from 0.92200\n","787/787 [==============================] - 557s 707ms/step - loss: 0.5937 - accuracy: 0.8335 - val_loss: 0.5305 - val_accuracy: 0.8760\n","Epoch 28/100\n","787/787 [==============================] - ETA: 0s - loss: 0.5434 - accuracy: 0.8410\n","Epoch 28: val_accuracy did not improve from 0.92200\n","787/787 [==============================] - 556s 707ms/step - loss: 0.5434 - accuracy: 0.8410 - val_loss: 0.3287 - val_accuracy: 0.9125\n","Epoch 29/100\n","787/787 [==============================] - ETA: 0s - loss: 0.5398 - accuracy: 0.8423\n","Epoch 29: val_accuracy did not improve from 0.92200\n","787/787 [==============================] - 544s 691ms/step - loss: 0.5398 - accuracy: 0.8423 - val_loss: 0.3468 - val_accuracy: 0.9091\n","Epoch 30/100\n","787/787 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.8445\n","Epoch 30: val_accuracy improved from 0.92200 to 0.94052, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 565s 718ms/step - loss: 0.5247 - accuracy: 0.8445 - val_loss: 0.3508 - val_accuracy: 0.9405\n","Epoch 31/100\n","787/787 [==============================] - ETA: 0s - loss: 0.6529 - accuracy: 0.8117\n","Epoch 31: val_accuracy did not improve from 0.94052\n","787/787 [==============================] - 522s 663ms/step - loss: 0.6529 - accuracy: 0.8117 - val_loss: 2.0487 - val_accuracy: 0.8709\n","Epoch 32/100\n","787/787 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.8419\n","Epoch 32: val_accuracy did not improve from 0.94052\n","787/787 [==============================] - 554s 705ms/step - loss: 0.5540 - accuracy: 0.8419 - val_loss: 0.3138 - val_accuracy: 0.9321\n","Epoch 33/100\n","787/787 [==============================] - ETA: 0s - loss: 0.4997 - accuracy: 0.8508\n","Epoch 33: val_accuracy did not improve from 0.94052\n","787/787 [==============================] - 610s 775ms/step - loss: 0.4997 - accuracy: 0.8508 - val_loss: 0.3144 - val_accuracy: 0.9192\n","Epoch 34/100\n","787/787 [==============================] - ETA: 0s - loss: 0.5976 - accuracy: 0.8278\n","Epoch 34: val_accuracy did not improve from 0.94052\n","787/787 [==============================] - 583s 741ms/step - loss: 0.5976 - accuracy: 0.8278 - val_loss: 0.3591 - val_accuracy: 0.9214\n","Epoch 35/100\n","787/787 [==============================] - ETA: 0s - loss: 0.4300 - accuracy: 0.8805\n","Epoch 35: val_accuracy did not improve from 0.94052\n","787/787 [==============================] - 587s 746ms/step - loss: 0.4300 - accuracy: 0.8805 - val_loss: 0.2650 - val_accuracy: 0.9327\n","Epoch 36/100\n","787/787 [==============================] - ETA: 0s - loss: 0.4355 - accuracy: 0.8838\n","Epoch 36: val_accuracy did not improve from 0.94052\n","787/787 [==============================] - 605s 769ms/step - loss: 0.4355 - accuracy: 0.8838 - val_loss: 0.2527 - val_accuracy: 0.9293\n","Epoch 37/100\n","787/787 [==============================] - ETA: 0s - loss: 0.4549 - accuracy: 0.8706\n","Epoch 37: val_accuracy did not improve from 0.94052\n","787/787 [==============================] - 581s 738ms/step - loss: 0.4549 - accuracy: 0.8706 - val_loss: 0.2597 - val_accuracy: 0.9287\n","Epoch 38/100\n","787/787 [==============================] - ETA: 0s - loss: 0.4152 - accuracy: 0.8857\n","Epoch 38: val_accuracy did not improve from 0.94052\n","787/787 [==============================] - 646s 821ms/step - loss: 0.4152 - accuracy: 0.8857 - val_loss: 0.2795 - val_accuracy: 0.9332\n","Epoch 39/100\n","787/787 [==============================] - ETA: 0s - loss: 0.3259 - accuracy: 0.9086\n","Epoch 39: val_accuracy improved from 0.94052 to 0.95342, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 621s 788ms/step - loss: 0.3259 - accuracy: 0.9086 - val_loss: 0.2477 - val_accuracy: 0.9534\n","Epoch 40/100\n","787/787 [==============================] - ETA: 0s - loss: 0.4196 - accuracy: 0.8851\n","Epoch 40: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 652s 828ms/step - loss: 0.4196 - accuracy: 0.8851 - val_loss: 1.8339 - val_accuracy: 0.9383\n","Epoch 41/100\n","787/787 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.8970\n","Epoch 41: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 626s 795ms/step - loss: 0.3749 - accuracy: 0.8970 - val_loss: 0.2281 - val_accuracy: 0.9450\n","Epoch 42/100\n","787/787 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.9147\n","Epoch 42: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 639s 812ms/step - loss: 0.3008 - accuracy: 0.9147 - val_loss: 0.6144 - val_accuracy: 0.9018\n","Epoch 43/100\n","787/787 [==============================] - ETA: 0s - loss: 0.3674 - accuracy: 0.8982\n","Epoch 43: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 645s 819ms/step - loss: 0.3674 - accuracy: 0.8982 - val_loss: 0.3852 - val_accuracy: 0.9080\n","Epoch 44/100\n","787/787 [==============================] - ETA: 0s - loss: 0.4196 - accuracy: 0.8861\n","Epoch 44: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 616s 783ms/step - loss: 0.4196 - accuracy: 0.8861 - val_loss: 0.3676 - val_accuracy: 0.9360\n","Epoch 45/100\n","787/787 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.9129\n","Epoch 45: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 645s 819ms/step - loss: 0.3209 - accuracy: 0.9129 - val_loss: 0.2428 - val_accuracy: 0.9405\n","Epoch 46/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2939 - accuracy: 0.9206\n","Epoch 46: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 616s 782ms/step - loss: 0.2939 - accuracy: 0.9206 - val_loss: 1.9184 - val_accuracy: 0.9024\n","Epoch 47/100\n","787/787 [==============================] - ETA: 0s - loss: 0.3194 - accuracy: 0.9131\n","Epoch 47: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 367s 466ms/step - loss: 0.3194 - accuracy: 0.9131 - val_loss: 0.2289 - val_accuracy: 0.9523\n","Epoch 48/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.9269\n","Epoch 48: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 306s 388ms/step - loss: 0.2755 - accuracy: 0.9269 - val_loss: 0.8092 - val_accuracy: 0.9444\n","Epoch 49/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.9190\n","Epoch 49: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 284s 361ms/step - loss: 0.2974 - accuracy: 0.9190 - val_loss: 0.2877 - val_accuracy: 0.9416\n","Epoch 50/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.9316\n","Epoch 50: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 336s 426ms/step - loss: 0.2499 - accuracy: 0.9316 - val_loss: 0.2634 - val_accuracy: 0.9478\n","Epoch 51/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.9359\n","Epoch 51: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 325s 413ms/step - loss: 0.2403 - accuracy: 0.9359 - val_loss: 0.2212 - val_accuracy: 0.9517\n","Epoch 52/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.9357\n","Epoch 52: val_accuracy did not improve from 0.95342\n","787/787 [==============================] - 324s 412ms/step - loss: 0.2351 - accuracy: 0.9357 - val_loss: 0.3593 - val_accuracy: 0.9512\n","Epoch 53/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.9439\n","Epoch 53: val_accuracy improved from 0.95342 to 0.96016, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 325s 413ms/step - loss: 0.2100 - accuracy: 0.9439 - val_loss: 0.2379 - val_accuracy: 0.9602\n","Epoch 54/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9321\n","Epoch 54: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 313s 397ms/step - loss: 0.2534 - accuracy: 0.9321 - val_loss: 0.3617 - val_accuracy: 0.9484\n","Epoch 55/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2215 - accuracy: 0.9405\n","Epoch 55: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 322s 409ms/step - loss: 0.2215 - accuracy: 0.9405 - val_loss: 0.2604 - val_accuracy: 0.9501\n","Epoch 56/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.9392\n","Epoch 56: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 321s 408ms/step - loss: 0.2365 - accuracy: 0.9392 - val_loss: 0.3189 - val_accuracy: 0.9562\n","Epoch 57/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2234 - accuracy: 0.9410\n","Epoch 57: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 310s 394ms/step - loss: 0.2234 - accuracy: 0.9410 - val_loss: 0.2348 - val_accuracy: 0.9568\n","Epoch 58/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.9441\n","Epoch 58: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 318s 404ms/step - loss: 0.2070 - accuracy: 0.9441 - val_loss: 0.3337 - val_accuracy: 0.9557\n","Epoch 59/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2025 - accuracy: 0.9489\n","Epoch 59: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 318s 404ms/step - loss: 0.2025 - accuracy: 0.9489 - val_loss: 0.2394 - val_accuracy: 0.9506\n","Epoch 60/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1993 - accuracy: 0.9490\n","Epoch 60: val_accuracy improved from 0.96016 to 0.96072, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 319s 405ms/step - loss: 0.1993 - accuracy: 0.9490 - val_loss: 0.1779 - val_accuracy: 0.9607\n","Epoch 61/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2018 - accuracy: 0.9483\n","Epoch 61: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 316s 401ms/step - loss: 0.2018 - accuracy: 0.9483 - val_loss: 0.2205 - val_accuracy: 0.9529\n","Epoch 62/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2354 - accuracy: 0.9347\n","Epoch 62: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 318s 403ms/step - loss: 0.2354 - accuracy: 0.9347 - val_loss: 0.4514 - val_accuracy: 0.8934\n","Epoch 63/100\n","787/787 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.9164\n","Epoch 63: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 319s 405ms/step - loss: 0.3179 - accuracy: 0.9164 - val_loss: 0.4165 - val_accuracy: 0.9007\n","Epoch 64/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9326\n","Epoch 64: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 328s 416ms/step - loss: 0.2574 - accuracy: 0.9326 - val_loss: 0.2047 - val_accuracy: 0.9585\n","Epoch 65/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.9486\n","Epoch 65: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 322s 408ms/step - loss: 0.2063 - accuracy: 0.9486 - val_loss: 0.2869 - val_accuracy: 0.9388\n","Epoch 66/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9436\n","Epoch 66: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 364s 462ms/step - loss: 0.2031 - accuracy: 0.9436 - val_loss: 0.3222 - val_accuracy: 0.9512\n","Epoch 67/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9573\n","Epoch 67: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 627s 797ms/step - loss: 0.1628 - accuracy: 0.9573 - val_loss: 0.3266 - val_accuracy: 0.9568\n","Epoch 68/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.9625\n","Epoch 68: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 633s 805ms/step - loss: 0.1543 - accuracy: 0.9625 - val_loss: 0.2574 - val_accuracy: 0.9456\n","Epoch 69/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2112 - accuracy: 0.9436\n","Epoch 69: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 615s 782ms/step - loss: 0.2112 - accuracy: 0.9436 - val_loss: 1.4142 - val_accuracy: 0.7952\n","Epoch 70/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9549\n","Epoch 70: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 1004s 1s/step - loss: 0.1717 - accuracy: 0.9549 - val_loss: 5.5632 - val_accuracy: 0.8788\n","Epoch 71/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1600 - accuracy: 0.9574\n","Epoch 71: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 1012s 1s/step - loss: 0.1600 - accuracy: 0.9574 - val_loss: 0.7526 - val_accuracy: 0.9119\n","Epoch 72/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1947 - accuracy: 0.9545\n","Epoch 72: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 1013s 1s/step - loss: 0.1947 - accuracy: 0.9545 - val_loss: 0.3407 - val_accuracy: 0.9450\n","Epoch 73/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9357\n","Epoch 73: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 1028s 1s/step - loss: 0.2471 - accuracy: 0.9357 - val_loss: 0.3039 - val_accuracy: 0.9517\n","Epoch 74/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9411\n","Epoch 74: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 977s 1s/step - loss: 0.2220 - accuracy: 0.9411 - val_loss: 0.2872 - val_accuracy: 0.9512\n","Epoch 75/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1710 - accuracy: 0.9545\n","Epoch 75: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 1023s 1s/step - loss: 0.1710 - accuracy: 0.9545 - val_loss: 0.2140 - val_accuracy: 0.9596\n","Epoch 76/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1806 - accuracy: 0.9533\n","Epoch 76: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 1001s 1s/step - loss: 0.1806 - accuracy: 0.9533 - val_loss: 0.3145 - val_accuracy: 0.9495\n","Epoch 77/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9418\n","Epoch 77: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 975s 1s/step - loss: 0.2267 - accuracy: 0.9418 - val_loss: 0.2318 - val_accuracy: 0.9545\n","Epoch 78/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9673\n","Epoch 78: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 993s 1s/step - loss: 0.1361 - accuracy: 0.9673 - val_loss: 0.4020 - val_accuracy: 0.9506\n","Epoch 79/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1511 - accuracy: 0.9622\n","Epoch 79: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 987s 1s/step - loss: 0.1511 - accuracy: 0.9622 - val_loss: 0.4116 - val_accuracy: 0.9568\n","Epoch 80/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1302 - accuracy: 0.9662\n","Epoch 80: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 1006s 1s/step - loss: 0.1302 - accuracy: 0.9662 - val_loss: 0.2333 - val_accuracy: 0.9562\n","Epoch 81/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9714\n","Epoch 81: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 979s 1s/step - loss: 0.1210 - accuracy: 0.9714 - val_loss: 0.5917 - val_accuracy: 0.9040\n","Epoch 82/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1537 - accuracy: 0.9610\n","Epoch 82: val_accuracy improved from 0.96072 to 0.96352, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 954s 1s/step - loss: 0.1537 - accuracy: 0.9610 - val_loss: 0.1679 - val_accuracy: 0.9635\n","Epoch 83/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9648\n","Epoch 83: val_accuracy did not improve from 0.96352\n","787/787 [==============================] - 958s 1s/step - loss: 0.1351 - accuracy: 0.9648 - val_loss: 0.2208 - val_accuracy: 0.9473\n","Epoch 84/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9625\n","Epoch 84: val_accuracy did not improve from 0.96352\n","787/787 [==============================] - 992s 1s/step - loss: 0.1410 - accuracy: 0.9625 - val_loss: 0.2191 - val_accuracy: 0.9557\n","Epoch 85/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1561 - accuracy: 0.9563\n","Epoch 85: val_accuracy did not improve from 0.96352\n","787/787 [==============================] - 995s 1s/step - loss: 0.1561 - accuracy: 0.9563 - val_loss: 0.1832 - val_accuracy: 0.9602\n","Epoch 86/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1234 - accuracy: 0.9681\n","Epoch 86: val_accuracy did not improve from 0.96352\n","787/787 [==============================] - 978s 1s/step - loss: 0.1234 - accuracy: 0.9681 - val_loss: 0.1922 - val_accuracy: 0.9562\n","Epoch 87/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9689\n","Epoch 87: val_accuracy did not improve from 0.96352\n","787/787 [==============================] - 1005s 1s/step - loss: 0.1202 - accuracy: 0.9689 - val_loss: 0.1744 - val_accuracy: 0.9635\n","Epoch 88/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.9659\n","Epoch 88: val_accuracy did not improve from 0.96352\n","787/787 [==============================] - 995s 1s/step - loss: 0.1415 - accuracy: 0.9659 - val_loss: 0.1744 - val_accuracy: 0.9624\n","Epoch 89/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1528 - accuracy: 0.9630\n","Epoch 89: val_accuracy did not improve from 0.96352\n","787/787 [==============================] - 985s 1s/step - loss: 0.1528 - accuracy: 0.9630 - val_loss: 0.2250 - val_accuracy: 0.9501\n","Epoch 90/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9713\n","Epoch 90: val_accuracy did not improve from 0.96352\n","787/787 [==============================] - 995s 1s/step - loss: 0.1204 - accuracy: 0.9713 - val_loss: 0.2350 - val_accuracy: 0.9512\n","Epoch 91/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9652\n","Epoch 91: val_accuracy did not improve from 0.96352\n","787/787 [==============================] - 996s 1s/step - loss: 0.1349 - accuracy: 0.9652 - val_loss: 0.2551 - val_accuracy: 0.9557\n","Epoch 92/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.9657\n","Epoch 92: val_accuracy did not improve from 0.96352\n","787/787 [==============================] - 1029s 1s/step - loss: 0.1311 - accuracy: 0.9657 - val_loss: 0.2792 - val_accuracy: 0.9484\n","Epoch 93/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9517\n","Epoch 93: val_accuracy did not improve from 0.96352\n","787/787 [==============================] - 1001s 1s/step - loss: 0.1822 - accuracy: 0.9517 - val_loss: 0.2083 - val_accuracy: 0.9607\n","Epoch 94/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9671\n","Epoch 94: val_accuracy improved from 0.96352 to 0.96521, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch100.h5\n","787/787 [==============================] - 997s 1s/step - loss: 0.1267 - accuracy: 0.9671 - val_loss: 0.1854 - val_accuracy: 0.9652\n","Epoch 95/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9711\n","Epoch 95: val_accuracy did not improve from 0.96521\n","787/787 [==============================] - 987s 1s/step - loss: 0.1128 - accuracy: 0.9711 - val_loss: 0.2339 - val_accuracy: 0.9501\n","Epoch 96/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9596\n","Epoch 96: val_accuracy did not improve from 0.96521\n","787/787 [==============================] - 1016s 1s/step - loss: 0.1659 - accuracy: 0.9596 - val_loss: 0.2157 - val_accuracy: 0.9540\n","Epoch 97/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1725 - accuracy: 0.9497\n","Epoch 97: val_accuracy did not improve from 0.96521\n","787/787 [==============================] - 1015s 1s/step - loss: 0.1725 - accuracy: 0.9497 - val_loss: 1.0612 - val_accuracy: 0.9304\n","Epoch 98/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.9635\n","Epoch 98: val_accuracy did not improve from 0.96521\n","787/787 [==============================] - 1001s 1s/step - loss: 0.1502 - accuracy: 0.9635 - val_loss: 1.5183 - val_accuracy: 0.9484\n","Epoch 99/100\n","787/787 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9683\n","Epoch 99: val_accuracy did not improve from 0.96521\n","787/787 [==============================] - 1005s 1s/step - loss: 0.1381 - accuracy: 0.9683 - val_loss: 2.3064 - val_accuracy: 0.9501\n","Epoch 100/100\n","787/787 [==============================] - ETA: 0s - loss: 0.2383 - accuracy: 0.9418\n","Epoch 100: val_accuracy did not improve from 0.96521\n","787/787 [==============================] - 1013s 1s/step - loss: 0.2383 - accuracy: 0.9418 - val_loss: 1.7995 - val_accuracy: 0.9433\n","66/66 [==============================] - 57s 862ms/step\n","[[5.8910638e-05 1.2767204e-04 1.9356650e-05 ... 3.9866981e-03\n","  3.0086931e-06 9.8837763e-01]\n"," [7.6082645e-24 3.0407140e-20 7.3653381e-36 ... 3.1004218e-23\n","  3.4086579e-22 0.0000000e+00]\n"," [2.5072783e-10 2.0176818e-13 3.4680689e-17 ... 6.1471894e-10\n","  2.0522201e-14 0.0000000e+00]\n"," ...\n"," [2.9215406e-07 3.6550927e-09 7.4634335e-12 ... 5.8190994e-07\n","  4.5495455e-10 5.0969386e-22]\n"," [3.8768164e-08 2.6080341e-04 2.9593752e-05 ... 8.4269095e-06\n","  3.7661441e-05 3.8975151e-04]\n"," [1.7671801e-04 1.3033962e-03 3.3686560e-04 ... 9.7376709e-05\n","  8.4464980e-04 9.7798312e-01]]\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Alexnet_code.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1ST_yrafE2p_4JHcbvSpBBYMW1lWIw5hD\n","\"\"\"\n","\n","from __future__ import print_function\n","import keras\n","import pandas as pd\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import os\n","import time\n","from openpyxl import load_workbook\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n","from keras.layers import BatchNormalization\n","from PIL import Image\n","import tensorflow as tf\n","\n","import xlsxwriter\n","import cv2\n","from random import shuffle\n","\n","\n","\n","\n","start1= time.time()\n","\n","# Training parameters\n","batch_size = 10  # orig paper trained all networks with batch_size=128\n","epochs = 100\n","data_augmentation = False\n","num_classes = 12\n","test_img_num = 2083\n","result_excel_link = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Result/fruit_batch10_epoch100.xlsx'\n","Train_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TRAIN'\n","TEST_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TEST'\n","label_link= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL_encoded_file.xlsx'\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 2\n","n = 2\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'AlexNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","##(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","\n","\n","def label_img(name):\n","    # print(name)\n","    filename1 = os.fsdecode(name)\n","    # print(filename1)\n","    s=[]\n","    s= filename1.split('_')\n","    # print(s)\n","    # print(int(s[0]))\n","    #word_label = name.split('-')[0]\n","    #if word_label == 'golden_retriever': return np.array([1, 0])\n","    #elif word_label == 'shetland_sheepdog' : return np.array([0, 1])\n","    wb = load_workbook(label_link)\n","    ws = wb.active\n","    #for cell in ws.columns[1]:  #here column 9 is value is what i am testing which is Column X of my example.\n","    for x in range(2, 18454):\n","        #for y in range(1,):\n","        cell= ws.cell(row=x, column=1)\n","        # print(cell.value)\n","        # print(cell.value)\n","        if cell.value == int(s[0]) :\n","               #print ws.cell(column=12).value\n","                #a= ws.columns(row).value\n","                #b= ws.columns(col).value\n","                cell2= ws.cell(row=x, column=2)\n","\n","                label1= cell2.value\n","                # print(label1)\n","                break\n","        #else:\n","                #print('hi')\n","    return label1\n","\n","def load_training_data():\n","    train_data = []\n","    for img in os.listdir(Train_DIR):\n","        #print(img)\n","        temp2 = img.replace('.png','')\n","        label = label_img(temp2)\n","        #print(label)\n","        path = os.path.join(Train_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        # print(path)\n","        # print(label)\n","        img2= cv2.imread(path)\n","        # cv2_imshow(img2)\n","        #img = img.convert('L')\n","        # img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        #path3= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/training_441_segmented_polar_A1_A2_A4'\n","        resized_img1 = cv2.resize(img2, (224,224), interpolation = cv2.INTER_AREA)\n","        #cv2.imwrite(path3+ '/' + temp2 , resized_img1 )\n","        train_data.append([np.array(resized_img1), label])\n","\n","    shuffle(train_data)\n","    return train_data\n","\n","\n","train_data = load_training_data()\n","\n","# s = [i[1] for i in train_data]\n","# print(train_data)\n","\n","# df2 = pd.DataFrame(np.array(df2),columns=['Label'])\n","# df2['Label'] = pd.Categorical(df2.Label)\n","IMG_SIZE = 224\n","# trainImages = np.array([i[0] for i in train_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","trainImages = np.array([i[0] for i in train_data])\n","# df = pd.read_csv()\n","# trainLabels = tf.convert_to_tensor(np.array([i[1] for i in train_data]).astype(np.int_))\n","trainLabels = np.array([i[1] for i in train_data])\n","print(trainLabels[2]) #successful till here\n","#print(trainLabels)\n","# trainLabels = to_categorical(trainLabels)                                                          #problem here\n","trainLabels = keras.utils.to_categorical(trainLabels, num_classes)\n","print(trainLabels[2])\n","\n","\n","\n","def load_test_data():\n","    test_data = []\n","    for img in os.listdir(TEST_DIR):\n","        temp1 = img.replace('.png','')\n","        # print(temp1)\n","        label = label_img(temp1)\n","        path = os.path.join(TEST_DIR, img)\n","        # temp1 = img\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img1= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        resized_img2 = cv2.resize(img1, (224,224), interpolation = cv2.INTER_AREA)\n","        #path2= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/testing folder_segmented_resized'\n","        #cv2.imwrite(path2+ '/' + temp1 , resized_img2 )\n","        test_data.append([np.array(resized_img2), label,temp1])\n","        # print(label)\n","\n","    shuffle(test_data)\n","    return test_data\n","\n","\n","test_data = load_test_data()\n","#plt.imshow(test_data[10][0], cmap = 'gist_gray')\n","\n","\n","# # convert it to tensorflow              ########################\n","# tensor1 = tf.convert_to_tensor(numpy_array)\n","# print(tensor1)\n","\n","# testImages = np.array([i[0] for i in test_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","# testLabels = tf.convert_to_tensor(np.array([i[1] for i in test_data]).astype(np.int_))\n","\n","testImages = np.array([i[0] for i in test_data])\n","testLabels = np.array([i[1] for i in test_data])\n","test_img_name = np.array([i[2] for i in test_data])\n","\n","\n","#testLabels = keras.utils.to_categorical(testLabels, num_classes)\n","\n","# Input image dimensions.\n","input_shape = trainImages.shape[1:]\n","\n","# Normalize data.\n","##trainImages = trainImages.astype('float32') / 255\n","##testImages = testImages.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","##if subtract_pixel_mean:\n","    ##trainImages_mean = np.mean(trainImages, axis=0)\n","    ##trainImages -= trainImages_mean\n","    ##testImages -= trainImages_mean\n","\n","#print('x_train shape:', x_train.shape)\n","#print(x_train.shape[0], 'train samples')\n","#print(x_test.shape[0], 'test samples')\n","#print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","#y_train = keras.utils.to_categorical(y_train, num_classes)\n","#y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","model = Sequential()\n","\n","# 1st Convolutional Layer\n","model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),strides=(4,4), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation before passing it to the next layer\n","model.add(BatchNormalization())\n","\n","# 2nd Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 3rd Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 5th Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# Passing it to a dense layer\n","model.add(Flatten())\n","# 1st Dense Layer\n","model.add(Dense(4096, input_shape=(224*224*3,)))\n","model.add(Activation('relu'))\n","# Add Dropout to prevent overfitting\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 2nd Dense Layer\n","model.add(Dense(4096))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 3rd Dense Layer\n","model.add(Dense(1000))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Dense Layer\n","model.add(Dense(100))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","\n","# Output Layer\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))\n","\n","model.summary()\n","model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","\n","# Prepare model model saving directory.\n","save_dir = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model'\n","model_name = 'Alexnet_model_fNet_batch10_epoch100.h5'\n","saved_model= save_dir + '/' + model_name\n","\n","# saved_model= save_dir + '/' + model_name\n","# saved_model_final = load_model(saved_model)\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200) #EPOCH CHANGING HERE\n","mc = ModelCheckpoint(saved_model , monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","\n","# print(type(trainImages))\n","# print(type(trainLabels))\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainLabels= np.asarray(trainLabels).astype(np.float32)\n","model.fit(trainImages, trainLabels, batch_size= batch_size, epochs = epochs, verbose=1, validation_split=0.1846, shuffle=True, callbacks=[es, mc])\n","\n","saved_model_final = load_model(saved_model)\n","\n","\n","# Score trained model.\n","preds = saved_model_final.predict(testImages)\n","print(preds)\n","\n","end1= time.time()\n","time_taken= end1- start1\n","\n","workbook = xlsxwriter.Workbook(result_excel_link)\n","worksheet = workbook.add_worksheet()\n","\n","for i in range(0,test_img_num):\n","        #wb1 = load_workbook('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        #ws1 = wb1.active\n","        #a = ws1.cell(row= i+1, column=1)\n","        #a.value = testLabels[i]\n","        #b = ws1.cell(row= i+1, column=2)\n","        #b.value = np.argmax(preds [i])\n","        #ws1.save('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        name = test_img_name[i]\n","        worksheet.write(i+1, 0 , name)\n","        a = testLabels[i]\n","        worksheet.write(i+1, 1 , a)\n","        b = np.argmax(preds[i])\n","        worksheet.write(i+1, 2 , b)\n","        if a==b:\n","             worksheet.write(i+1, 3 , 1)\n","        else:\n","             worksheet.write(i+1, 3 , 0)\n","\n","# worksheet.write(1, 4 , time_taken)\n","\n","workbook.close()\n"]},{"cell_type":"markdown","metadata":{},"source":["Epoch 150"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["11\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_15 (Conv2D)          (None, 54, 54, 96)        34944     \n","                                                                 \n"," activation_30 (Activation)  (None, 54, 54, 96)        0         \n","                                                                 \n"," max_pooling2d_9 (MaxPoolin  (None, 27, 27, 96)        0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_27 (Ba  (None, 27, 27, 96)        384       \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_16 (Conv2D)          (None, 17, 17, 256)       2973952   \n","                                                                 \n"," activation_31 (Activation)  (None, 17, 17, 256)       0         \n","                                                                 \n"," max_pooling2d_10 (MaxPooli  (None, 8, 8, 256)         0         \n"," ng2D)                                                           \n","                                                                 \n"," batch_normalization_28 (Ba  (None, 8, 8, 256)         1024      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 6, 6, 384)         885120    \n","                                                                 \n"," activation_32 (Activation)  (None, 6, 6, 384)         0         \n","                                                                 \n"," batch_normalization_29 (Ba  (None, 6, 6, 384)         1536      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_18 (Conv2D)          (None, 4, 4, 384)         1327488   \n","                                                                 \n"," activation_33 (Activation)  (None, 4, 4, 384)         0         \n","                                                                 \n"," batch_normalization_30 (Ba  (None, 4, 4, 384)         1536      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_19 (Conv2D)          (None, 2, 2, 256)         884992    \n","                                                                 \n"," activation_34 (Activation)  (None, 2, 2, 256)         0         \n","                                                                 \n"," max_pooling2d_11 (MaxPooli  (None, 1, 1, 256)         0         \n"," ng2D)                                                           \n","                                                                 \n"," batch_normalization_31 (Ba  (None, 1, 1, 256)         1024      \n"," tchNormalization)                                               \n","                                                                 \n"," flatten_3 (Flatten)         (None, 256)               0         \n","                                                                 \n"," dense_15 (Dense)            (None, 4096)              1052672   \n","                                                                 \n"," activation_35 (Activation)  (None, 4096)              0         \n","                                                                 \n"," dropout_12 (Dropout)        (None, 4096)              0         \n","                                                                 \n"," batch_normalization_32 (Ba  (None, 4096)              16384     \n"," tchNormalization)                                               \n","                                                                 \n"," dense_16 (Dense)            (None, 4096)              16781312  \n","                                                                 \n"," activation_36 (Activation)  (None, 4096)              0         \n","                                                                 \n"," dropout_13 (Dropout)        (None, 4096)              0         \n","                                                                 \n"," batch_normalization_33 (Ba  (None, 4096)              16384     \n"," tchNormalization)                                               \n","                                                                 \n"," dense_17 (Dense)            (None, 1000)              4097000   \n","                                                                 \n"," activation_37 (Activation)  (None, 1000)              0         \n","                                                                 \n"," dropout_14 (Dropout)        (None, 1000)              0         \n","                                                                 \n"," batch_normalization_34 (Ba  (None, 1000)              4000      \n"," tchNormalization)                                               \n","                                                                 \n"," dense_18 (Dense)            (None, 100)               100100    \n","                                                                 \n"," activation_38 (Activation)  (None, 100)               0         \n","                                                                 \n"," dropout_15 (Dropout)        (None, 100)               0         \n","                                                                 \n"," batch_normalization_35 (Ba  (None, 100)               400       \n"," tchNormalization)                                               \n","                                                                 \n"," dense_19 (Dense)            (None, 12)                1212      \n","                                                                 \n"," activation_39 (Activation)  (None, 12)                0         \n","                                                                 \n","=================================================================\n","Total params: 28181464 (107.50 MB)\n","Trainable params: 28160128 (107.42 MB)\n","Non-trainable params: 21336 (83.34 KB)\n","_________________________________________________________________\n","Epoch 1/150\n","787/787 [==============================] - ETA: 0s - loss: 1.8827 - accuracy: 0.4007\n","Epoch 1: val_accuracy improved from -inf to 0.51684, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 1006s 1s/step - loss: 1.8827 - accuracy: 0.4007 - val_loss: 1.4794 - val_accuracy: 0.5168\n","Epoch 2/150\n","787/787 [==============================] - ETA: 0s - loss: 1.5677 - accuracy: 0.4765\n","Epoch 2: val_accuracy improved from 0.51684 to 0.51740, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 984s 1s/step - loss: 1.5677 - accuracy: 0.4765 - val_loss: 1.6030 - val_accuracy: 0.5174\n","Epoch 3/150\n","787/787 [==============================] - ETA: 0s - loss: 1.4757 - accuracy: 0.5049\n","Epoch 3: val_accuracy improved from 0.51740 to 0.57688, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 983s 1s/step - loss: 1.4757 - accuracy: 0.5049 - val_loss: 1.2291 - val_accuracy: 0.5769\n","Epoch 4/150\n","787/787 [==============================] - ETA: 0s - loss: 1.4397 - accuracy: 0.5077\n","Epoch 4: val_accuracy did not improve from 0.57688\n","787/787 [==============================] - 980s 1s/step - loss: 1.4397 - accuracy: 0.5077 - val_loss: 1.2265 - val_accuracy: 0.5606\n","Epoch 5/150\n","787/787 [==============================] - ETA: 0s - loss: 1.3609 - accuracy: 0.5329\n","Epoch 5: val_accuracy improved from 0.57688 to 0.62851, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 987s 1s/step - loss: 1.3609 - accuracy: 0.5329 - val_loss: 1.1467 - val_accuracy: 0.6285\n","Epoch 6/150\n","787/787 [==============================] - ETA: 0s - loss: 1.2760 - accuracy: 0.5714\n","Epoch 6: val_accuracy improved from 0.62851 to 0.66947, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 984s 1s/step - loss: 1.2760 - accuracy: 0.5714 - val_loss: 1.0134 - val_accuracy: 0.6695\n","Epoch 7/150\n","787/787 [==============================] - ETA: 0s - loss: 1.2231 - accuracy: 0.5861\n","Epoch 7: val_accuracy improved from 0.66947 to 0.73288, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 995s 1s/step - loss: 1.2231 - accuracy: 0.5861 - val_loss: 0.8619 - val_accuracy: 0.7329\n","Epoch 8/150\n","787/787 [==============================] - ETA: 0s - loss: 1.2480 - accuracy: 0.5923\n","Epoch 8: val_accuracy did not improve from 0.73288\n","787/787 [==============================] - 963s 1s/step - loss: 1.2480 - accuracy: 0.5923 - val_loss: 0.9523 - val_accuracy: 0.6958\n","Epoch 9/150\n","787/787 [==============================] - ETA: 0s - loss: 1.1762 - accuracy: 0.6104\n","Epoch 9: val_accuracy did not improve from 0.73288\n","787/787 [==============================] - 1033s 1s/step - loss: 1.1762 - accuracy: 0.6104 - val_loss: 1.1131 - val_accuracy: 0.6549\n","Epoch 10/150\n","787/787 [==============================] - ETA: 0s - loss: 1.1484 - accuracy: 0.6176\n","Epoch 10: val_accuracy did not improve from 0.73288\n","787/787 [==============================] - 1005s 1s/step - loss: 1.1484 - accuracy: 0.6176 - val_loss: 0.8668 - val_accuracy: 0.7172\n","Epoch 11/150\n","787/787 [==============================] - ETA: 0s - loss: 1.0159 - accuracy: 0.6690\n","Epoch 11: val_accuracy improved from 0.73288 to 0.77609, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 946s 1s/step - loss: 1.0159 - accuracy: 0.6690 - val_loss: 0.7659 - val_accuracy: 0.7761\n","Epoch 12/150\n","787/787 [==============================] - ETA: 0s - loss: 1.0730 - accuracy: 0.6579\n","Epoch 12: val_accuracy did not improve from 0.77609\n","787/787 [==============================] - 984s 1s/step - loss: 1.0730 - accuracy: 0.6579 - val_loss: 2.1631 - val_accuracy: 0.5208\n","Epoch 13/150\n","787/787 [==============================] - ETA: 0s - loss: 1.0068 - accuracy: 0.6750\n","Epoch 13: val_accuracy did not improve from 0.77609\n","787/787 [==============================] - 977s 1s/step - loss: 1.0068 - accuracy: 0.6750 - val_loss: 1.8265 - val_accuracy: 0.5157\n","Epoch 14/150\n","787/787 [==============================] - ETA: 0s - loss: 0.9298 - accuracy: 0.7076\n","Epoch 14: val_accuracy improved from 0.77609 to 0.81874, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 1058s 1s/step - loss: 0.9298 - accuracy: 0.7076 - val_loss: 0.6284 - val_accuracy: 0.8187\n","Epoch 15/150\n","787/787 [==============================] - ETA: 0s - loss: 0.9299 - accuracy: 0.7202\n","Epoch 15: val_accuracy did not improve from 0.81874\n","787/787 [==============================] - 982s 1s/step - loss: 0.9299 - accuracy: 0.7202 - val_loss: 1.5820 - val_accuracy: 0.6212\n","Epoch 16/150\n","787/787 [==============================] - ETA: 0s - loss: 0.9027 - accuracy: 0.7280\n","Epoch 16: val_accuracy improved from 0.81874 to 0.83614, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 1059s 1s/step - loss: 0.9027 - accuracy: 0.7280 - val_loss: 0.5088 - val_accuracy: 0.8361\n","Epoch 17/150\n","787/787 [==============================] - ETA: 0s - loss: 0.7314 - accuracy: 0.7782\n","Epoch 17: val_accuracy improved from 0.83614 to 0.84175, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 1034s 1s/step - loss: 0.7314 - accuracy: 0.7782 - val_loss: 0.5740 - val_accuracy: 0.8418\n","Epoch 18/150\n","787/787 [==============================] - ETA: 0s - loss: 0.8073 - accuracy: 0.7609\n","Epoch 18: val_accuracy did not improve from 0.84175\n","787/787 [==============================] - 973s 1s/step - loss: 0.8073 - accuracy: 0.7609 - val_loss: 0.8364 - val_accuracy: 0.8013\n","Epoch 19/150\n","787/787 [==============================] - ETA: 0s - loss: 0.7650 - accuracy: 0.7765\n","Epoch 19: val_accuracy improved from 0.84175 to 0.86083, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 986s 1s/step - loss: 0.7650 - accuracy: 0.7765 - val_loss: 0.5074 - val_accuracy: 0.8608\n","Epoch 20/150\n","787/787 [==============================] - ETA: 0s - loss: 0.7043 - accuracy: 0.7965\n","Epoch 20: val_accuracy did not improve from 0.86083\n","787/787 [==============================] - 1005s 1s/step - loss: 0.7043 - accuracy: 0.7965 - val_loss: 1.5374 - val_accuracy: 0.8098\n","Epoch 21/150\n","787/787 [==============================] - ETA: 0s - loss: 0.6140 - accuracy: 0.8251\n","Epoch 21: val_accuracy improved from 0.86083 to 0.87542, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 1035s 1s/step - loss: 0.6140 - accuracy: 0.8251 - val_loss: 0.6289 - val_accuracy: 0.8754\n","Epoch 22/150\n","787/787 [==============================] - ETA: 0s - loss: 0.6009 - accuracy: 0.8278\n","Epoch 22: val_accuracy improved from 0.87542 to 0.88384, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 1009s 1s/step - loss: 0.6009 - accuracy: 0.8278 - val_loss: 0.6222 - val_accuracy: 0.8838\n","Epoch 23/150\n","787/787 [==============================] - ETA: 0s - loss: 0.5784 - accuracy: 0.8396\n","Epoch 23: val_accuracy did not improve from 0.88384\n","787/787 [==============================] - 1007s 1s/step - loss: 0.5784 - accuracy: 0.8396 - val_loss: 0.6945 - val_accuracy: 0.8182\n","Epoch 24/150\n","787/787 [==============================] - ETA: 0s - loss: 0.5962 - accuracy: 0.8257\n","Epoch 24: val_accuracy did not improve from 0.88384\n","787/787 [==============================] - 988s 1s/step - loss: 0.5962 - accuracy: 0.8257 - val_loss: 0.5083 - val_accuracy: 0.8586\n","Epoch 25/150\n","787/787 [==============================] - ETA: 0s - loss: 0.5327 - accuracy: 0.8505\n","Epoch 25: val_accuracy improved from 0.88384 to 0.90292, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 984s 1s/step - loss: 0.5327 - accuracy: 0.8505 - val_loss: 0.3993 - val_accuracy: 0.9029\n","Epoch 26/150\n","787/787 [==============================] - ETA: 0s - loss: 0.5423 - accuracy: 0.8537\n","Epoch 26: val_accuracy improved from 0.90292 to 0.91526, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 999s 1s/step - loss: 0.5423 - accuracy: 0.8537 - val_loss: 0.3273 - val_accuracy: 0.9153\n","Epoch 27/150\n","787/787 [==============================] - ETA: 0s - loss: 0.4587 - accuracy: 0.8721\n","Epoch 27: val_accuracy did not improve from 0.91526\n","787/787 [==============================] - 980s 1s/step - loss: 0.4587 - accuracy: 0.8721 - val_loss: 0.3171 - val_accuracy: 0.9119\n","Epoch 28/150\n","787/787 [==============================] - ETA: 0s - loss: 0.4265 - accuracy: 0.8855\n","Epoch 28: val_accuracy did not improve from 0.91526\n","787/787 [==============================] - 1010s 1s/step - loss: 0.4265 - accuracy: 0.8855 - val_loss: 0.6654 - val_accuracy: 0.8474\n","Epoch 29/150\n","787/787 [==============================] - ETA: 0s - loss: 0.4575 - accuracy: 0.8697\n","Epoch 29: val_accuracy did not improve from 0.91526\n","787/787 [==============================] - 1035s 1s/step - loss: 0.4575 - accuracy: 0.8697 - val_loss: 0.3170 - val_accuracy: 0.9108\n","Epoch 30/150\n","787/787 [==============================] - ETA: 0s - loss: 0.3631 - accuracy: 0.9017\n","Epoch 30: val_accuracy did not improve from 0.91526\n","787/787 [==============================] - 1015s 1s/step - loss: 0.3631 - accuracy: 0.9017 - val_loss: 0.3516 - val_accuracy: 0.9040\n","Epoch 31/150\n","787/787 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.8987\n","Epoch 31: val_accuracy improved from 0.91526 to 0.93322, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 1054s 1s/step - loss: 0.3731 - accuracy: 0.8987 - val_loss: 0.2487 - val_accuracy: 0.9332\n","Epoch 32/150\n","787/787 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.9000\n","Epoch 32: val_accuracy did not improve from 0.93322\n","787/787 [==============================] - 974s 1s/step - loss: 0.3698 - accuracy: 0.9000 - val_loss: 0.4578 - val_accuracy: 0.8900\n","Epoch 33/150\n","787/787 [==============================] - ETA: 0s - loss: 0.3965 - accuracy: 0.8898\n","Epoch 33: val_accuracy did not improve from 0.93322\n","787/787 [==============================] - 1015s 1s/step - loss: 0.3965 - accuracy: 0.8898 - val_loss: 0.4810 - val_accuracy: 0.8838\n","Epoch 34/150\n","787/787 [==============================] - ETA: 0s - loss: 0.4105 - accuracy: 0.8920\n","Epoch 34: val_accuracy did not improve from 0.93322\n","787/787 [==============================] - 1012s 1s/step - loss: 0.4105 - accuracy: 0.8920 - val_loss: 0.4154 - val_accuracy: 0.8979\n","Epoch 35/150\n","787/787 [==============================] - ETA: 0s - loss: 0.3219 - accuracy: 0.9115\n","Epoch 35: val_accuracy improved from 0.93322 to 0.93603, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 988s 1s/step - loss: 0.3219 - accuracy: 0.9115 - val_loss: 0.3133 - val_accuracy: 0.9360\n","Epoch 36/150\n","787/787 [==============================] - ETA: 0s - loss: 0.3774 - accuracy: 0.8963\n","Epoch 36: val_accuracy did not improve from 0.93603\n","787/787 [==============================] - 1012s 1s/step - loss: 0.3774 - accuracy: 0.8963 - val_loss: 0.4524 - val_accuracy: 0.8844\n","Epoch 37/150\n","787/787 [==============================] - ETA: 0s - loss: 0.4703 - accuracy: 0.8717\n","Epoch 37: val_accuracy did not improve from 0.93603\n","787/787 [==============================] - 988s 1s/step - loss: 0.4703 - accuracy: 0.8717 - val_loss: 0.3871 - val_accuracy: 0.9209\n","Epoch 38/150\n","787/787 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.9134\n","Epoch 38: val_accuracy did not improve from 0.93603\n","787/787 [==============================] - 1028s 1s/step - loss: 0.3270 - accuracy: 0.9134 - val_loss: 0.3283 - val_accuracy: 0.9332\n","Epoch 39/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2789 - accuracy: 0.9296\n","Epoch 39: val_accuracy did not improve from 0.93603\n","787/787 [==============================] - 1029s 1s/step - loss: 0.2789 - accuracy: 0.9296 - val_loss: 0.3370 - val_accuracy: 0.9327\n","Epoch 40/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9286\n","Epoch 40: val_accuracy did not improve from 0.93603\n","787/787 [==============================] - 1026s 1s/step - loss: 0.2665 - accuracy: 0.9286 - val_loss: 0.3253 - val_accuracy: 0.9343\n","Epoch 41/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.9258\n","Epoch 41: val_accuracy did not improve from 0.93603\n","787/787 [==============================] - 1024s 1s/step - loss: 0.2847 - accuracy: 0.9258 - val_loss: 0.4347 - val_accuracy: 0.9147\n","Epoch 42/150\n","787/787 [==============================] - ETA: 0s - loss: 0.3827 - accuracy: 0.8958\n","Epoch 42: val_accuracy did not improve from 0.93603\n","787/787 [==============================] - 983s 1s/step - loss: 0.3827 - accuracy: 0.8958 - val_loss: 0.2915 - val_accuracy: 0.9293\n","Epoch 43/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2687 - accuracy: 0.9292\n","Epoch 43: val_accuracy improved from 0.93603 to 0.93827, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 1052s 1s/step - loss: 0.2687 - accuracy: 0.9292 - val_loss: 0.2424 - val_accuracy: 0.9383\n","Epoch 44/150\n","787/787 [==============================] - ETA: 0s - loss: 0.3463 - accuracy: 0.9082\n","Epoch 44: val_accuracy did not improve from 0.93827\n","787/787 [==============================] - 965s 1s/step - loss: 0.3463 - accuracy: 0.9082 - val_loss: 0.2693 - val_accuracy: 0.9321\n","Epoch 45/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2993 - accuracy: 0.9216\n","Epoch 45: val_accuracy did not improve from 0.93827\n","787/787 [==============================] - 1048s 1s/step - loss: 0.2993 - accuracy: 0.9216 - val_loss: 0.2800 - val_accuracy: 0.9304\n","Epoch 46/150\n","787/787 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.9181\n","Epoch 46: val_accuracy did not improve from 0.93827\n","787/787 [==============================] - 1030s 1s/step - loss: 0.3047 - accuracy: 0.9181 - val_loss: 0.2941 - val_accuracy: 0.9355\n","Epoch 47/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.9437\n","Epoch 47: val_accuracy improved from 0.93827 to 0.95006, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 1009s 1s/step - loss: 0.2137 - accuracy: 0.9437 - val_loss: 0.1898 - val_accuracy: 0.9501\n","Epoch 48/150\n","787/787 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.8831\n","Epoch 48: val_accuracy did not improve from 0.95006\n","787/787 [==============================] - 1032s 1s/step - loss: 0.4281 - accuracy: 0.8831 - val_loss: 0.3321 - val_accuracy: 0.9270\n","Epoch 49/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 0.9237\n","Epoch 49: val_accuracy did not improve from 0.95006\n","787/787 [==============================] - 1007s 1s/step - loss: 0.2796 - accuracy: 0.9237 - val_loss: 0.3225 - val_accuracy: 0.9338\n","Epoch 50/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9422\n","Epoch 50: val_accuracy did not improve from 0.95006\n","787/787 [==============================] - 1054s 1s/step - loss: 0.2176 - accuracy: 0.9422 - val_loss: 0.2985 - val_accuracy: 0.9416\n","Epoch 51/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2840 - accuracy: 0.9247\n","Epoch 51: val_accuracy did not improve from 0.95006\n","787/787 [==============================] - 984s 1s/step - loss: 0.2840 - accuracy: 0.9247 - val_loss: 0.2241 - val_accuracy: 0.9416\n","Epoch 52/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2250 - accuracy: 0.9406\n","Epoch 52: val_accuracy did not improve from 0.95006\n","787/787 [==============================] - 1034s 1s/step - loss: 0.2250 - accuracy: 0.9406 - val_loss: 0.2381 - val_accuracy: 0.9473\n","Epoch 53/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.9451\n","Epoch 53: val_accuracy did not improve from 0.95006\n","787/787 [==============================] - 1010s 1s/step - loss: 0.2137 - accuracy: 0.9451 - val_loss: 0.5900 - val_accuracy: 0.9125\n","Epoch 54/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.9356\n","Epoch 54: val_accuracy did not improve from 0.95006\n","787/787 [==============================] - 998s 1s/step - loss: 0.2443 - accuracy: 0.9356 - val_loss: 0.3111 - val_accuracy: 0.9394\n","Epoch 55/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9559\n","Epoch 55: val_accuracy did not improve from 0.95006\n","787/787 [==============================] - 1010s 1s/step - loss: 0.1768 - accuracy: 0.9559 - val_loss: 0.3305 - val_accuracy: 0.9371\n","Epoch 56/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2203 - accuracy: 0.9434\n","Epoch 56: val_accuracy improved from 0.95006 to 0.95118, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 965s 1s/step - loss: 0.2203 - accuracy: 0.9434 - val_loss: 0.1988 - val_accuracy: 0.9512\n","Epoch 57/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.9575\n","Epoch 57: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 988s 1s/step - loss: 0.1625 - accuracy: 0.9575 - val_loss: 2.8326 - val_accuracy: 0.9315\n","Epoch 58/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1779 - accuracy: 0.9542\n","Epoch 58: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 997s 1s/step - loss: 0.1779 - accuracy: 0.9542 - val_loss: 0.2667 - val_accuracy: 0.9394\n","Epoch 59/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.9563\n","Epoch 59: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 632s 803ms/step - loss: 0.1741 - accuracy: 0.9563 - val_loss: 0.2242 - val_accuracy: 0.9450\n","Epoch 60/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.9325\n","Epoch 60: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 533s 678ms/step - loss: 0.2622 - accuracy: 0.9325 - val_loss: 0.4662 - val_accuracy: 0.8984\n","Epoch 61/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.9420\n","Epoch 61: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 514s 653ms/step - loss: 0.2193 - accuracy: 0.9420 - val_loss: 0.2437 - val_accuracy: 0.9444\n","Epoch 62/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.9485\n","Epoch 62: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 535s 680ms/step - loss: 0.2059 - accuracy: 0.9485 - val_loss: 0.2515 - val_accuracy: 0.9428\n","Epoch 63/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9600\n","Epoch 63: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 508s 646ms/step - loss: 0.1596 - accuracy: 0.9600 - val_loss: 0.2189 - val_accuracy: 0.9501\n","Epoch 64/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9682\n","Epoch 64: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 527s 669ms/step - loss: 0.1323 - accuracy: 0.9682 - val_loss: 0.2667 - val_accuracy: 0.9456\n","Epoch 65/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9663\n","Epoch 65: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 526s 668ms/step - loss: 0.1388 - accuracy: 0.9663 - val_loss: 0.3818 - val_accuracy: 0.9214\n","Epoch 66/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9424\n","Epoch 66: val_accuracy improved from 0.95118 to 0.95230, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 509s 647ms/step - loss: 0.2167 - accuracy: 0.9424 - val_loss: 0.2324 - val_accuracy: 0.9523\n","Epoch 67/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1582 - accuracy: 0.9622\n","Epoch 67: val_accuracy did not improve from 0.95230\n","787/787 [==============================] - 509s 647ms/step - loss: 0.1582 - accuracy: 0.9622 - val_loss: 0.3095 - val_accuracy: 0.9433\n","Epoch 68/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9536\n","Epoch 68: val_accuracy did not improve from 0.95230\n","787/787 [==============================] - 514s 653ms/step - loss: 0.2007 - accuracy: 0.9536 - val_loss: 0.3891 - val_accuracy: 0.9052\n","Epoch 69/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1583 - accuracy: 0.9582\n","Epoch 69: val_accuracy did not improve from 0.95230\n","787/787 [==============================] - 524s 666ms/step - loss: 0.1583 - accuracy: 0.9582 - val_loss: 0.2947 - val_accuracy: 0.9321\n","Epoch 70/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9471\n","Epoch 70: val_accuracy did not improve from 0.95230\n","787/787 [==============================] - 520s 660ms/step - loss: 0.2050 - accuracy: 0.9471 - val_loss: 0.7308 - val_accuracy: 0.8614\n","Epoch 71/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1908 - accuracy: 0.9509\n","Epoch 71: val_accuracy did not improve from 0.95230\n","787/787 [==============================] - 510s 648ms/step - loss: 0.1908 - accuracy: 0.9509 - val_loss: 0.2074 - val_accuracy: 0.9512\n","Epoch 72/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9288\n","Epoch 72: val_accuracy did not improve from 0.95230\n","787/787 [==============================] - 531s 674ms/step - loss: 0.2668 - accuracy: 0.9288 - val_loss: 0.3734 - val_accuracy: 0.9237\n","Epoch 73/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2214 - accuracy: 0.9422\n","Epoch 73: val_accuracy did not improve from 0.95230\n","787/787 [==============================] - 525s 667ms/step - loss: 0.2214 - accuracy: 0.9422 - val_loss: 0.3807 - val_accuracy: 0.9186\n","Epoch 74/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9483\n","Epoch 74: val_accuracy did not improve from 0.95230\n","787/787 [==============================] - 518s 658ms/step - loss: 0.2005 - accuracy: 0.9483 - val_loss: 0.3600 - val_accuracy: 0.9203\n","Epoch 75/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1730 - accuracy: 0.9553\n","Epoch 75: val_accuracy did not improve from 0.95230\n","787/787 [==============================] - 516s 656ms/step - loss: 0.1730 - accuracy: 0.9553 - val_loss: 0.1834 - val_accuracy: 0.9523\n","Epoch 76/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1489 - accuracy: 0.9638\n","Epoch 76: val_accuracy improved from 0.95230 to 0.95847, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 515s 655ms/step - loss: 0.1489 - accuracy: 0.9638 - val_loss: 0.1982 - val_accuracy: 0.9585\n","Epoch 77/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.9648\n","Epoch 77: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 502s 638ms/step - loss: 0.1462 - accuracy: 0.9648 - val_loss: 0.2189 - val_accuracy: 0.9444\n","Epoch 78/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9616\n","Epoch 78: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 521s 662ms/step - loss: 0.1481 - accuracy: 0.9616 - val_loss: 0.1843 - val_accuracy: 0.9562\n","Epoch 79/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9563\n","Epoch 79: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 547s 695ms/step - loss: 0.1821 - accuracy: 0.9563 - val_loss: 0.1761 - val_accuracy: 0.9545\n","Epoch 80/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2216 - accuracy: 0.9451\n","Epoch 80: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 520s 661ms/step - loss: 0.2216 - accuracy: 0.9451 - val_loss: 0.4956 - val_accuracy: 0.8883\n","Epoch 81/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1886 - accuracy: 0.9480\n","Epoch 81: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 517s 656ms/step - loss: 0.1886 - accuracy: 0.9480 - val_loss: 0.3145 - val_accuracy: 0.9529\n","Epoch 82/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9709\n","Epoch 82: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 539s 685ms/step - loss: 0.1099 - accuracy: 0.9709 - val_loss: 0.1809 - val_accuracy: 0.9585\n","Epoch 83/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9708\n","Epoch 83: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 525s 667ms/step - loss: 0.1194 - accuracy: 0.9708 - val_loss: 0.2931 - val_accuracy: 0.9545\n","Epoch 84/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9682\n","Epoch 84: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 519s 659ms/step - loss: 0.1260 - accuracy: 0.9682 - val_loss: 0.2047 - val_accuracy: 0.9512\n","Epoch 85/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9648\n","Epoch 85: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 519s 660ms/step - loss: 0.1379 - accuracy: 0.9648 - val_loss: 0.1956 - val_accuracy: 0.9529\n","Epoch 86/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9614\n","Epoch 86: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 518s 658ms/step - loss: 0.1419 - accuracy: 0.9614 - val_loss: 0.7909 - val_accuracy: 0.8760\n","Epoch 87/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.9593\n","Epoch 87: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 546s 694ms/step - loss: 0.1574 - accuracy: 0.9593 - val_loss: 0.2658 - val_accuracy: 0.9568\n","Epoch 88/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1199 - accuracy: 0.9689\n","Epoch 88: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 526s 669ms/step - loss: 0.1199 - accuracy: 0.9689 - val_loss: 0.3063 - val_accuracy: 0.9388\n","Epoch 89/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9661\n","Epoch 89: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 554s 704ms/step - loss: 0.1372 - accuracy: 0.9661 - val_loss: 0.2032 - val_accuracy: 0.9540\n","Epoch 90/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9776\n","Epoch 90: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 628s 798ms/step - loss: 0.0904 - accuracy: 0.9776 - val_loss: 0.2327 - val_accuracy: 0.9534\n","Epoch 91/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9677\n","Epoch 91: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 640s 813ms/step - loss: 0.1169 - accuracy: 0.9677 - val_loss: 0.2669 - val_accuracy: 0.9450\n","Epoch 92/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9694\n","Epoch 92: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 614s 780ms/step - loss: 0.1096 - accuracy: 0.9694 - val_loss: 0.3761 - val_accuracy: 0.9450\n","Epoch 93/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9703\n","Epoch 93: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 590s 749ms/step - loss: 0.1244 - accuracy: 0.9703 - val_loss: 0.4244 - val_accuracy: 0.9489\n","Epoch 94/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9747\n","Epoch 94: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 605s 769ms/step - loss: 0.0971 - accuracy: 0.9747 - val_loss: 0.4772 - val_accuracy: 0.9304\n","Epoch 95/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.9685\n","Epoch 95: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 606s 771ms/step - loss: 0.1352 - accuracy: 0.9685 - val_loss: 4.4857 - val_accuracy: 0.9416\n","Epoch 96/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9695\n","Epoch 96: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 593s 753ms/step - loss: 0.1321 - accuracy: 0.9695 - val_loss: 42.4259 - val_accuracy: 0.9411\n","Epoch 97/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9727\n","Epoch 97: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 612s 777ms/step - loss: 0.1068 - accuracy: 0.9727 - val_loss: 93.6395 - val_accuracy: 0.9501\n","Epoch 98/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9708\n","Epoch 98: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 598s 760ms/step - loss: 0.1104 - accuracy: 0.9708 - val_loss: 61.4924 - val_accuracy: 0.9444\n","Epoch 99/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.9728\n","Epoch 99: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 609s 774ms/step - loss: 0.1149 - accuracy: 0.9728 - val_loss: 55.0850 - val_accuracy: 0.9557\n","Epoch 100/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9729\n","Epoch 100: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 628s 797ms/step - loss: 0.1067 - accuracy: 0.9729 - val_loss: 20.9199 - val_accuracy: 0.9377\n","Epoch 101/150\n","787/787 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.9123\n","Epoch 101: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 619s 787ms/step - loss: 0.3410 - accuracy: 0.9123 - val_loss: 68.1443 - val_accuracy: 0.9141\n","Epoch 102/150\n","787/787 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9394\n","Epoch 102: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 614s 779ms/step - loss: 0.2111 - accuracy: 0.9394 - val_loss: 36.8457 - val_accuracy: 0.9338\n","Epoch 103/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9540\n","Epoch 103: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 593s 753ms/step - loss: 0.1789 - accuracy: 0.9540 - val_loss: 110.2938 - val_accuracy: 0.9214\n","Epoch 104/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9711\n","Epoch 104: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 624s 793ms/step - loss: 0.1128 - accuracy: 0.9711 - val_loss: 35.4760 - val_accuracy: 0.9450\n","Epoch 105/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9666\n","Epoch 105: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 599s 762ms/step - loss: 0.1351 - accuracy: 0.9666 - val_loss: 11.2806 - val_accuracy: 0.9478\n","Epoch 106/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9723\n","Epoch 106: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 615s 781ms/step - loss: 0.1045 - accuracy: 0.9723 - val_loss: 10.0939 - val_accuracy: 0.9338\n","Epoch 107/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9757\n","Epoch 107: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 626s 795ms/step - loss: 0.0988 - accuracy: 0.9757 - val_loss: 3.2846 - val_accuracy: 0.9169\n","Epoch 108/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9765\n","Epoch 108: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 594s 754ms/step - loss: 0.0962 - accuracy: 0.9765 - val_loss: 2.6597 - val_accuracy: 0.9108\n","Epoch 109/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9703\n","Epoch 109: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 619s 787ms/step - loss: 0.1208 - accuracy: 0.9703 - val_loss: 0.2101 - val_accuracy: 0.9579\n","Epoch 110/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 0.9739\n","Epoch 110: val_accuracy improved from 0.95847 to 0.96016, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 613s 779ms/step - loss: 0.1078 - accuracy: 0.9739 - val_loss: 0.2629 - val_accuracy: 0.9602\n","Epoch 111/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9761\n","Epoch 111: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 605s 768ms/step - loss: 0.0948 - accuracy: 0.9761 - val_loss: 0.5809 - val_accuracy: 0.9489\n","Epoch 112/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9680\n","Epoch 112: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 620s 788ms/step - loss: 0.1235 - accuracy: 0.9680 - val_loss: 2.2909 - val_accuracy: 0.9416\n","Epoch 113/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9717\n","Epoch 113: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 892s 1s/step - loss: 0.1160 - accuracy: 0.9717 - val_loss: 0.7056 - val_accuracy: 0.9327\n","Epoch 114/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1151 - accuracy: 0.9687\n","Epoch 114: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1033s 1s/step - loss: 0.1151 - accuracy: 0.9687 - val_loss: 1.0975 - val_accuracy: 0.9450\n","Epoch 115/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9807\n","Epoch 115: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 986s 1s/step - loss: 0.0855 - accuracy: 0.9807 - val_loss: 1.9890 - val_accuracy: 0.9540\n","Epoch 116/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9827\n","Epoch 116: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1026s 1s/step - loss: 0.0646 - accuracy: 0.9827 - val_loss: 1.6649 - val_accuracy: 0.9523\n","Epoch 117/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9793\n","Epoch 117: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1037s 1s/step - loss: 0.0813 - accuracy: 0.9793 - val_loss: 6.7283 - val_accuracy: 0.9383\n","Epoch 118/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9780\n","Epoch 118: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 991s 1s/step - loss: 0.0918 - accuracy: 0.9780 - val_loss: 7.6569 - val_accuracy: 0.9276\n","Epoch 119/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9725\n","Epoch 119: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1036s 1s/step - loss: 0.1059 - accuracy: 0.9725 - val_loss: 143.4075 - val_accuracy: 0.9203\n","Epoch 120/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9802\n","Epoch 120: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 977s 1s/step - loss: 0.0783 - accuracy: 0.9802 - val_loss: 6.3851 - val_accuracy: 0.9489\n","Epoch 121/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9830\n","Epoch 121: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1023s 1s/step - loss: 0.0842 - accuracy: 0.9830 - val_loss: 0.6835 - val_accuracy: 0.9456\n","Epoch 122/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9813\n","Epoch 122: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1018s 1s/step - loss: 0.0722 - accuracy: 0.9813 - val_loss: 0.3094 - val_accuracy: 0.9579\n","Epoch 123/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.9748\n","Epoch 123: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1020s 1s/step - loss: 0.0941 - accuracy: 0.9748 - val_loss: 0.2939 - val_accuracy: 0.9478\n","Epoch 124/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9790\n","Epoch 124: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1036s 1s/step - loss: 0.0814 - accuracy: 0.9790 - val_loss: 0.8900 - val_accuracy: 0.9523\n","Epoch 125/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9822\n","Epoch 125: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 998s 1s/step - loss: 0.0647 - accuracy: 0.9822 - val_loss: 0.5926 - val_accuracy: 0.9489\n","Epoch 126/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9804\n","Epoch 126: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1016s 1s/step - loss: 0.0794 - accuracy: 0.9804 - val_loss: 1.3354 - val_accuracy: 0.9495\n","Epoch 127/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9789\n","Epoch 127: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 969s 1s/step - loss: 0.0873 - accuracy: 0.9789 - val_loss: 0.8351 - val_accuracy: 0.7772\n","Epoch 128/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9697\n","Epoch 128: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1045s 1s/step - loss: 0.1271 - accuracy: 0.9697 - val_loss: 0.5747 - val_accuracy: 0.9478\n","Epoch 129/150\n","787/787 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9747\n","Epoch 129: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1112s 1s/step - loss: 0.1035 - accuracy: 0.9747 - val_loss: 0.3739 - val_accuracy: 0.9444\n","Epoch 130/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9814\n","Epoch 130: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1038s 1s/step - loss: 0.0714 - accuracy: 0.9814 - val_loss: 0.3254 - val_accuracy: 0.9540\n","Epoch 131/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9802\n","Epoch 131: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1096s 1s/step - loss: 0.0774 - accuracy: 0.9802 - val_loss: 0.2895 - val_accuracy: 0.9506\n","Epoch 132/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9823\n","Epoch 132: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1135s 1s/step - loss: 0.0681 - accuracy: 0.9823 - val_loss: 0.2464 - val_accuracy: 0.9574\n","Epoch 133/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9861\n","Epoch 133: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 989s 1s/step - loss: 0.0535 - accuracy: 0.9861 - val_loss: 0.2705 - val_accuracy: 0.9506\n","Epoch 134/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9766\n","Epoch 134: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1041s 1s/step - loss: 0.0936 - accuracy: 0.9766 - val_loss: 0.5452 - val_accuracy: 0.9338\n","Epoch 135/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0925 - accuracy: 0.9767\n","Epoch 135: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1039s 1s/step - loss: 0.0925 - accuracy: 0.9767 - val_loss: 0.6595 - val_accuracy: 0.9456\n","Epoch 136/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9808\n","Epoch 136: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1104s 1s/step - loss: 0.0790 - accuracy: 0.9808 - val_loss: 0.5546 - val_accuracy: 0.9495\n","Epoch 137/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9839\n","Epoch 137: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1078s 1s/step - loss: 0.0649 - accuracy: 0.9839 - val_loss: 1.3804 - val_accuracy: 0.9551\n","Epoch 138/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9847\n","Epoch 138: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1128s 1s/step - loss: 0.0632 - accuracy: 0.9847 - val_loss: 1.8458 - val_accuracy: 0.9506\n","Epoch 139/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9783\n","Epoch 139: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1153s 1s/step - loss: 0.0948 - accuracy: 0.9783 - val_loss: 0.7966 - val_accuracy: 0.9450\n","Epoch 140/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9737\n","Epoch 140: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1262s 2s/step - loss: 0.0992 - accuracy: 0.9737 - val_loss: 7.6322 - val_accuracy: 0.9517\n","Epoch 141/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9849\n","Epoch 141: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1124s 1s/step - loss: 0.0585 - accuracy: 0.9849 - val_loss: 0.9079 - val_accuracy: 0.9562\n","Epoch 142/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9794\n","Epoch 142: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1048s 1s/step - loss: 0.0904 - accuracy: 0.9794 - val_loss: 64.0909 - val_accuracy: 0.9371\n","Epoch 143/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9804\n","Epoch 143: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1138s 1s/step - loss: 0.0748 - accuracy: 0.9804 - val_loss: 109.8088 - val_accuracy: 0.9450\n","Epoch 144/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9783\n","Epoch 144: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1097s 1s/step - loss: 0.0904 - accuracy: 0.9783 - val_loss: 124.9642 - val_accuracy: 0.9501\n","Epoch 145/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9842\n","Epoch 145: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1130s 1s/step - loss: 0.0658 - accuracy: 0.9842 - val_loss: 69.5175 - val_accuracy: 0.9467\n","Epoch 146/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9778\n","Epoch 146: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1080s 1s/step - loss: 0.0967 - accuracy: 0.9778 - val_loss: 58.8176 - val_accuracy: 0.9304\n","Epoch 147/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9826\n","Epoch 147: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1284s 2s/step - loss: 0.0659 - accuracy: 0.9826 - val_loss: 56.5653 - val_accuracy: 0.9557\n","Epoch 148/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9833\n","Epoch 148: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1314s 2s/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 12.0738 - val_accuracy: 0.9540\n","Epoch 149/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9868\n","Epoch 149: val_accuracy did not improve from 0.96016\n","787/787 [==============================] - 1248s 2s/step - loss: 0.0605 - accuracy: 0.9868 - val_loss: 2.0205 - val_accuracy: 0.9529\n","Epoch 150/150\n","787/787 [==============================] - ETA: 0s - loss: 0.0420 - accuracy: 0.9898\n","Epoch 150: val_accuracy improved from 0.96016 to 0.96128, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch150.h5\n","787/787 [==============================] - 1313s 2s/step - loss: 0.0420 - accuracy: 0.9898 - val_loss: 20.5229 - val_accuracy: 0.9613\n","66/66 [==============================] - 78s 1s/step\n","[[1.0523132e-05 1.9714322e-03 4.5702595e-07 ... 9.6805888e-04\n","  1.1953162e-04 9.9541605e-01]\n"," [2.8831228e-06 7.6110546e-10 2.1010513e-12 ... 6.4263399e-09\n","  5.7715199e-13 1.1035856e-31]\n"," [1.5644872e-06 9.3341379e-10 2.0950251e-06 ... 3.3734511e-06\n","  2.1947425e-08 1.0291894e-07]\n"," ...\n"," [7.8969952e-06 4.2801411e-04 1.8666987e-07 ... 5.3178746e-04\n","  6.0192011e-05 9.9859995e-01]\n"," [4.2867185e-13 1.6440175e-07 7.4316100e-12 ... 1.1213292e-27\n","  8.0358314e-10 9.9999988e-01]\n"," [9.9999607e-01 1.6218391e-07 2.2414144e-07 ... 3.1641521e-09\n","  2.6202131e-06 8.3166654e-11]]\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Alexnet_code.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1ST_yrafE2p_4JHcbvSpBBYMW1lWIw5hD\n","\"\"\"\n","\n","from __future__ import print_function\n","import keras\n","import pandas as pd\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import os\n","import time\n","from openpyxl import load_workbook\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n","from keras.layers import BatchNormalization\n","from PIL import Image\n","import tensorflow as tf\n","\n","import xlsxwriter\n","import cv2\n","from random import shuffle\n","\n","\n","\n","\n","start1= time.time()\n","\n","# Training parameters\n","batch_size = 10  # orig paper trained all networks with batch_size=128\n","epochs = 150\n","data_augmentation = False\n","num_classes = 12\n","test_img_num = 2083\n","result_excel_link = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Result/fruit_batch10_epoch150.xlsx'\n","Train_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TRAIN'\n","TEST_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TEST'\n","label_link= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL_encoded_file.xlsx'\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 2\n","n = 2\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'AlexNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","##(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","\n","\n","def label_img(name):\n","    # print(name)\n","    filename1 = os.fsdecode(name)\n","    # print(filename1)\n","    s=[]\n","    s= filename1.split('_')\n","    # print(s)\n","    # print(int(s[0]))\n","    #word_label = name.split('-')[0]\n","    #if word_label == 'golden_retriever': return np.array([1, 0])\n","    #elif word_label == 'shetland_sheepdog' : return np.array([0, 1])\n","    wb = load_workbook(label_link)\n","    ws = wb.active\n","    #for cell in ws.columns[1]:  #here column 9 is value is what i am testing which is Column X of my example.\n","    for x in range(2, 18454):\n","        #for y in range(1,):\n","        cell= ws.cell(row=x, column=1)\n","        # print(cell.value)\n","        # print(cell.value)\n","        if cell.value == int(s[0]) :\n","               #print ws.cell(column=12).value\n","                #a= ws.columns(row).value\n","                #b= ws.columns(col).value\n","                cell2= ws.cell(row=x, column=2)\n","\n","                label1= cell2.value\n","                # print(label1)\n","                break\n","        #else:\n","                #print('hi')\n","    return label1\n","\n","def load_training_data():\n","    train_data = []\n","    for img in os.listdir(Train_DIR):\n","        #print(img)\n","        temp2 = img.replace('.png','')\n","        label = label_img(temp2)\n","        #print(label)\n","        path = os.path.join(Train_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        # print(path)\n","        # print(label)\n","        img2= cv2.imread(path)\n","        # cv2_imshow(img2)\n","        #img = img.convert('L')\n","        # img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        #path3= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/training_441_segmented_polar_A1_A2_A4'\n","        resized_img1 = cv2.resize(img2, (224,224), interpolation = cv2.INTER_AREA)\n","        #cv2.imwrite(path3+ '/' + temp2 , resized_img1 )\n","        train_data.append([np.array(resized_img1), label])\n","\n","    shuffle(train_data)\n","    return train_data\n","\n","\n","train_data = load_training_data()\n","\n","# s = [i[1] for i in train_data]\n","# print(train_data)\n","\n","# df2 = pd.DataFrame(np.array(df2),columns=['Label'])\n","# df2['Label'] = pd.Categorical(df2.Label)\n","IMG_SIZE = 224\n","# trainImages = np.array([i[0] for i in train_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","trainImages = np.array([i[0] for i in train_data])\n","# df = pd.read_csv()\n","# trainLabels = tf.convert_to_tensor(np.array([i[1] for i in train_data]).astype(np.int_))\n","trainLabels = np.array([i[1] for i in train_data])\n","print(trainLabels[2]) #successful till here\n","#print(trainLabels)\n","# trainLabels = to_categorical(trainLabels)                                                          #problem here\n","trainLabels = keras.utils.to_categorical(trainLabels, num_classes)\n","print(trainLabels[2])\n","\n","\n","\n","def load_test_data():\n","    test_data = []\n","    for img in os.listdir(TEST_DIR):\n","        temp1 = img.replace('.png','')\n","        # print(temp1)\n","        label = label_img(temp1)\n","        path = os.path.join(TEST_DIR, img)\n","        # temp1 = img\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img1= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        resized_img2 = cv2.resize(img1, (224,224), interpolation = cv2.INTER_AREA)\n","        #path2= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/testing folder_segmented_resized'\n","        #cv2.imwrite(path2+ '/' + temp1 , resized_img2 )\n","        test_data.append([np.array(resized_img2), label,temp1])\n","        # print(label)\n","\n","    shuffle(test_data)\n","    return test_data\n","\n","\n","test_data = load_test_data()\n","#plt.imshow(test_data[10][0], cmap = 'gist_gray')\n","\n","\n","# # convert it to tensorflow              ########################\n","# tensor1 = tf.convert_to_tensor(numpy_array)\n","# print(tensor1)\n","\n","# testImages = np.array([i[0] for i in test_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","# testLabels = tf.convert_to_tensor(np.array([i[1] for i in test_data]).astype(np.int_))\n","\n","testImages = np.array([i[0] for i in test_data])\n","testLabels = np.array([i[1] for i in test_data])\n","test_img_name = np.array([i[2] for i in test_data])\n","\n","\n","#testLabels = keras.utils.to_categorical(testLabels, num_classes)\n","\n","# Input image dimensions.\n","input_shape = trainImages.shape[1:]\n","\n","# Normalize data.\n","##trainImages = trainImages.astype('float32') / 255\n","##testImages = testImages.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","##if subtract_pixel_mean:\n","    ##trainImages_mean = np.mean(trainImages, axis=0)\n","    ##trainImages -= trainImages_mean\n","    ##testImages -= trainImages_mean\n","\n","#print('x_train shape:', x_train.shape)\n","#print(x_train.shape[0], 'train samples')\n","#print(x_test.shape[0], 'test samples')\n","#print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","#y_train = keras.utils.to_categorical(y_train, num_classes)\n","#y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","model = Sequential()\n","\n","# 1st Convolutional Layer\n","model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),strides=(4,4), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation before passing it to the next layer\n","model.add(BatchNormalization())\n","\n","# 2nd Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 3rd Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 5th Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# Passing it to a dense layer\n","model.add(Flatten())\n","# 1st Dense Layer\n","model.add(Dense(4096, input_shape=(224*224*3,)))\n","model.add(Activation('relu'))\n","# Add Dropout to prevent overfitting\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 2nd Dense Layer\n","model.add(Dense(4096))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 3rd Dense Layer\n","model.add(Dense(1000))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Dense Layer\n","model.add(Dense(100))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","\n","# Output Layer\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))\n","\n","model.summary()\n","model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","\n","# Prepare model model saving directory.\n","save_dir = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model'\n","model_name = 'Alexnet_model_fNet_batch10_epoch150.h5'\n","saved_model= save_dir + '/' + model_name\n","\n","# saved_model= save_dir + '/' + model_name\n","# saved_model_final = load_model(saved_model)\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200) #EPOCH CHANGING HERE\n","mc = ModelCheckpoint(saved_model , monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","\n","# print(type(trainImages))\n","# print(type(trainLabels))\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainLabels= np.asarray(trainLabels).astype(np.float32)\n","model.fit(trainImages, trainLabels, batch_size= batch_size, epochs = epochs, verbose=1, validation_split=0.1846, shuffle=True, callbacks=[es, mc])\n","\n","saved_model_final = load_model(saved_model)\n","\n","\n","# Score trained model.\n","preds = saved_model_final.predict(testImages)\n","print(preds)\n","\n","end1= time.time()\n","time_taken= end1- start1\n","\n","workbook = xlsxwriter.Workbook(result_excel_link)\n","worksheet = workbook.add_worksheet()\n","\n","for i in range(0,test_img_num):\n","        #wb1 = load_workbook('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        #ws1 = wb1.active\n","        #a = ws1.cell(row= i+1, column=1)\n","        #a.value = testLabels[i]\n","        #b = ws1.cell(row= i+1, column=2)\n","        #b.value = np.argmax(preds [i])\n","        #ws1.save('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        name = test_img_name[i]\n","        worksheet.write(i+1, 0 , name)\n","        a = testLabels[i]\n","        worksheet.write(i+1, 1 , a)\n","        b = np.argmax(preds[i])\n","        worksheet.write(i+1, 2 , b)\n","        if a==b:\n","             worksheet.write(i+1, 3 , 1)\n","        else:\n","             worksheet.write(i+1, 3 , 0)\n","\n","# worksheet.write(1, 4 , time_taken)\n","\n","workbook.close()\n"]},{"cell_type":"markdown","metadata":{},"source":["Epoch 200"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4\n","[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_20 (Conv2D)          (None, 54, 54, 96)        34944     \n","                                                                 \n"," activation_40 (Activation)  (None, 54, 54, 96)        0         \n","                                                                 \n"," max_pooling2d_12 (MaxPooli  (None, 27, 27, 96)        0         \n"," ng2D)                                                           \n","                                                                 \n"," batch_normalization_36 (Ba  (None, 27, 27, 96)        384       \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_21 (Conv2D)          (None, 17, 17, 256)       2973952   \n","                                                                 \n"," activation_41 (Activation)  (None, 17, 17, 256)       0         \n","                                                                 \n"," max_pooling2d_13 (MaxPooli  (None, 8, 8, 256)         0         \n"," ng2D)                                                           \n","                                                                 \n"," batch_normalization_37 (Ba  (None, 8, 8, 256)         1024      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_22 (Conv2D)          (None, 6, 6, 384)         885120    \n","                                                                 \n"," activation_42 (Activation)  (None, 6, 6, 384)         0         \n","                                                                 \n"," batch_normalization_38 (Ba  (None, 6, 6, 384)         1536      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_23 (Conv2D)          (None, 4, 4, 384)         1327488   \n","                                                                 \n"," activation_43 (Activation)  (None, 4, 4, 384)         0         \n","                                                                 \n"," batch_normalization_39 (Ba  (None, 4, 4, 384)         1536      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_24 (Conv2D)          (None, 2, 2, 256)         884992    \n","                                                                 \n"," activation_44 (Activation)  (None, 2, 2, 256)         0         \n","                                                                 \n"," max_pooling2d_14 (MaxPooli  (None, 1, 1, 256)         0         \n"," ng2D)                                                           \n","                                                                 \n"," batch_normalization_40 (Ba  (None, 1, 1, 256)         1024      \n"," tchNormalization)                                               \n","                                                                 \n"," flatten_4 (Flatten)         (None, 256)               0         \n","                                                                 \n"," dense_20 (Dense)            (None, 4096)              1052672   \n","                                                                 \n"," activation_45 (Activation)  (None, 4096)              0         \n","                                                                 \n"," dropout_16 (Dropout)        (None, 4096)              0         \n","                                                                 \n"," batch_normalization_41 (Ba  (None, 4096)              16384     \n"," tchNormalization)                                               \n","                                                                 \n"," dense_21 (Dense)            (None, 4096)              16781312  \n","                                                                 \n"," activation_46 (Activation)  (None, 4096)              0         \n","                                                                 \n"," dropout_17 (Dropout)        (None, 4096)              0         \n","                                                                 \n"," batch_normalization_42 (Ba  (None, 4096)              16384     \n"," tchNormalization)                                               \n","                                                                 \n"," dense_22 (Dense)            (None, 1000)              4097000   \n","                                                                 \n"," activation_47 (Activation)  (None, 1000)              0         \n","                                                                 \n"," dropout_18 (Dropout)        (None, 1000)              0         \n","                                                                 \n"," batch_normalization_43 (Ba  (None, 1000)              4000      \n"," tchNormalization)                                               \n","                                                                 \n"," dense_23 (Dense)            (None, 100)               100100    \n","                                                                 \n"," activation_48 (Activation)  (None, 100)               0         \n","                                                                 \n"," dropout_19 (Dropout)        (None, 100)               0         \n","                                                                 \n"," batch_normalization_44 (Ba  (None, 100)               400       \n"," tchNormalization)                                               \n","                                                                 \n"," dense_24 (Dense)            (None, 12)                1212      \n","                                                                 \n"," activation_49 (Activation)  (None, 12)                0         \n","                                                                 \n","=================================================================\n","Total params: 28181464 (107.50 MB)\n","Trainable params: 28160128 (107.42 MB)\n","Non-trainable params: 21336 (83.34 KB)\n","_________________________________________________________________\n","Epoch 1/200\n","787/787 [==============================] - ETA: 0s - loss: 2.0071 - accuracy: 0.3777\n","Epoch 1: val_accuracy improved from -inf to 0.51684, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 2060s 3s/step - loss: 2.0071 - accuracy: 0.3777 - val_loss: 1.3482 - val_accuracy: 0.5168\n","Epoch 2/200\n","787/787 [==============================] - ETA: 0s - loss: 1.5789 - accuracy: 0.4599\n","Epoch 2: val_accuracy improved from 0.51684 to 0.63075, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1944s 2s/step - loss: 1.5789 - accuracy: 0.4599 - val_loss: 1.1075 - val_accuracy: 0.6308\n","Epoch 3/200\n","787/787 [==============================] - ETA: 0s - loss: 1.4396 - accuracy: 0.5180\n","Epoch 3: val_accuracy improved from 0.63075 to 0.65713, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 2031s 3s/step - loss: 1.4396 - accuracy: 0.5180 - val_loss: 1.0575 - val_accuracy: 0.6571\n","Epoch 4/200\n","787/787 [==============================] - ETA: 0s - loss: 1.3919 - accuracy: 0.5372\n","Epoch 4: val_accuracy did not improve from 0.65713\n","787/787 [==============================] - 2016s 3s/step - loss: 1.3919 - accuracy: 0.5372 - val_loss: 1.4473 - val_accuracy: 0.6268\n","Epoch 5/200\n","787/787 [==============================] - ETA: 0s - loss: 1.3710 - accuracy: 0.5479\n","Epoch 5: val_accuracy did not improve from 0.65713\n","787/787 [==============================] - 1950s 2s/step - loss: 1.3710 - accuracy: 0.5479 - val_loss: 1.8759 - val_accuracy: 0.4315\n","Epoch 6/200\n","787/787 [==============================] - ETA: 0s - loss: 1.4051 - accuracy: 0.5312\n","Epoch 6: val_accuracy did not improve from 0.65713\n","787/787 [==============================] - 2161s 3s/step - loss: 1.4051 - accuracy: 0.5312 - val_loss: 1.0855 - val_accuracy: 0.6111\n","Epoch 7/200\n","787/787 [==============================] - ETA: 0s - loss: 1.3237 - accuracy: 0.5620\n","Epoch 7: val_accuracy did not improve from 0.65713\n","787/787 [==============================] - 2330s 3s/step - loss: 1.3237 - accuracy: 0.5620 - val_loss: 1.3076 - val_accuracy: 0.6134\n","Epoch 8/200\n","787/787 [==============================] - ETA: 0s - loss: 1.1952 - accuracy: 0.6137\n","Epoch 8: val_accuracy improved from 0.65713 to 0.70146, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 2549s 3s/step - loss: 1.1952 - accuracy: 0.6137 - val_loss: 0.9057 - val_accuracy: 0.7015\n","Epoch 9/200\n","787/787 [==============================] - ETA: 0s - loss: 1.3422 - accuracy: 0.5569\n","Epoch 9: val_accuracy did not improve from 0.70146\n","787/787 [==============================] - 2547s 3s/step - loss: 1.3422 - accuracy: 0.5569 - val_loss: 1.2814 - val_accuracy: 0.6139\n","Epoch 10/200\n","787/787 [==============================] - ETA: 0s - loss: 1.1977 - accuracy: 0.6104\n","Epoch 10: val_accuracy did not improve from 0.70146\n","787/787 [==============================] - 2450s 3s/step - loss: 1.1977 - accuracy: 0.6104 - val_loss: 1.3693 - val_accuracy: 0.6633\n","Epoch 11/200\n","787/787 [==============================] - ETA: 0s - loss: 1.1578 - accuracy: 0.6270\n","Epoch 11: val_accuracy improved from 0.70146 to 0.71886, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 2523s 3s/step - loss: 1.1578 - accuracy: 0.6270 - val_loss: 0.8828 - val_accuracy: 0.7189\n","Epoch 12/200\n","787/787 [==============================] - ETA: 0s - loss: 1.0210 - accuracy: 0.6732\n","Epoch 12: val_accuracy improved from 0.71886 to 0.79742, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 2016s 3s/step - loss: 1.0210 - accuracy: 0.6732 - val_loss: 0.7293 - val_accuracy: 0.7974\n","Epoch 13/200\n","787/787 [==============================] - ETA: 0s - loss: 0.9982 - accuracy: 0.6856\n","Epoch 13: val_accuracy did not improve from 0.79742\n","787/787 [==============================] - 1885s 2s/step - loss: 0.9982 - accuracy: 0.6856 - val_loss: 1.0175 - val_accuracy: 0.7716\n","Epoch 14/200\n","787/787 [==============================] - ETA: 0s - loss: 0.9768 - accuracy: 0.6944\n","Epoch 14: val_accuracy did not improve from 0.79742\n","787/787 [==============================] - 1859s 2s/step - loss: 0.9768 - accuracy: 0.6944 - val_loss: 1.6568 - val_accuracy: 0.6510\n","Epoch 15/200\n","787/787 [==============================] - ETA: 0s - loss: 0.9458 - accuracy: 0.7040\n","Epoch 15: val_accuracy did not improve from 0.79742\n","787/787 [==============================] - 1873s 2s/step - loss: 0.9458 - accuracy: 0.7040 - val_loss: 0.8655 - val_accuracy: 0.7727\n","Epoch 16/200\n","787/787 [==============================] - ETA: 0s - loss: 0.7684 - accuracy: 0.7667\n","Epoch 16: val_accuracy did not improve from 0.79742\n","787/787 [==============================] - 1909s 2s/step - loss: 0.7684 - accuracy: 0.7667 - val_loss: 0.7004 - val_accuracy: 0.7868\n","Epoch 17/200\n","787/787 [==============================] - ETA: 0s - loss: 0.8291 - accuracy: 0.7482\n","Epoch 17: val_accuracy improved from 0.79742 to 0.85802, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1837s 2s/step - loss: 0.8291 - accuracy: 0.7482 - val_loss: 0.6286 - val_accuracy: 0.8580\n","Epoch 18/200\n","787/787 [==============================] - ETA: 0s - loss: 0.7243 - accuracy: 0.7853\n","Epoch 18: val_accuracy did not improve from 0.85802\n","787/787 [==============================] - 1877s 2s/step - loss: 0.7243 - accuracy: 0.7853 - val_loss: 0.5015 - val_accuracy: 0.8345\n","Epoch 19/200\n","787/787 [==============================] - ETA: 0s - loss: 0.7403 - accuracy: 0.7776\n","Epoch 19: val_accuracy did not improve from 0.85802\n","787/787 [==============================] - 1845s 2s/step - loss: 0.7403 - accuracy: 0.7776 - val_loss: 3.4565 - val_accuracy: 0.5808\n","Epoch 20/200\n","787/787 [==============================] - ETA: 0s - loss: 0.7801 - accuracy: 0.7653\n","Epoch 20: val_accuracy improved from 0.85802 to 0.88552, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1873s 2s/step - loss: 0.7801 - accuracy: 0.7653 - val_loss: 0.4185 - val_accuracy: 0.8855\n","Epoch 21/200\n","787/787 [==============================] - ETA: 0s - loss: 0.6509 - accuracy: 0.8095\n","Epoch 21: val_accuracy did not improve from 0.88552\n","787/787 [==============================] - 1892s 2s/step - loss: 0.6509 - accuracy: 0.8095 - val_loss: 0.4169 - val_accuracy: 0.8743\n","Epoch 22/200\n","787/787 [==============================] - ETA: 0s - loss: 0.6228 - accuracy: 0.8167\n","Epoch 22: val_accuracy did not improve from 0.88552\n","787/787 [==============================] - 1813s 2s/step - loss: 0.6228 - accuracy: 0.8167 - val_loss: 0.5231 - val_accuracy: 0.8642\n","Epoch 23/200\n","787/787 [==============================] - ETA: 0s - loss: 0.8096 - accuracy: 0.7585\n","Epoch 23: val_accuracy did not improve from 0.88552\n","787/787 [==============================] - 1918s 2s/step - loss: 0.8096 - accuracy: 0.7585 - val_loss: 0.5378 - val_accuracy: 0.8513\n","Epoch 24/200\n","787/787 [==============================] - ETA: 0s - loss: 0.6406 - accuracy: 0.8131\n","Epoch 24: val_accuracy improved from 0.88552 to 0.90404, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1841s 2s/step - loss: 0.6406 - accuracy: 0.8131 - val_loss: 0.3499 - val_accuracy: 0.9040\n","Epoch 25/200\n","787/787 [==============================] - ETA: 0s - loss: 0.5495 - accuracy: 0.8342\n","Epoch 25: val_accuracy improved from 0.90404 to 0.91246, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1871s 2s/step - loss: 0.5495 - accuracy: 0.8342 - val_loss: 0.2956 - val_accuracy: 0.9125\n","Epoch 26/200\n","787/787 [==============================] - ETA: 0s - loss: 0.7490 - accuracy: 0.7777\n","Epoch 26: val_accuracy did not improve from 0.91246\n","787/787 [==============================] - 1886s 2s/step - loss: 0.7490 - accuracy: 0.7777 - val_loss: 0.8565 - val_accuracy: 0.7851\n","Epoch 27/200\n","787/787 [==============================] - ETA: 0s - loss: 0.5449 - accuracy: 0.8405\n","Epoch 27: val_accuracy improved from 0.91246 to 0.91639, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1940s 2s/step - loss: 0.5449 - accuracy: 0.8405 - val_loss: 0.3053 - val_accuracy: 0.9164\n","Epoch 28/200\n","787/787 [==============================] - ETA: 0s - loss: 0.4464 - accuracy: 0.8716\n","Epoch 28: val_accuracy improved from 0.91639 to 0.93042, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1944s 2s/step - loss: 0.4464 - accuracy: 0.8716 - val_loss: 0.2493 - val_accuracy: 0.9304\n","Epoch 29/200\n","787/787 [==============================] - ETA: 0s - loss: 0.4194 - accuracy: 0.8803\n","Epoch 29: val_accuracy did not improve from 0.93042\n","787/787 [==============================] - 1928s 2s/step - loss: 0.4194 - accuracy: 0.8803 - val_loss: 0.2784 - val_accuracy: 0.9203\n","Epoch 30/200\n","787/787 [==============================] - ETA: 0s - loss: 0.4232 - accuracy: 0.8776\n","Epoch 30: val_accuracy did not improve from 0.93042\n","787/787 [==============================] - 1996s 3s/step - loss: 0.4232 - accuracy: 0.8776 - val_loss: 0.2998 - val_accuracy: 0.9203\n","Epoch 31/200\n","787/787 [==============================] - ETA: 0s - loss: 0.5662 - accuracy: 0.8379\n","Epoch 31: val_accuracy did not improve from 0.93042\n","787/787 [==============================] - 2036s 3s/step - loss: 0.5662 - accuracy: 0.8379 - val_loss: 0.3013 - val_accuracy: 0.9248\n","Epoch 32/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3873 - accuracy: 0.8902\n","Epoch 32: val_accuracy did not improve from 0.93042\n","787/787 [==============================] - 2302s 3s/step - loss: 0.3873 - accuracy: 0.8902 - val_loss: 0.3296 - val_accuracy: 0.9097\n","Epoch 33/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3500 - accuracy: 0.9017\n","Epoch 33: val_accuracy improved from 0.93042 to 0.93939, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 2439s 3s/step - loss: 0.3500 - accuracy: 0.9017 - val_loss: 0.2713 - val_accuracy: 0.9394\n","Epoch 34/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.9109\n","Epoch 34: val_accuracy improved from 0.93939 to 0.94949, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 2089s 3s/step - loss: 0.3230 - accuracy: 0.9109 - val_loss: 0.2418 - val_accuracy: 0.9495\n","Epoch 35/200\n","787/787 [==============================] - ETA: 0s - loss: 0.4466 - accuracy: 0.8753\n","Epoch 35: val_accuracy did not improve from 0.94949\n","787/787 [==============================] - 1961s 2s/step - loss: 0.4466 - accuracy: 0.8753 - val_loss: 0.6137 - val_accuracy: 0.8704\n","Epoch 36/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3809 - accuracy: 0.8920\n","Epoch 36: val_accuracy improved from 0.94949 to 0.95174, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1654s 2s/step - loss: 0.3809 - accuracy: 0.8920 - val_loss: 0.2042 - val_accuracy: 0.9517\n","Epoch 37/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.9125\n","Epoch 37: val_accuracy did not improve from 0.95174\n","787/787 [==============================] - 1696s 2s/step - loss: 0.3111 - accuracy: 0.9125 - val_loss: 0.2574 - val_accuracy: 0.9321\n","Epoch 38/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3235 - accuracy: 0.9095\n","Epoch 38: val_accuracy improved from 0.95174 to 0.95511, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1765s 2s/step - loss: 0.3235 - accuracy: 0.9095 - val_loss: 0.1994 - val_accuracy: 0.9551\n","Epoch 39/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2810 - accuracy: 0.9197\n","Epoch 39: val_accuracy did not improve from 0.95511\n","787/787 [==============================] - 1624s 2s/step - loss: 0.2810 - accuracy: 0.9197 - val_loss: 0.3446 - val_accuracy: 0.9270\n","Epoch 40/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3074 - accuracy: 0.9167\n","Epoch 40: val_accuracy did not improve from 0.95511\n","787/787 [==============================] - 1643s 2s/step - loss: 0.3074 - accuracy: 0.9167 - val_loss: 0.1977 - val_accuracy: 0.9439\n","Epoch 41/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.9218\n","Epoch 41: val_accuracy did not improve from 0.95511\n","787/787 [==============================] - 1622s 2s/step - loss: 0.2841 - accuracy: 0.9218 - val_loss: 0.3026 - val_accuracy: 0.9366\n","Epoch 42/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3339 - accuracy: 0.9062\n","Epoch 42: val_accuracy did not improve from 0.95511\n","787/787 [==============================] - 1549s 2s/step - loss: 0.3339 - accuracy: 0.9062 - val_loss: 0.2352 - val_accuracy: 0.9501\n","Epoch 43/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.9261\n","Epoch 43: val_accuracy did not improve from 0.95511\n","787/787 [==============================] - 1556s 2s/step - loss: 0.2873 - accuracy: 0.9261 - val_loss: 0.2533 - val_accuracy: 0.9416\n","Epoch 44/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3152 - accuracy: 0.9096\n","Epoch 44: val_accuracy improved from 0.95511 to 0.95735, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1555s 2s/step - loss: 0.3152 - accuracy: 0.9096 - val_loss: 0.1654 - val_accuracy: 0.9574\n","Epoch 45/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2162 - accuracy: 0.9414\n","Epoch 45: val_accuracy did not improve from 0.95735\n","787/787 [==============================] - 1601s 2s/step - loss: 0.2162 - accuracy: 0.9414 - val_loss: 0.4397 - val_accuracy: 0.9097\n","Epoch 46/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.9194\n","Epoch 46: val_accuracy did not improve from 0.95735\n","787/787 [==============================] - 1577s 2s/step - loss: 0.2990 - accuracy: 0.9194 - val_loss: 0.3414 - val_accuracy: 0.9231\n","Epoch 47/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.9193\n","Epoch 47: val_accuracy did not improve from 0.95735\n","787/787 [==============================] - 1587s 2s/step - loss: 0.3020 - accuracy: 0.9193 - val_loss: 0.6458 - val_accuracy: 0.8648\n","Epoch 48/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.9185\n","Epoch 48: val_accuracy did not improve from 0.95735\n","787/787 [==============================] - 1618s 2s/step - loss: 0.2955 - accuracy: 0.9185 - val_loss: 0.1979 - val_accuracy: 0.9545\n","Epoch 49/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.9471\n","Epoch 49: val_accuracy did not improve from 0.95735\n","787/787 [==============================] - 1597s 2s/step - loss: 0.2100 - accuracy: 0.9471 - val_loss: 0.2101 - val_accuracy: 0.9467\n","Epoch 50/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3194 - accuracy: 0.9095\n","Epoch 50: val_accuracy did not improve from 0.95735\n","787/787 [==============================] - 1571s 2s/step - loss: 0.3194 - accuracy: 0.9095 - val_loss: 0.3660 - val_accuracy: 0.9310\n","Epoch 51/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3017 - accuracy: 0.9161\n","Epoch 51: val_accuracy improved from 0.95735 to 0.95903, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1567s 2s/step - loss: 0.3017 - accuracy: 0.9161 - val_loss: 0.1867 - val_accuracy: 0.9590\n","Epoch 52/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2256 - accuracy: 0.9411\n","Epoch 52: val_accuracy did not improve from 0.95903\n","787/787 [==============================] - 1643s 2s/step - loss: 0.2256 - accuracy: 0.9411 - val_loss: 0.2083 - val_accuracy: 0.9473\n","Epoch 53/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 0.9371\n","Epoch 53: val_accuracy improved from 0.95903 to 0.96465, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1642s 2s/step - loss: 0.2274 - accuracy: 0.9371 - val_loss: 0.1483 - val_accuracy: 0.9646\n","Epoch 54/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9486\n","Epoch 54: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1670s 2s/step - loss: 0.1865 - accuracy: 0.9486 - val_loss: 0.1904 - val_accuracy: 0.9501\n","Epoch 55/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.9324\n","Epoch 55: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1614s 2s/step - loss: 0.2575 - accuracy: 0.9324 - val_loss: 0.2411 - val_accuracy: 0.9400\n","Epoch 56/200\n","787/787 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.9202\n","Epoch 56: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1698s 2s/step - loss: 0.3221 - accuracy: 0.9202 - val_loss: 0.2801 - val_accuracy: 0.9355\n","Epoch 57/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2281 - accuracy: 0.9387\n","Epoch 57: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1763s 2s/step - loss: 0.2281 - accuracy: 0.9387 - val_loss: 0.1812 - val_accuracy: 0.9557\n","Epoch 58/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2131 - accuracy: 0.9418\n","Epoch 58: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1537s 2s/step - loss: 0.2131 - accuracy: 0.9418 - val_loss: 0.2326 - val_accuracy: 0.9501\n","Epoch 59/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1984 - accuracy: 0.9453\n","Epoch 59: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1518s 2s/step - loss: 0.1984 - accuracy: 0.9453 - val_loss: 0.2923 - val_accuracy: 0.9529\n","Epoch 60/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9387\n","Epoch 60: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1593s 2s/step - loss: 0.2363 - accuracy: 0.9387 - val_loss: 0.2066 - val_accuracy: 0.9579\n","Epoch 61/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9436\n","Epoch 61: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1564s 2s/step - loss: 0.1987 - accuracy: 0.9436 - val_loss: 0.2281 - val_accuracy: 0.9551\n","Epoch 62/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9495\n","Epoch 62: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1558s 2s/step - loss: 0.1939 - accuracy: 0.9495 - val_loss: 0.1969 - val_accuracy: 0.9585\n","Epoch 63/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1395 - accuracy: 0.9612\n","Epoch 63: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1538s 2s/step - loss: 0.1395 - accuracy: 0.9612 - val_loss: 0.2603 - val_accuracy: 0.9439\n","Epoch 64/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.9486\n","Epoch 64: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1572s 2s/step - loss: 0.1982 - accuracy: 0.9486 - val_loss: 0.2090 - val_accuracy: 0.9590\n","Epoch 65/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1690 - accuracy: 0.9554\n","Epoch 65: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1550s 2s/step - loss: 0.1690 - accuracy: 0.9554 - val_loss: 0.5576 - val_accuracy: 0.9012\n","Epoch 66/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2037 - accuracy: 0.9451\n","Epoch 66: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1557s 2s/step - loss: 0.2037 - accuracy: 0.9451 - val_loss: 0.3052 - val_accuracy: 0.9534\n","Epoch 67/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9619\n","Epoch 67: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1558s 2s/step - loss: 0.1573 - accuracy: 0.9619 - val_loss: 0.3886 - val_accuracy: 0.9523\n","Epoch 68/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9559\n","Epoch 68: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1539s 2s/step - loss: 0.1646 - accuracy: 0.9559 - val_loss: 0.2760 - val_accuracy: 0.9579\n","Epoch 69/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9462\n","Epoch 69: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1557s 2s/step - loss: 0.2092 - accuracy: 0.9462 - val_loss: 0.3197 - val_accuracy: 0.9596\n","Epoch 70/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9570\n","Epoch 70: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1529s 2s/step - loss: 0.1656 - accuracy: 0.9570 - val_loss: 0.1888 - val_accuracy: 0.9574\n","Epoch 71/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9504\n","Epoch 71: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1571s 2s/step - loss: 0.1934 - accuracy: 0.9504 - val_loss: 0.3981 - val_accuracy: 0.9259\n","Epoch 72/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9518\n","Epoch 72: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1568s 2s/step - loss: 0.1865 - accuracy: 0.9518 - val_loss: 0.1649 - val_accuracy: 0.9607\n","Epoch 73/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1589 - accuracy: 0.9570\n","Epoch 73: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1558s 2s/step - loss: 0.1589 - accuracy: 0.9570 - val_loss: 0.1867 - val_accuracy: 0.9557\n","Epoch 74/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9450\n","Epoch 74: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1582s 2s/step - loss: 0.2050 - accuracy: 0.9450 - val_loss: 0.2043 - val_accuracy: 0.9529\n","Epoch 75/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9500\n","Epoch 75: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1546s 2s/step - loss: 0.1815 - accuracy: 0.9500 - val_loss: 0.1814 - val_accuracy: 0.9590\n","Epoch 76/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9418\n","Epoch 76: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1570s 2s/step - loss: 0.2125 - accuracy: 0.9418 - val_loss: 0.2322 - val_accuracy: 0.9545\n","Epoch 77/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.9620\n","Epoch 77: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1554s 2s/step - loss: 0.1522 - accuracy: 0.9620 - val_loss: 0.2424 - val_accuracy: 0.9461\n","Epoch 78/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.9554\n","Epoch 78: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1560s 2s/step - loss: 0.1647 - accuracy: 0.9554 - val_loss: 0.1948 - val_accuracy: 0.9579\n","Epoch 79/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.9582\n","Epoch 79: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1552s 2s/step - loss: 0.1501 - accuracy: 0.9582 - val_loss: 0.1623 - val_accuracy: 0.9618\n","Epoch 80/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9636\n","Epoch 80: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1529s 2s/step - loss: 0.1300 - accuracy: 0.9636 - val_loss: 0.2143 - val_accuracy: 0.9529\n","Epoch 81/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9691\n","Epoch 81: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1536s 2s/step - loss: 0.1126 - accuracy: 0.9691 - val_loss: 0.1805 - val_accuracy: 0.9602\n","Epoch 82/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9615\n","Epoch 82: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1485s 2s/step - loss: 0.1473 - accuracy: 0.9615 - val_loss: 0.6428 - val_accuracy: 0.8709\n","Epoch 83/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9626\n","Epoch 83: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1528s 2s/step - loss: 0.1439 - accuracy: 0.9626 - val_loss: 0.2276 - val_accuracy: 0.9489\n","Epoch 84/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9638\n","Epoch 84: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1522s 2s/step - loss: 0.1424 - accuracy: 0.9638 - val_loss: 0.6324 - val_accuracy: 0.8805\n","Epoch 85/200\n","787/787 [==============================] - ETA: 0s - loss: 0.2146 - accuracy: 0.9493\n","Epoch 85: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1503s 2s/step - loss: 0.2146 - accuracy: 0.9493 - val_loss: 0.2284 - val_accuracy: 0.9506\n","Epoch 86/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9697\n","Epoch 86: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1516s 2s/step - loss: 0.1128 - accuracy: 0.9697 - val_loss: 0.2312 - val_accuracy: 0.9590\n","Epoch 87/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1191 - accuracy: 0.9690\n","Epoch 87: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1480s 2s/step - loss: 0.1191 - accuracy: 0.9690 - val_loss: 0.2083 - val_accuracy: 0.9590\n","Epoch 88/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9704\n","Epoch 88: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1547s 2s/step - loss: 0.1108 - accuracy: 0.9704 - val_loss: 0.1859 - val_accuracy: 0.9607\n","Epoch 89/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9727\n","Epoch 89: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1522s 2s/step - loss: 0.1047 - accuracy: 0.9727 - val_loss: 0.1931 - val_accuracy: 0.9602\n","Epoch 90/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9657\n","Epoch 90: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1511s 2s/step - loss: 0.1336 - accuracy: 0.9657 - val_loss: 0.2092 - val_accuracy: 0.9467\n","Epoch 91/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9695\n","Epoch 91: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1499s 2s/step - loss: 0.1210 - accuracy: 0.9695 - val_loss: 0.2024 - val_accuracy: 0.9602\n","Epoch 92/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9657\n","Epoch 92: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1511s 2s/step - loss: 0.1424 - accuracy: 0.9657 - val_loss: 0.1706 - val_accuracy: 0.9613\n","Epoch 93/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9653\n","Epoch 93: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1571s 2s/step - loss: 0.1284 - accuracy: 0.9653 - val_loss: 0.2074 - val_accuracy: 0.9461\n","Epoch 94/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.9708\n","Epoch 94: val_accuracy did not improve from 0.96465\n","787/787 [==============================] - 1500s 2s/step - loss: 0.1159 - accuracy: 0.9708 - val_loss: 0.2649 - val_accuracy: 0.9506\n","Epoch 95/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9723\n","Epoch 95: val_accuracy improved from 0.96465 to 0.96970, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch200.h5\n","787/787 [==============================] - 1548s 2s/step - loss: 0.1019 - accuracy: 0.9723 - val_loss: 0.1576 - val_accuracy: 0.9697\n","Epoch 96/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9699\n","Epoch 96: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1519s 2s/step - loss: 0.1231 - accuracy: 0.9699 - val_loss: 0.2301 - val_accuracy: 0.9574\n","Epoch 97/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9668\n","Epoch 97: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1500s 2s/step - loss: 0.1247 - accuracy: 0.9668 - val_loss: 0.2301 - val_accuracy: 0.9630\n","Epoch 98/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9506\n","Epoch 98: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1497s 2s/step - loss: 0.1815 - accuracy: 0.9506 - val_loss: 0.7025 - val_accuracy: 0.9315\n","Epoch 99/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9625\n","Epoch 99: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1494s 2s/step - loss: 0.1363 - accuracy: 0.9625 - val_loss: 0.4042 - val_accuracy: 0.9501\n","Epoch 100/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9658\n","Epoch 100: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1520s 2s/step - loss: 0.1239 - accuracy: 0.9658 - val_loss: 0.2588 - val_accuracy: 0.9574\n","Epoch 101/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0965 - accuracy: 0.9764\n","Epoch 101: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1488s 2s/step - loss: 0.0965 - accuracy: 0.9764 - val_loss: 0.2163 - val_accuracy: 0.9574\n","Epoch 102/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9736\n","Epoch 102: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1506s 2s/step - loss: 0.1033 - accuracy: 0.9736 - val_loss: 0.2087 - val_accuracy: 0.9585\n","Epoch 103/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9751\n","Epoch 103: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1526s 2s/step - loss: 0.0984 - accuracy: 0.9751 - val_loss: 0.1897 - val_accuracy: 0.9562\n","Epoch 104/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9819\n","Epoch 104: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1490s 2s/step - loss: 0.0758 - accuracy: 0.9819 - val_loss: 0.3379 - val_accuracy: 0.9523\n","Epoch 105/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9760\n","Epoch 105: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1510s 2s/step - loss: 0.0890 - accuracy: 0.9760 - val_loss: 0.3561 - val_accuracy: 0.9658\n","Epoch 106/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9727\n","Epoch 106: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1490s 2s/step - loss: 0.0943 - accuracy: 0.9727 - val_loss: 0.2912 - val_accuracy: 0.9607\n","Epoch 107/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9720\n","Epoch 107: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1082s 1s/step - loss: 0.0987 - accuracy: 0.9720 - val_loss: 0.8721 - val_accuracy: 0.8681\n","Epoch 108/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9666\n","Epoch 108: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1042s 1s/step - loss: 0.1292 - accuracy: 0.9666 - val_loss: 0.3050 - val_accuracy: 0.9517\n","Epoch 109/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9789\n","Epoch 109: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1037s 1s/step - loss: 0.0842 - accuracy: 0.9789 - val_loss: 0.2386 - val_accuracy: 0.9523\n","Epoch 110/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9758\n","Epoch 110: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1036s 1s/step - loss: 0.1017 - accuracy: 0.9758 - val_loss: 0.1726 - val_accuracy: 0.9652\n","Epoch 111/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9800\n","Epoch 111: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1012s 1s/step - loss: 0.0836 - accuracy: 0.9800 - val_loss: 0.1710 - val_accuracy: 0.9630\n","Epoch 112/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9778\n","Epoch 112: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1033s 1s/step - loss: 0.0904 - accuracy: 0.9778 - val_loss: 0.2379 - val_accuracy: 0.9590\n","Epoch 113/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1404 - accuracy: 0.9633\n","Epoch 113: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1011s 1s/step - loss: 0.1404 - accuracy: 0.9633 - val_loss: 0.3346 - val_accuracy: 0.9585\n","Epoch 114/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9780\n","Epoch 114: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1007s 1s/step - loss: 0.0829 - accuracy: 0.9780 - val_loss: 0.3697 - val_accuracy: 0.9646\n","Epoch 115/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9776\n","Epoch 115: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 997s 1s/step - loss: 0.0890 - accuracy: 0.9776 - val_loss: 0.2176 - val_accuracy: 0.9557\n","Epoch 116/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9709\n","Epoch 116: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1016s 1s/step - loss: 0.1145 - accuracy: 0.9709 - val_loss: 1.1932 - val_accuracy: 0.7722\n","Epoch 117/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9575\n","Epoch 117: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1037s 1s/step - loss: 0.1631 - accuracy: 0.9575 - val_loss: 0.1633 - val_accuracy: 0.9641\n","Epoch 118/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9808\n","Epoch 118: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1004s 1s/step - loss: 0.0736 - accuracy: 0.9808 - val_loss: 0.1651 - val_accuracy: 0.9675\n","Epoch 119/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9731\n","Epoch 119: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1007s 1s/step - loss: 0.1041 - accuracy: 0.9731 - val_loss: 0.5108 - val_accuracy: 0.9428\n","Epoch 120/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1752 - accuracy: 0.9540\n","Epoch 120: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1006s 1s/step - loss: 0.1752 - accuracy: 0.9540 - val_loss: 0.2301 - val_accuracy: 0.9495\n","Epoch 121/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1421 - accuracy: 0.9596\n","Epoch 121: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1031s 1s/step - loss: 0.1421 - accuracy: 0.9596 - val_loss: 0.2434 - val_accuracy: 0.9551\n","Epoch 122/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9706\n","Epoch 122: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1004s 1s/step - loss: 0.1094 - accuracy: 0.9706 - val_loss: 0.2055 - val_accuracy: 0.9635\n","Epoch 123/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 0.9756\n","Epoch 123: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1031s 1s/step - loss: 0.0975 - accuracy: 0.9756 - val_loss: 0.3731 - val_accuracy: 0.9529\n","Epoch 124/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9783\n","Epoch 124: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1036s 1s/step - loss: 0.0864 - accuracy: 0.9783 - val_loss: 0.2461 - val_accuracy: 0.9646\n","Epoch 125/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9795\n","Epoch 125: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1026s 1s/step - loss: 0.0760 - accuracy: 0.9795 - val_loss: 0.2672 - val_accuracy: 0.9641\n","Epoch 126/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9819\n","Epoch 126: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1030s 1s/step - loss: 0.0733 - accuracy: 0.9819 - val_loss: 0.2585 - val_accuracy: 0.9478\n","Epoch 127/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9666\n","Epoch 127: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1022s 1s/step - loss: 0.1321 - accuracy: 0.9666 - val_loss: 0.5414 - val_accuracy: 0.9450\n","Epoch 128/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9720\n","Epoch 128: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1031s 1s/step - loss: 0.1123 - accuracy: 0.9720 - val_loss: 0.4239 - val_accuracy: 0.9473\n","Epoch 129/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9832\n","Epoch 129: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1058s 1s/step - loss: 0.0682 - accuracy: 0.9832 - val_loss: 0.2211 - val_accuracy: 0.9646\n","Epoch 130/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9808\n","Epoch 130: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 995s 1s/step - loss: 0.0789 - accuracy: 0.9808 - val_loss: 0.2842 - val_accuracy: 0.9551\n","Epoch 131/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9854\n","Epoch 131: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1022s 1s/step - loss: 0.0631 - accuracy: 0.9854 - val_loss: 0.2710 - val_accuracy: 0.9590\n","Epoch 132/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9783\n","Epoch 132: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1029s 1s/step - loss: 0.0855 - accuracy: 0.9783 - val_loss: 0.2177 - val_accuracy: 0.9602\n","Epoch 133/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9766\n","Epoch 133: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1010s 1s/step - loss: 0.0874 - accuracy: 0.9766 - val_loss: 0.3301 - val_accuracy: 0.9579\n","Epoch 134/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9774\n","Epoch 134: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1020s 1s/step - loss: 0.0850 - accuracy: 0.9774 - val_loss: 0.4815 - val_accuracy: 0.9641\n","Epoch 135/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9813\n","Epoch 135: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1019s 1s/step - loss: 0.0685 - accuracy: 0.9813 - val_loss: 0.5612 - val_accuracy: 0.8979\n","Epoch 136/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9811\n","Epoch 136: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1035s 1s/step - loss: 0.0786 - accuracy: 0.9811 - val_loss: 0.3492 - val_accuracy: 0.9478\n","Epoch 137/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9596\n","Epoch 137: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1024s 1s/step - loss: 0.1760 - accuracy: 0.9596 - val_loss: 0.6754 - val_accuracy: 0.9259\n","Epoch 138/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9772\n","Epoch 138: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1030s 1s/step - loss: 0.0835 - accuracy: 0.9772 - val_loss: 0.1901 - val_accuracy: 0.9613\n","Epoch 139/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1651 - accuracy: 0.9567\n","Epoch 139: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1035s 1s/step - loss: 0.1651 - accuracy: 0.9567 - val_loss: 2.8622 - val_accuracy: 0.9085\n","Epoch 140/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9643\n","Epoch 140: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1029s 1s/step - loss: 0.1344 - accuracy: 0.9643 - val_loss: 3.4243 - val_accuracy: 0.9097\n","Epoch 141/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9769\n","Epoch 141: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1059s 1s/step - loss: 0.0935 - accuracy: 0.9769 - val_loss: 4.8475 - val_accuracy: 0.9046\n","Epoch 142/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9760\n","Epoch 142: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1032s 1s/step - loss: 0.0916 - accuracy: 0.9760 - val_loss: 1.2610 - val_accuracy: 0.9332\n","Epoch 143/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9771\n","Epoch 143: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1035s 1s/step - loss: 0.0857 - accuracy: 0.9771 - val_loss: 2.9518 - val_accuracy: 0.9416\n","Epoch 144/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9813\n","Epoch 144: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1023s 1s/step - loss: 0.0694 - accuracy: 0.9813 - val_loss: 1.0985 - val_accuracy: 0.9293\n","Epoch 145/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9818\n","Epoch 145: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1010s 1s/step - loss: 0.0699 - accuracy: 0.9818 - val_loss: 2.8885 - val_accuracy: 0.9248\n","Epoch 146/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9860\n","Epoch 146: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1035s 1s/step - loss: 0.0522 - accuracy: 0.9860 - val_loss: 0.8302 - val_accuracy: 0.9360\n","Epoch 147/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9841\n","Epoch 147: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 998s 1s/step - loss: 0.0613 - accuracy: 0.9841 - val_loss: 0.3016 - val_accuracy: 0.9512\n","Epoch 148/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9832\n","Epoch 148: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1040s 1s/step - loss: 0.0621 - accuracy: 0.9832 - val_loss: 0.2138 - val_accuracy: 0.9635\n","Epoch 149/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9870\n","Epoch 149: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1027s 1s/step - loss: 0.0531 - accuracy: 0.9870 - val_loss: 0.2668 - val_accuracy: 0.9574\n","Epoch 150/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9767\n","Epoch 150: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1011s 1s/step - loss: 0.0957 - accuracy: 0.9767 - val_loss: 0.2706 - val_accuracy: 0.9613\n","Epoch 151/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9757\n","Epoch 151: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1017s 1s/step - loss: 0.0897 - accuracy: 0.9757 - val_loss: 0.4003 - val_accuracy: 0.9562\n","Epoch 152/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9747\n","Epoch 152: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1024s 1s/step - loss: 0.0931 - accuracy: 0.9747 - val_loss: 0.2392 - val_accuracy: 0.9596\n","Epoch 153/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9846\n","Epoch 153: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1042s 1s/step - loss: 0.0571 - accuracy: 0.9846 - val_loss: 0.2931 - val_accuracy: 0.9658\n","Epoch 154/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9809\n","Epoch 154: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1025s 1s/step - loss: 0.0696 - accuracy: 0.9809 - val_loss: 0.3379 - val_accuracy: 0.9557\n","Epoch 155/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9808\n","Epoch 155: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1048s 1s/step - loss: 0.0753 - accuracy: 0.9808 - val_loss: 0.4276 - val_accuracy: 0.9602\n","Epoch 156/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.9774\n","Epoch 156: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1043s 1s/step - loss: 0.0981 - accuracy: 0.9774 - val_loss: 0.4421 - val_accuracy: 0.9568\n","Epoch 157/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9830\n","Epoch 157: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1036s 1s/step - loss: 0.0619 - accuracy: 0.9830 - val_loss: 0.2255 - val_accuracy: 0.9675\n","Epoch 158/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9892\n","Epoch 158: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1037s 1s/step - loss: 0.0369 - accuracy: 0.9892 - val_loss: 0.4861 - val_accuracy: 0.9557\n","Epoch 159/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9828\n","Epoch 159: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1011s 1s/step - loss: 0.0714 - accuracy: 0.9828 - val_loss: 0.2923 - val_accuracy: 0.9551\n","Epoch 160/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9816\n","Epoch 160: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1055s 1s/step - loss: 0.0702 - accuracy: 0.9816 - val_loss: 0.3975 - val_accuracy: 0.9540\n","Epoch 161/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9819\n","Epoch 161: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1026s 1s/step - loss: 0.0823 - accuracy: 0.9819 - val_loss: 1.4683 - val_accuracy: 0.9439\n","Epoch 162/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9840\n","Epoch 162: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1023s 1s/step - loss: 0.0669 - accuracy: 0.9840 - val_loss: 0.3181 - val_accuracy: 0.9618\n","Epoch 163/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9788\n","Epoch 163: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1044s 1s/step - loss: 0.0869 - accuracy: 0.9788 - val_loss: 0.2944 - val_accuracy: 0.9574\n","Epoch 164/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9807\n","Epoch 164: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1064s 1s/step - loss: 0.0728 - accuracy: 0.9807 - val_loss: 0.2418 - val_accuracy: 0.9613\n","Epoch 165/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9842\n","Epoch 165: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1058s 1s/step - loss: 0.0637 - accuracy: 0.9842 - val_loss: 0.5472 - val_accuracy: 0.9489\n","Epoch 166/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9891\n","Epoch 166: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 995s 1s/step - loss: 0.0425 - accuracy: 0.9891 - val_loss: 0.5558 - val_accuracy: 0.9512\n","Epoch 167/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9755\n","Epoch 167: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1073s 1s/step - loss: 0.0985 - accuracy: 0.9755 - val_loss: 0.8934 - val_accuracy: 0.9388\n","Epoch 168/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9781\n","Epoch 168: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1045s 1s/step - loss: 0.0824 - accuracy: 0.9781 - val_loss: 0.4730 - val_accuracy: 0.9315\n","Epoch 169/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9818\n","Epoch 169: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1053s 1s/step - loss: 0.0774 - accuracy: 0.9818 - val_loss: 0.4802 - val_accuracy: 0.9259\n","Epoch 170/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9835\n","Epoch 170: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1126s 1s/step - loss: 0.0655 - accuracy: 0.9835 - val_loss: 1.0773 - val_accuracy: 0.9214\n","Epoch 171/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9859\n","Epoch 171: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1068s 1s/step - loss: 0.0702 - accuracy: 0.9859 - val_loss: 0.5835 - val_accuracy: 0.9371\n","Epoch 172/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9842\n","Epoch 172: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1101s 1s/step - loss: 0.0664 - accuracy: 0.9842 - val_loss: 0.3225 - val_accuracy: 0.9602\n","Epoch 173/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9849\n","Epoch 173: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1052s 1s/step - loss: 0.0558 - accuracy: 0.9849 - val_loss: 0.4095 - val_accuracy: 0.9523\n","Epoch 174/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9877\n","Epoch 174: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1040s 1s/step - loss: 0.0534 - accuracy: 0.9877 - val_loss: 0.5298 - val_accuracy: 0.9534\n","Epoch 175/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9896\n","Epoch 175: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1085s 1s/step - loss: 0.0439 - accuracy: 0.9896 - val_loss: 0.6064 - val_accuracy: 0.9484\n","Epoch 176/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9883\n","Epoch 176: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1069s 1s/step - loss: 0.0476 - accuracy: 0.9883 - val_loss: 0.2846 - val_accuracy: 0.9618\n","Epoch 177/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9794\n","Epoch 177: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1112s 1s/step - loss: 0.0877 - accuracy: 0.9794 - val_loss: 0.4218 - val_accuracy: 0.9371\n","Epoch 178/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9845\n","Epoch 178: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1088s 1s/step - loss: 0.0669 - accuracy: 0.9845 - val_loss: 0.2575 - val_accuracy: 0.9585\n","Epoch 179/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9892\n","Epoch 179: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1099s 1s/step - loss: 0.0493 - accuracy: 0.9892 - val_loss: 0.2192 - val_accuracy: 0.9613\n","Epoch 180/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9882\n","Epoch 180: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1038s 1s/step - loss: 0.0454 - accuracy: 0.9882 - val_loss: 0.2318 - val_accuracy: 0.9646\n","Epoch 181/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9849\n","Epoch 181: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1043s 1s/step - loss: 0.0653 - accuracy: 0.9849 - val_loss: 0.2184 - val_accuracy: 0.9686\n","Epoch 182/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9833\n","Epoch 182: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1077s 1s/step - loss: 0.0702 - accuracy: 0.9833 - val_loss: 0.9946 - val_accuracy: 0.9338\n","Epoch 183/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9830\n","Epoch 183: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1008s 1s/step - loss: 0.0794 - accuracy: 0.9830 - val_loss: 3.1921 - val_accuracy: 0.8917\n","Epoch 184/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9869\n","Epoch 184: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1069s 1s/step - loss: 0.0511 - accuracy: 0.9869 - val_loss: 0.8066 - val_accuracy: 0.9461\n","Epoch 185/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9767\n","Epoch 185: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1061s 1s/step - loss: 0.0880 - accuracy: 0.9767 - val_loss: 0.2860 - val_accuracy: 0.9551\n","Epoch 186/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9861\n","Epoch 186: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1052s 1s/step - loss: 0.0571 - accuracy: 0.9861 - val_loss: 0.9656 - val_accuracy: 0.8631\n","Epoch 187/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9865\n","Epoch 187: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1077s 1s/step - loss: 0.0610 - accuracy: 0.9865 - val_loss: 1.0561 - val_accuracy: 0.9371\n","Epoch 188/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9900\n","Epoch 188: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1036s 1s/step - loss: 0.0401 - accuracy: 0.9900 - val_loss: 0.2953 - val_accuracy: 0.9512\n","Epoch 189/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9870\n","Epoch 189: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1087s 1s/step - loss: 0.0490 - accuracy: 0.9870 - val_loss: 0.2455 - val_accuracy: 0.9596\n","Epoch 190/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9853\n","Epoch 190: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1044s 1s/step - loss: 0.0640 - accuracy: 0.9853 - val_loss: 0.2260 - val_accuracy: 0.9669\n","Epoch 191/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9886\n","Epoch 191: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1073s 1s/step - loss: 0.0482 - accuracy: 0.9886 - val_loss: 0.2136 - val_accuracy: 0.9680\n","Epoch 192/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9900\n","Epoch 192: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1059s 1s/step - loss: 0.0428 - accuracy: 0.9900 - val_loss: 0.3317 - val_accuracy: 0.9562\n","Epoch 193/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9855\n","Epoch 193: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1052s 1s/step - loss: 0.0618 - accuracy: 0.9855 - val_loss: 0.5618 - val_accuracy: 0.9585\n","Epoch 194/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9902\n","Epoch 194: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1112s 1s/step - loss: 0.0435 - accuracy: 0.9902 - val_loss: 0.4208 - val_accuracy: 0.9635\n","Epoch 195/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9811\n","Epoch 195: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1035s 1s/step - loss: 0.0858 - accuracy: 0.9811 - val_loss: 0.7705 - val_accuracy: 0.9400\n","Epoch 196/200\n","787/787 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9697\n","Epoch 196: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1113s 1s/step - loss: 0.1213 - accuracy: 0.9697 - val_loss: 1.3216 - val_accuracy: 0.9226\n","Epoch 197/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9845\n","Epoch 197: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1062s 1s/step - loss: 0.0575 - accuracy: 0.9845 - val_loss: 0.6716 - val_accuracy: 0.9506\n","Epoch 198/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9865\n","Epoch 198: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1069s 1s/step - loss: 0.0493 - accuracy: 0.9865 - val_loss: 0.5809 - val_accuracy: 0.9489\n","Epoch 199/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9825\n","Epoch 199: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1083s 1s/step - loss: 0.0701 - accuracy: 0.9825 - val_loss: 2.2608 - val_accuracy: 0.9231\n","Epoch 200/200\n","787/787 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9892\n","Epoch 200: val_accuracy did not improve from 0.96970\n","787/787 [==============================] - 1035s 1s/step - loss: 0.0522 - accuracy: 0.9892 - val_loss: 0.4003 - val_accuracy: 0.9613\n","66/66 [==============================] - 63s 951ms/step\n","[[1.3205719e-02 8.5674070e-02 1.3190740e-05 ... 1.8697834e-04\n","  8.3438998e-01 6.4126961e-02]\n"," [1.0000000e+00 7.8583471e-16 1.7388211e-15 ... 7.7326674e-19\n","  1.5576773e-14 0.0000000e+00]\n"," [5.2991412e-03 1.6844641e-02 7.7371779e-03 ... 2.4282148e-03\n","  1.9509813e-02 8.0967057e-01]\n"," ...\n"," [1.0000000e+00 7.8754311e-14 1.8057305e-15 ... 8.4679952e-18\n","  4.9000891e-13 0.0000000e+00]\n"," [6.1515759e-10 5.4145529e-12 2.2079592e-11 ... 1.6627370e-22\n","  1.0000000e+00 0.0000000e+00]\n"," [1.0000000e+00 4.9208131e-18 1.0037985e-16 ... 3.6814225e-22\n","  1.6708680e-16 0.0000000e+00]]\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Alexnet_code.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1ST_yrafE2p_4JHcbvSpBBYMW1lWIw5hD\n","\"\"\"\n","\n","from __future__ import print_function\n","import keras\n","import pandas as pd\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import os\n","import time\n","from openpyxl import load_workbook\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n","from keras.layers import BatchNormalization\n","from PIL import Image\n","import tensorflow as tf\n","\n","import xlsxwriter\n","import cv2\n","from random import shuffle\n","\n","\n","\n","\n","start1= time.time()\n","\n","# Training parameters\n","batch_size = 10  # orig paper trained all networks with batch_size=128\n","epochs = 200\n","data_augmentation = False\n","num_classes = 12\n","test_img_num = 2083\n","result_excel_link = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Result/fruit_batch10_epoch200.xlsx'\n","Train_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TRAIN'\n","TEST_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TEST'\n","label_link= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL_encoded_file.xlsx'\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 2\n","n = 2\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'AlexNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","##(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","\n","\n","def label_img(name):\n","    # print(name)\n","    filename1 = os.fsdecode(name)\n","    # print(filename1)\n","    s=[]\n","    s= filename1.split('_')\n","    # print(s)\n","    # print(int(s[0]))\n","    #word_label = name.split('-')[0]\n","    #if word_label == 'golden_retriever': return np.array([1, 0])\n","    #elif word_label == 'shetland_sheepdog' : return np.array([0, 1])\n","    wb = load_workbook(label_link)\n","    ws = wb.active\n","    #for cell in ws.columns[1]:  #here column 9 is value is what i am testing which is Column X of my example.\n","    for x in range(2, 18454):\n","        #for y in range(1,):\n","        cell= ws.cell(row=x, column=1)\n","        # print(cell.value)\n","        # print(cell.value)\n","        if cell.value == int(s[0]) :\n","               #print ws.cell(column=12).value\n","                #a= ws.columns(row).value\n","                #b= ws.columns(col).value\n","                cell2= ws.cell(row=x, column=2)\n","\n","                label1= cell2.value\n","                # print(label1)\n","                break\n","        #else:\n","                #print('hi')\n","    return label1\n","\n","def load_training_data():\n","    train_data = []\n","    for img in os.listdir(Train_DIR):\n","        #print(img)\n","        temp2 = img.replace('.png','')\n","        label = label_img(temp2)\n","        #print(label)\n","        path = os.path.join(Train_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        # print(path)\n","        # print(label)\n","        img2= cv2.imread(path)\n","        # cv2_imshow(img2)\n","        #img = img.convert('L')\n","        # img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        #path3= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/training_441_segmented_polar_A1_A2_A4'\n","        resized_img1 = cv2.resize(img2, (224,224), interpolation = cv2.INTER_AREA)\n","        #cv2.imwrite(path3+ '/' + temp2 , resized_img1 )\n","        train_data.append([np.array(resized_img1), label])\n","\n","    shuffle(train_data)\n","    return train_data\n","\n","\n","train_data = load_training_data()\n","\n","# s = [i[1] for i in train_data]\n","# print(train_data)\n","\n","# df2 = pd.DataFrame(np.array(df2),columns=['Label'])\n","# df2['Label'] = pd.Categorical(df2.Label)\n","IMG_SIZE = 224\n","# trainImages = np.array([i[0] for i in train_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","trainImages = np.array([i[0] for i in train_data])\n","# df = pd.read_csv()\n","# trainLabels = tf.convert_to_tensor(np.array([i[1] for i in train_data]).astype(np.int_))\n","trainLabels = np.array([i[1] for i in train_data])\n","print(trainLabels[2]) #successful till here\n","#print(trainLabels)\n","# trainLabels = to_categorical(trainLabels)                                                          #problem here\n","trainLabels = keras.utils.to_categorical(trainLabels, num_classes)\n","print(trainLabels[2])\n","\n","\n","\n","def load_test_data():\n","    test_data = []\n","    for img in os.listdir(TEST_DIR):\n","        temp1 = img.replace('.png','')\n","        # print(temp1)\n","        label = label_img(temp1)\n","        path = os.path.join(TEST_DIR, img)\n","        # temp1 = img\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img1= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        resized_img2 = cv2.resize(img1, (224,224), interpolation = cv2.INTER_AREA)\n","        #path2= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/testing folder_segmented_resized'\n","        #cv2.imwrite(path2+ '/' + temp1 , resized_img2 )\n","        test_data.append([np.array(resized_img2), label,temp1])\n","        # print(label)\n","\n","    shuffle(test_data)\n","    return test_data\n","\n","\n","test_data = load_test_data()\n","#plt.imshow(test_data[10][0], cmap = 'gist_gray')\n","\n","\n","# # convert it to tensorflow              ########################\n","# tensor1 = tf.convert_to_tensor(numpy_array)\n","# print(tensor1)\n","\n","# testImages = np.array([i[0] for i in test_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","# testLabels = tf.convert_to_tensor(np.array([i[1] for i in test_data]).astype(np.int_))\n","\n","testImages = np.array([i[0] for i in test_data])\n","testLabels = np.array([i[1] for i in test_data])\n","test_img_name = np.array([i[2] for i in test_data])\n","\n","\n","#testLabels = keras.utils.to_categorical(testLabels, num_classes)\n","\n","# Input image dimensions.\n","input_shape = trainImages.shape[1:]\n","\n","# Normalize data.\n","##trainImages = trainImages.astype('float32') / 255\n","##testImages = testImages.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","##if subtract_pixel_mean:\n","    ##trainImages_mean = np.mean(trainImages, axis=0)\n","    ##trainImages -= trainImages_mean\n","    ##testImages -= trainImages_mean\n","\n","#print('x_train shape:', x_train.shape)\n","#print(x_train.shape[0], 'train samples')\n","#print(x_test.shape[0], 'test samples')\n","#print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","#y_train = keras.utils.to_categorical(y_train, num_classes)\n","#y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","model = Sequential()\n","\n","# 1st Convolutional Layer\n","model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),strides=(4,4), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation before passing it to the next layer\n","model.add(BatchNormalization())\n","\n","# 2nd Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 3rd Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 5th Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# Passing it to a dense layer\n","model.add(Flatten())\n","# 1st Dense Layer\n","model.add(Dense(4096, input_shape=(224*224*3,)))\n","model.add(Activation('relu'))\n","# Add Dropout to prevent overfitting\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 2nd Dense Layer\n","model.add(Dense(4096))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 3rd Dense Layer\n","model.add(Dense(1000))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Dense Layer\n","model.add(Dense(100))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","\n","# Output Layer\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))\n","\n","model.summary()\n","model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","\n","# Prepare model model saving directory.\n","save_dir = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model'\n","model_name = 'Alexnet_model_fNet_batch10_epoch200.h5'\n","saved_model= save_dir + '/' + model_name\n","\n","# saved_model= save_dir + '/' + model_name\n","# saved_model_final = load_model(saved_model)\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200) #EPOCH CHANGING HERE\n","mc = ModelCheckpoint(saved_model , monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","\n","# print(type(trainImages))\n","# print(type(trainLabels))\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainLabels= np.asarray(trainLabels).astype(np.float32)\n","model.fit(trainImages, trainLabels, batch_size= batch_size, epochs = epochs, verbose=1, validation_split=0.1846, shuffle=True, callbacks=[es, mc])\n","\n","saved_model_final = load_model(saved_model)\n","\n","\n","# Score trained model.\n","preds = saved_model_final.predict(testImages)\n","print(preds)\n","\n","end1= time.time()\n","time_taken= end1- start1\n","\n","workbook = xlsxwriter.Workbook(result_excel_link)\n","worksheet = workbook.add_worksheet()\n","\n","for i in range(0,test_img_num):\n","        #wb1 = load_workbook('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        #ws1 = wb1.active\n","        #a = ws1.cell(row= i+1, column=1)\n","        #a.value = testLabels[i]\n","        #b = ws1.cell(row= i+1, column=2)\n","        #b.value = np.argmax(preds [i])\n","        #ws1.save('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        name = test_img_name[i]\n","        worksheet.write(i+1, 0 , name)\n","        a = testLabels[i]\n","        worksheet.write(i+1, 1 , a)\n","        b = np.argmax(preds[i])\n","        worksheet.write(i+1, 2 , b)\n","        if a==b:\n","             worksheet.write(i+1, 3 , 1)\n","        else:\n","             worksheet.write(i+1, 3 , 0)\n","\n","# worksheet.write(1, 4 , time_taken)\n","\n","workbook.close()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["8\n","[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n","Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_25 (Conv2D)          (None, 54, 54, 96)        34944     \n","                                                                 \n"," activation_50 (Activation)  (None, 54, 54, 96)        0         \n","                                                                 \n"," max_pooling2d_15 (MaxPooli  (None, 27, 27, 96)        0         \n"," ng2D)                                                           \n","                                                                 \n"," batch_normalization_45 (Ba  (None, 27, 27, 96)        384       \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_26 (Conv2D)          (None, 17, 17, 256)       2973952   \n","                                                                 \n"," activation_51 (Activation)  (None, 17, 17, 256)       0         \n","                                                                 \n"," max_pooling2d_16 (MaxPooli  (None, 8, 8, 256)         0         \n"," ng2D)                                                           \n","                                                                 \n"," batch_normalization_46 (Ba  (None, 8, 8, 256)         1024      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_27 (Conv2D)          (None, 6, 6, 384)         885120    \n","                                                                 \n"," activation_52 (Activation)  (None, 6, 6, 384)         0         \n","                                                                 \n"," batch_normalization_47 (Ba  (None, 6, 6, 384)         1536      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_28 (Conv2D)          (None, 4, 4, 384)         1327488   \n","                                                                 \n"," activation_53 (Activation)  (None, 4, 4, 384)         0         \n","                                                                 \n"," batch_normalization_48 (Ba  (None, 4, 4, 384)         1536      \n"," tchNormalization)                                               \n","                                                                 \n"," conv2d_29 (Conv2D)          (None, 2, 2, 256)         884992    \n","                                                                 \n"," activation_54 (Activation)  (None, 2, 2, 256)         0         \n","                                                                 \n"," max_pooling2d_17 (MaxPooli  (None, 1, 1, 256)         0         \n"," ng2D)                                                           \n","                                                                 \n"," batch_normalization_49 (Ba  (None, 1, 1, 256)         1024      \n"," tchNormalization)                                               \n","                                                                 \n"," flatten_5 (Flatten)         (None, 256)               0         \n","                                                                 \n"," dense_25 (Dense)            (None, 4096)              1052672   \n","                                                                 \n"," activation_55 (Activation)  (None, 4096)              0         \n","                                                                 \n"," dropout_20 (Dropout)        (None, 4096)              0         \n","                                                                 \n"," batch_normalization_50 (Ba  (None, 4096)              16384     \n"," tchNormalization)                                               \n","                                                                 \n"," dense_26 (Dense)            (None, 4096)              16781312  \n","                                                                 \n"," activation_56 (Activation)  (None, 4096)              0         \n","                                                                 \n"," dropout_21 (Dropout)        (None, 4096)              0         \n","                                                                 \n"," batch_normalization_51 (Ba  (None, 4096)              16384     \n"," tchNormalization)                                               \n","                                                                 \n"," dense_27 (Dense)            (None, 1000)              4097000   \n","                                                                 \n"," activation_57 (Activation)  (None, 1000)              0         \n","                                                                 \n"," dropout_22 (Dropout)        (None, 1000)              0         \n","                                                                 \n"," batch_normalization_52 (Ba  (None, 1000)              4000      \n"," tchNormalization)                                               \n","                                                                 \n"," dense_28 (Dense)            (None, 100)               100100    \n","                                                                 \n"," activation_58 (Activation)  (None, 100)               0         \n","                                                                 \n"," dropout_23 (Dropout)        (None, 100)               0         \n","                                                                 \n"," batch_normalization_53 (Ba  (None, 100)               400       \n"," tchNormalization)                                               \n","                                                                 \n"," dense_29 (Dense)            (None, 12)                1212      \n","                                                                 \n"," activation_59 (Activation)  (None, 12)                0         \n","                                                                 \n","=================================================================\n","Total params: 28181464 (107.50 MB)\n","Trainable params: 28160128 (107.42 MB)\n","Non-trainable params: 21336 (83.34 KB)\n","_________________________________________________________________\n","Epoch 1/250\n","787/787 [==============================] - ETA: 0s - loss: 1.9622 - accuracy: 0.3731\n","Epoch 1: val_accuracy improved from -inf to 0.44557, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 610s 773ms/step - loss: 1.9622 - accuracy: 0.3731 - val_loss: 1.8781 - val_accuracy: 0.4456\n","Epoch 2/250\n","787/787 [==============================] - ETA: 0s - loss: 1.5583 - accuracy: 0.4778\n","Epoch 2: val_accuracy improved from 0.44557 to 0.44837, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 567s 720ms/step - loss: 1.5583 - accuracy: 0.4778 - val_loss: 2.2268 - val_accuracy: 0.4484\n","Epoch 3/250\n","787/787 [==============================] - ETA: 0s - loss: 1.4303 - accuracy: 0.5233\n","Epoch 3: val_accuracy improved from 0.44837 to 0.51796, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 551s 700ms/step - loss: 1.4303 - accuracy: 0.5233 - val_loss: 1.6290 - val_accuracy: 0.5180\n","Epoch 4/250\n","787/787 [==============================] - ETA: 0s - loss: 1.3484 - accuracy: 0.5533\n","Epoch 4: val_accuracy did not improve from 0.51796\n","787/787 [==============================] - 581s 738ms/step - loss: 1.3484 - accuracy: 0.5533 - val_loss: 1.8692 - val_accuracy: 0.4832\n","Epoch 5/250\n","787/787 [==============================] - ETA: 0s - loss: 1.2499 - accuracy: 0.5762\n","Epoch 5: val_accuracy improved from 0.51796 to 0.65600, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 571s 726ms/step - loss: 1.2499 - accuracy: 0.5762 - val_loss: 1.0747 - val_accuracy: 0.6560\n","Epoch 6/250\n","787/787 [==============================] - ETA: 0s - loss: 1.2479 - accuracy: 0.5815\n","Epoch 6: val_accuracy did not improve from 0.65600\n","787/787 [==============================] - 564s 717ms/step - loss: 1.2479 - accuracy: 0.5815 - val_loss: 1.0390 - val_accuracy: 0.6397\n","Epoch 7/250\n","787/787 [==============================] - ETA: 0s - loss: 1.1459 - accuracy: 0.6230\n","Epoch 7: val_accuracy improved from 0.65600 to 0.66049, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 591s 751ms/step - loss: 1.1459 - accuracy: 0.6230 - val_loss: 1.1952 - val_accuracy: 0.6605\n","Epoch 8/250\n","787/787 [==============================] - ETA: 0s - loss: 1.1674 - accuracy: 0.6174\n","Epoch 8: val_accuracy did not improve from 0.66049\n","787/787 [==============================] - 557s 708ms/step - loss: 1.1674 - accuracy: 0.6174 - val_loss: 2.8012 - val_accuracy: 0.4944\n","Epoch 9/250\n","787/787 [==============================] - ETA: 0s - loss: 1.0796 - accuracy: 0.6516\n","Epoch 9: val_accuracy did not improve from 0.66049\n","787/787 [==============================] - 560s 711ms/step - loss: 1.0796 - accuracy: 0.6516 - val_loss: 1.3490 - val_accuracy: 0.6414\n","Epoch 10/250\n","787/787 [==============================] - ETA: 0s - loss: 1.1490 - accuracy: 0.6290\n","Epoch 10: val_accuracy did not improve from 0.66049\n","787/787 [==============================] - 701s 890ms/step - loss: 1.1490 - accuracy: 0.6290 - val_loss: 1.1761 - val_accuracy: 0.6397\n","Epoch 11/250\n","787/787 [==============================] - ETA: 0s - loss: 1.0659 - accuracy: 0.6475\n","Epoch 11: val_accuracy improved from 0.66049 to 0.70988, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 936s 1s/step - loss: 1.0659 - accuracy: 0.6475 - val_loss: 0.8892 - val_accuracy: 0.7099\n","Epoch 12/250\n","787/787 [==============================] - ETA: 0s - loss: 0.9683 - accuracy: 0.6864\n","Epoch 12: val_accuracy did not improve from 0.70988\n","787/787 [==============================] - 955s 1s/step - loss: 0.9683 - accuracy: 0.6864 - val_loss: 1.1640 - val_accuracy: 0.6813\n","Epoch 13/250\n","787/787 [==============================] - ETA: 0s - loss: 1.0366 - accuracy: 0.6651\n","Epoch 13: val_accuracy did not improve from 0.70988\n","787/787 [==============================] - 916s 1s/step - loss: 1.0366 - accuracy: 0.6651 - val_loss: 0.9616 - val_accuracy: 0.6908\n","Epoch 14/250\n","787/787 [==============================] - ETA: 0s - loss: 0.9223 - accuracy: 0.7092\n","Epoch 14: val_accuracy did not improve from 0.70988\n","787/787 [==============================] - 959s 1s/step - loss: 0.9223 - accuracy: 0.7092 - val_loss: 0.9876 - val_accuracy: 0.6857\n","Epoch 15/250\n","787/787 [==============================] - ETA: 0s - loss: 0.8750 - accuracy: 0.7207\n","Epoch 15: val_accuracy improved from 0.70988 to 0.75814, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 909s 1s/step - loss: 0.8750 - accuracy: 0.7207 - val_loss: 0.8035 - val_accuracy: 0.7581\n","Epoch 16/250\n","787/787 [==============================] - ETA: 0s - loss: 0.8061 - accuracy: 0.7500\n","Epoch 16: val_accuracy improved from 0.75814 to 0.81874, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 924s 1s/step - loss: 0.8061 - accuracy: 0.7500 - val_loss: 1.4925 - val_accuracy: 0.8187\n","Epoch 17/250\n","787/787 [==============================] - ETA: 0s - loss: 0.7547 - accuracy: 0.7727\n","Epoch 17: val_accuracy did not improve from 0.81874\n","787/787 [==============================] - 904s 1s/step - loss: 0.7547 - accuracy: 0.7727 - val_loss: 2.7595 - val_accuracy: 0.4714\n","Epoch 18/250\n","787/787 [==============================] - ETA: 0s - loss: 0.7560 - accuracy: 0.7674\n","Epoch 18: val_accuracy improved from 0.81874 to 0.86364, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 936s 1s/step - loss: 0.7560 - accuracy: 0.7674 - val_loss: 0.4856 - val_accuracy: 0.8636\n","Epoch 19/250\n","787/787 [==============================] - ETA: 0s - loss: 0.6735 - accuracy: 0.7980\n","Epoch 19: val_accuracy did not improve from 0.86364\n","787/787 [==============================] - 925s 1s/step - loss: 0.6735 - accuracy: 0.7980 - val_loss: 7.3145 - val_accuracy: 0.8558\n","Epoch 20/250\n","787/787 [==============================] - ETA: 0s - loss: 0.6773 - accuracy: 0.7974\n","Epoch 20: val_accuracy did not improve from 0.86364\n","787/787 [==============================] - 918s 1s/step - loss: 0.6773 - accuracy: 0.7974 - val_loss: 1.7016 - val_accuracy: 0.6723\n","Epoch 21/250\n","787/787 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.7956\n","Epoch 21: val_accuracy improved from 0.86364 to 0.89618, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 930s 1s/step - loss: 0.6996 - accuracy: 0.7956 - val_loss: 7.1731 - val_accuracy: 0.8962\n","Epoch 22/250\n","787/787 [==============================] - ETA: 0s - loss: 0.6141 - accuracy: 0.8192\n","Epoch 22: val_accuracy did not improve from 0.89618\n","787/787 [==============================] - 902s 1s/step - loss: 0.6141 - accuracy: 0.8192 - val_loss: 0.3878 - val_accuracy: 0.8962\n","Epoch 23/250\n","787/787 [==============================] - ETA: 0s - loss: 0.5686 - accuracy: 0.8346\n","Epoch 23: val_accuracy did not improve from 0.89618\n","787/787 [==============================] - 916s 1s/step - loss: 0.5686 - accuracy: 0.8346 - val_loss: 0.4999 - val_accuracy: 0.8676\n","Epoch 24/250\n","787/787 [==============================] - ETA: 0s - loss: 0.4994 - accuracy: 0.8519\n","Epoch 24: val_accuracy did not improve from 0.89618\n","787/787 [==============================] - 915s 1s/step - loss: 0.4994 - accuracy: 0.8519 - val_loss: 0.3716 - val_accuracy: 0.8934\n","Epoch 25/250\n","787/787 [==============================] - ETA: 0s - loss: 0.5225 - accuracy: 0.8556\n","Epoch 25: val_accuracy improved from 0.89618 to 0.91302, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 901s 1s/step - loss: 0.5225 - accuracy: 0.8556 - val_loss: 0.3545 - val_accuracy: 0.9130\n","Epoch 26/250\n","787/787 [==============================] - ETA: 0s - loss: 0.5267 - accuracy: 0.8512\n","Epoch 26: val_accuracy did not improve from 0.91302\n","787/787 [==============================] - 924s 1s/step - loss: 0.5267 - accuracy: 0.8512 - val_loss: 0.5565 - val_accuracy: 0.8496\n","Epoch 27/250\n","787/787 [==============================] - ETA: 0s - loss: 0.5359 - accuracy: 0.8486\n","Epoch 27: val_accuracy improved from 0.91302 to 0.92593, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 895s 1s/step - loss: 0.5359 - accuracy: 0.8486 - val_loss: 0.2881 - val_accuracy: 0.9259\n","Epoch 28/250\n","787/787 [==============================] - ETA: 0s - loss: 0.4079 - accuracy: 0.8818\n","Epoch 28: val_accuracy did not improve from 0.92593\n","787/787 [==============================] - 920s 1s/step - loss: 0.4079 - accuracy: 0.8818 - val_loss: 0.3935 - val_accuracy: 0.9007\n","Epoch 29/250\n","787/787 [==============================] - ETA: 0s - loss: 0.5000 - accuracy: 0.8588\n","Epoch 29: val_accuracy did not improve from 0.92593\n","787/787 [==============================] - 915s 1s/step - loss: 0.5000 - accuracy: 0.8588 - val_loss: 0.5146 - val_accuracy: 0.8861\n","Epoch 30/250\n","787/787 [==============================] - ETA: 0s - loss: 0.5590 - accuracy: 0.8434\n","Epoch 30: val_accuracy did not improve from 0.92593\n","787/787 [==============================] - 935s 1s/step - loss: 0.5590 - accuracy: 0.8434 - val_loss: 0.4452 - val_accuracy: 0.8810\n","Epoch 31/250\n","787/787 [==============================] - ETA: 0s - loss: 0.4223 - accuracy: 0.8794\n","Epoch 31: val_accuracy improved from 0.92593 to 0.92817, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 915s 1s/step - loss: 0.4223 - accuracy: 0.8794 - val_loss: 0.2923 - val_accuracy: 0.9282\n","Epoch 32/250\n","787/787 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.9109\n","Epoch 32: val_accuracy improved from 0.92817 to 0.93771, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 889s 1s/step - loss: 0.3291 - accuracy: 0.9109 - val_loss: 0.2584 - val_accuracy: 0.9377\n","Epoch 33/250\n","787/787 [==============================] - ETA: 0s - loss: 0.3576 - accuracy: 0.9033\n","Epoch 33: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 897s 1s/step - loss: 0.3576 - accuracy: 0.9033 - val_loss: 0.2878 - val_accuracy: 0.9282\n","Epoch 34/250\n","787/787 [==============================] - ETA: 0s - loss: 0.3143 - accuracy: 0.9155\n","Epoch 34: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 900s 1s/step - loss: 0.3143 - accuracy: 0.9155 - val_loss: 0.2450 - val_accuracy: 0.9332\n","Epoch 35/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.9217\n","Epoch 35: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 917s 1s/step - loss: 0.2955 - accuracy: 0.9217 - val_loss: 0.4245 - val_accuracy: 0.8855\n","Epoch 36/250\n","787/787 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.9202\n","Epoch 36: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 917s 1s/step - loss: 0.3202 - accuracy: 0.9202 - val_loss: 0.3494 - val_accuracy: 0.9147\n","Epoch 37/250\n","787/787 [==============================] - ETA: 0s - loss: 0.3338 - accuracy: 0.9101\n","Epoch 37: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 897s 1s/step - loss: 0.3338 - accuracy: 0.9101 - val_loss: 0.4048 - val_accuracy: 0.9080\n","Epoch 38/250\n","787/787 [==============================] - ETA: 0s - loss: 0.3095 - accuracy: 0.9137\n","Epoch 38: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 924s 1s/step - loss: 0.3095 - accuracy: 0.9137 - val_loss: 0.2687 - val_accuracy: 0.9259\n","Epoch 39/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2860 - accuracy: 0.9235\n","Epoch 39: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 916s 1s/step - loss: 0.2860 - accuracy: 0.9235 - val_loss: 0.7968 - val_accuracy: 0.8967\n","Epoch 40/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.9353\n","Epoch 40: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 949s 1s/step - loss: 0.2554 - accuracy: 0.9353 - val_loss: 0.6463 - val_accuracy: 0.8754\n","Epoch 41/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.9310\n","Epoch 41: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 929s 1s/step - loss: 0.2721 - accuracy: 0.9310 - val_loss: 0.3588 - val_accuracy: 0.9310\n","Epoch 42/250\n","787/787 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.9151\n","Epoch 42: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 934s 1s/step - loss: 0.3027 - accuracy: 0.9151 - val_loss: 0.6327 - val_accuracy: 0.9035\n","Epoch 43/250\n","787/787 [==============================] - ETA: 0s - loss: 0.4422 - accuracy: 0.8798\n","Epoch 43: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 928s 1s/step - loss: 0.4422 - accuracy: 0.8798 - val_loss: 2.1741 - val_accuracy: 0.8962\n","Epoch 44/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.9237\n","Epoch 44: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 922s 1s/step - loss: 0.2906 - accuracy: 0.9237 - val_loss: 0.4721 - val_accuracy: 0.9343\n","Epoch 45/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2672 - accuracy: 0.9288\n","Epoch 45: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 931s 1s/step - loss: 0.2672 - accuracy: 0.9288 - val_loss: 0.4642 - val_accuracy: 0.9186\n","Epoch 46/250\n","787/787 [==============================] - ETA: 0s - loss: 0.3871 - accuracy: 0.8960\n","Epoch 46: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 925s 1s/step - loss: 0.3871 - accuracy: 0.8960 - val_loss: 0.5304 - val_accuracy: 0.9091\n","Epoch 47/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.9244\n","Epoch 47: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 941s 1s/step - loss: 0.2888 - accuracy: 0.9244 - val_loss: 0.6604 - val_accuracy: 0.9237\n","Epoch 48/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9338\n","Epoch 48: val_accuracy did not improve from 0.93771\n","787/787 [==============================] - 924s 1s/step - loss: 0.2709 - accuracy: 0.9338 - val_loss: 0.5071 - val_accuracy: 0.9338\n","Epoch 49/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.9450\n","Epoch 49: val_accuracy improved from 0.93771 to 0.95118, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 922s 1s/step - loss: 0.2175 - accuracy: 0.9450 - val_loss: 0.5376 - val_accuracy: 0.9512\n","Epoch 50/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.9451\n","Epoch 50: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 934s 1s/step - loss: 0.2067 - accuracy: 0.9451 - val_loss: 0.6172 - val_accuracy: 0.9085\n","Epoch 51/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.9320\n","Epoch 51: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 911s 1s/step - loss: 0.2623 - accuracy: 0.9320 - val_loss: 1.6014 - val_accuracy: 0.9063\n","Epoch 52/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9418\n","Epoch 52: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 932s 1s/step - loss: 0.2182 - accuracy: 0.9418 - val_loss: 0.2367 - val_accuracy: 0.9444\n","Epoch 53/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.9481\n","Epoch 53: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 926s 1s/step - loss: 0.2012 - accuracy: 0.9481 - val_loss: 0.5820 - val_accuracy: 0.9433\n","Epoch 54/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.9475\n","Epoch 54: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 920s 1s/step - loss: 0.1976 - accuracy: 0.9475 - val_loss: 0.7134 - val_accuracy: 0.9231\n","Epoch 55/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9494\n","Epoch 55: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 924s 1s/step - loss: 0.1929 - accuracy: 0.9494 - val_loss: 1.1911 - val_accuracy: 0.9192\n","Epoch 56/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.9521\n","Epoch 56: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 903s 1s/step - loss: 0.1842 - accuracy: 0.9521 - val_loss: 2.8228 - val_accuracy: 0.8911\n","Epoch 57/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9348\n","Epoch 57: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 924s 1s/step - loss: 0.2534 - accuracy: 0.9348 - val_loss: 1.0023 - val_accuracy: 0.9405\n","Epoch 58/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.9554\n","Epoch 58: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 903s 1s/step - loss: 0.1804 - accuracy: 0.9554 - val_loss: 0.9158 - val_accuracy: 0.9270\n","Epoch 59/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9522\n","Epoch 59: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 946s 1s/step - loss: 0.1705 - accuracy: 0.9522 - val_loss: 0.2082 - val_accuracy: 0.9512\n","Epoch 60/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.9597\n","Epoch 60: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 942s 1s/step - loss: 0.1499 - accuracy: 0.9597 - val_loss: 0.2717 - val_accuracy: 0.9433\n","Epoch 61/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9550\n","Epoch 61: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 911s 1s/step - loss: 0.1650 - accuracy: 0.9550 - val_loss: 0.5177 - val_accuracy: 0.9473\n","Epoch 62/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9611\n","Epoch 62: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 931s 1s/step - loss: 0.1423 - accuracy: 0.9611 - val_loss: 0.3937 - val_accuracy: 0.9343\n","Epoch 63/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1564 - accuracy: 0.9591\n","Epoch 63: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 906s 1s/step - loss: 0.1564 - accuracy: 0.9591 - val_loss: 0.9038 - val_accuracy: 0.9012\n","Epoch 64/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9488\n","Epoch 64: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 941s 1s/step - loss: 0.1903 - accuracy: 0.9488 - val_loss: 0.3856 - val_accuracy: 0.9332\n","Epoch 65/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1802 - accuracy: 0.9544\n","Epoch 65: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 907s 1s/step - loss: 0.1802 - accuracy: 0.9544 - val_loss: 0.3011 - val_accuracy: 0.9461\n","Epoch 66/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9602\n","Epoch 66: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 930s 1s/step - loss: 0.1581 - accuracy: 0.9602 - val_loss: 0.5723 - val_accuracy: 0.9293\n","Epoch 67/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.9425\n","Epoch 67: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 932s 1s/step - loss: 0.2174 - accuracy: 0.9425 - val_loss: 3.3690 - val_accuracy: 0.9411\n","Epoch 68/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1695 - accuracy: 0.9578\n","Epoch 68: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 926s 1s/step - loss: 0.1695 - accuracy: 0.9578 - val_loss: 75.6353 - val_accuracy: 0.6751\n","Epoch 69/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2690 - accuracy: 0.9300\n","Epoch 69: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 936s 1s/step - loss: 0.2690 - accuracy: 0.9300 - val_loss: 5.6364 - val_accuracy: 0.9276\n","Epoch 70/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 0.9556\n","Epoch 70: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 910s 1s/step - loss: 0.1735 - accuracy: 0.9556 - val_loss: 2.0035 - val_accuracy: 0.9220\n","Epoch 71/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9583\n","Epoch 71: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 945s 1s/step - loss: 0.1726 - accuracy: 0.9583 - val_loss: 0.3824 - val_accuracy: 0.9282\n","Epoch 72/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9644\n","Epoch 72: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 926s 1s/step - loss: 0.1397 - accuracy: 0.9644 - val_loss: 0.5038 - val_accuracy: 0.9265\n","Epoch 73/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9542\n","Epoch 73: val_accuracy did not improve from 0.95118\n","787/787 [==============================] - 916s 1s/step - loss: 0.1713 - accuracy: 0.9542 - val_loss: 0.2218 - val_accuracy: 0.9484\n","Epoch 74/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9572\n","Epoch 74: val_accuracy improved from 0.95118 to 0.95511, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 946s 1s/step - loss: 0.1620 - accuracy: 0.9572 - val_loss: 0.2322 - val_accuracy: 0.9551\n","Epoch 75/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9652\n","Epoch 75: val_accuracy did not improve from 0.95511\n","787/787 [==============================] - 878s 1s/step - loss: 0.1367 - accuracy: 0.9652 - val_loss: 0.4691 - val_accuracy: 0.9383\n","Epoch 76/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9569\n","Epoch 76: val_accuracy did not improve from 0.95511\n","787/787 [==============================] - 920s 1s/step - loss: 0.1760 - accuracy: 0.9569 - val_loss: 2.6508 - val_accuracy: 0.8103\n","Epoch 77/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.9358\n","Epoch 77: val_accuracy did not improve from 0.95511\n","787/787 [==============================] - 916s 1s/step - loss: 0.2467 - accuracy: 0.9358 - val_loss: 0.8949 - val_accuracy: 0.8760\n","Epoch 78/250\n","787/787 [==============================] - ETA: 0s - loss: 0.2060 - accuracy: 0.9455\n","Epoch 78: val_accuracy did not improve from 0.95511\n","787/787 [==============================] - 926s 1s/step - loss: 0.2060 - accuracy: 0.9455 - val_loss: 0.4018 - val_accuracy: 0.9209\n","Epoch 79/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9535\n","Epoch 79: val_accuracy improved from 0.95511 to 0.95847, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 936s 1s/step - loss: 0.1677 - accuracy: 0.9535 - val_loss: 0.1853 - val_accuracy: 0.9585\n","Epoch 80/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9652\n","Epoch 80: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 888s 1s/step - loss: 0.1388 - accuracy: 0.9652 - val_loss: 0.4434 - val_accuracy: 0.9467\n","Epoch 81/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9709\n","Epoch 81: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 917s 1s/step - loss: 0.1094 - accuracy: 0.9709 - val_loss: 0.2062 - val_accuracy: 0.9585\n","Epoch 82/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9729\n","Epoch 82: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 907s 1s/step - loss: 0.1033 - accuracy: 0.9729 - val_loss: 7.2140 - val_accuracy: 0.9506\n","Epoch 83/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9714\n","Epoch 83: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 918s 1s/step - loss: 0.1158 - accuracy: 0.9714 - val_loss: 0.2340 - val_accuracy: 0.9574\n","Epoch 84/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9513\n","Epoch 84: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 935s 1s/step - loss: 0.1933 - accuracy: 0.9513 - val_loss: 0.7933 - val_accuracy: 0.9523\n","Epoch 85/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9598\n","Epoch 85: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 926s 1s/step - loss: 0.1630 - accuracy: 0.9598 - val_loss: 0.2549 - val_accuracy: 0.9512\n","Epoch 86/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9671\n","Epoch 86: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 948s 1s/step - loss: 0.1276 - accuracy: 0.9671 - val_loss: 0.2375 - val_accuracy: 0.9467\n","Epoch 87/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9683\n","Epoch 87: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 919s 1s/step - loss: 0.1412 - accuracy: 0.9683 - val_loss: 0.5672 - val_accuracy: 0.9282\n","Epoch 88/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9663\n","Epoch 88: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 957s 1s/step - loss: 0.1370 - accuracy: 0.9663 - val_loss: 0.2032 - val_accuracy: 0.9557\n","Epoch 89/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9779\n","Epoch 89: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 930s 1s/step - loss: 0.0879 - accuracy: 0.9779 - val_loss: 0.2114 - val_accuracy: 0.9529\n","Epoch 90/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9761\n","Epoch 90: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 924s 1s/step - loss: 0.0904 - accuracy: 0.9761 - val_loss: 0.7587 - val_accuracy: 0.9551\n","Epoch 91/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9733\n","Epoch 91: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 940s 1s/step - loss: 0.1112 - accuracy: 0.9733 - val_loss: 0.2098 - val_accuracy: 0.9574\n","Epoch 92/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9738\n","Epoch 92: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 918s 1s/step - loss: 0.1051 - accuracy: 0.9738 - val_loss: 0.5098 - val_accuracy: 0.9444\n","Epoch 93/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.9625\n","Epoch 93: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 933s 1s/step - loss: 0.1305 - accuracy: 0.9625 - val_loss: 3.7156 - val_accuracy: 0.9371\n","Epoch 94/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9700\n","Epoch 94: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 916s 1s/step - loss: 0.1348 - accuracy: 0.9700 - val_loss: 14.7255 - val_accuracy: 0.9484\n","Epoch 95/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9680\n","Epoch 95: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 940s 1s/step - loss: 0.1241 - accuracy: 0.9680 - val_loss: 35.7713 - val_accuracy: 0.9355\n","Epoch 96/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9814\n","Epoch 96: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 947s 1s/step - loss: 0.0717 - accuracy: 0.9814 - val_loss: 90.8995 - val_accuracy: 0.9456\n","Epoch 97/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9797\n","Epoch 97: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 920s 1s/step - loss: 0.0882 - accuracy: 0.9797 - val_loss: 1.5455 - val_accuracy: 0.9349\n","Epoch 98/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9713\n","Epoch 98: val_accuracy did not improve from 0.95847\n","787/787 [==============================] - 932s 1s/step - loss: 0.1041 - accuracy: 0.9713 - val_loss: 0.6789 - val_accuracy: 0.9484\n","Epoch 99/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9736\n","Epoch 99: val_accuracy improved from 0.95847 to 0.96072, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model/Alexnet_model_fNet_batch10_epoch250.h5\n","787/787 [==============================] - 573s 728ms/step - loss: 0.0964 - accuracy: 0.9736 - val_loss: 0.2056 - val_accuracy: 0.9607\n","Epoch 100/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9710\n","Epoch 100: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 585s 742ms/step - loss: 0.1175 - accuracy: 0.9710 - val_loss: 0.4201 - val_accuracy: 0.9366\n","Epoch 101/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9703\n","Epoch 101: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 583s 741ms/step - loss: 0.1213 - accuracy: 0.9703 - val_loss: 2.0144 - val_accuracy: 0.9242\n","Epoch 102/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1407 - accuracy: 0.9648\n","Epoch 102: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 573s 729ms/step - loss: 0.1407 - accuracy: 0.9648 - val_loss: 0.7628 - val_accuracy: 0.9209\n","Epoch 103/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9667\n","Epoch 103: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 589s 748ms/step - loss: 0.1299 - accuracy: 0.9667 - val_loss: 4.6012 - val_accuracy: 0.9366\n","Epoch 104/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9728\n","Epoch 104: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 573s 729ms/step - loss: 0.1106 - accuracy: 0.9728 - val_loss: 0.1896 - val_accuracy: 0.9562\n","Epoch 105/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9811\n","Epoch 105: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 602s 765ms/step - loss: 0.0751 - accuracy: 0.9811 - val_loss: 1.1782 - val_accuracy: 0.9501\n","Epoch 106/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9841\n","Epoch 106: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 576s 732ms/step - loss: 0.0620 - accuracy: 0.9841 - val_loss: 2.2503 - val_accuracy: 0.9467\n","Epoch 107/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9817\n","Epoch 107: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 583s 741ms/step - loss: 0.0732 - accuracy: 0.9817 - val_loss: 0.6629 - val_accuracy: 0.9506\n","Epoch 108/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9752\n","Epoch 108: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 592s 752ms/step - loss: 0.1034 - accuracy: 0.9752 - val_loss: 1.6738 - val_accuracy: 0.8996\n","Epoch 109/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9725\n","Epoch 109: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 600s 763ms/step - loss: 0.1128 - accuracy: 0.9725 - val_loss: 2.1971 - val_accuracy: 0.9332\n","Epoch 110/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9767\n","Epoch 110: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 590s 749ms/step - loss: 0.0947 - accuracy: 0.9767 - val_loss: 1.2914 - val_accuracy: 0.9467\n","Epoch 111/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9709\n","Epoch 111: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 572s 727ms/step - loss: 0.1068 - accuracy: 0.9709 - val_loss: 3.1429 - val_accuracy: 0.9304\n","Epoch 112/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9734\n","Epoch 112: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 587s 746ms/step - loss: 0.1117 - accuracy: 0.9734 - val_loss: 1.2217 - val_accuracy: 0.9439\n","Epoch 113/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9840\n","Epoch 113: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 605s 768ms/step - loss: 0.0653 - accuracy: 0.9840 - val_loss: 1.6631 - val_accuracy: 0.9416\n","Epoch 114/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9818\n","Epoch 114: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 575s 731ms/step - loss: 0.0695 - accuracy: 0.9818 - val_loss: 6.3121 - val_accuracy: 0.9315\n","Epoch 115/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 0.9734\n","Epoch 115: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 579s 736ms/step - loss: 0.0975 - accuracy: 0.9734 - val_loss: 2.4273 - val_accuracy: 0.9158\n","Epoch 116/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9738\n","Epoch 116: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 564s 717ms/step - loss: 0.0971 - accuracy: 0.9738 - val_loss: 4.0718 - val_accuracy: 0.9304\n","Epoch 117/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9809\n","Epoch 117: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 578s 735ms/step - loss: 0.0715 - accuracy: 0.9809 - val_loss: 1.5401 - val_accuracy: 0.9484\n","Epoch 118/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9772\n","Epoch 118: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 587s 746ms/step - loss: 0.0872 - accuracy: 0.9772 - val_loss: 10.8456 - val_accuracy: 0.9113\n","Epoch 119/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9771\n","Epoch 119: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 566s 719ms/step - loss: 0.0814 - accuracy: 0.9771 - val_loss: 2.3150 - val_accuracy: 0.9287\n","Epoch 120/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9789\n","Epoch 120: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 577s 733ms/step - loss: 0.0914 - accuracy: 0.9789 - val_loss: 1.6059 - val_accuracy: 0.9433\n","Epoch 121/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9800\n","Epoch 121: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 576s 731ms/step - loss: 0.0857 - accuracy: 0.9800 - val_loss: 3.2090 - val_accuracy: 0.9270\n","Epoch 122/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9785\n","Epoch 122: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 590s 749ms/step - loss: 0.0768 - accuracy: 0.9785 - val_loss: 0.4537 - val_accuracy: 0.9517\n","Epoch 123/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9837\n","Epoch 123: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 564s 716ms/step - loss: 0.0609 - accuracy: 0.9837 - val_loss: 1.9213 - val_accuracy: 0.9220\n","Epoch 124/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9823\n","Epoch 124: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 579s 735ms/step - loss: 0.0629 - accuracy: 0.9823 - val_loss: 1.7144 - val_accuracy: 0.9366\n","Epoch 125/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9832\n","Epoch 125: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 581s 738ms/step - loss: 0.0634 - accuracy: 0.9832 - val_loss: 0.6505 - val_accuracy: 0.9529\n","Epoch 126/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9837\n","Epoch 126: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 576s 732ms/step - loss: 0.0678 - accuracy: 0.9837 - val_loss: 1.5583 - val_accuracy: 0.9551\n","Epoch 127/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9821\n","Epoch 127: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 576s 732ms/step - loss: 0.0697 - accuracy: 0.9821 - val_loss: 50.1489 - val_accuracy: 0.9287\n","Epoch 128/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9837\n","Epoch 128: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 554s 704ms/step - loss: 0.0689 - accuracy: 0.9837 - val_loss: 31.2418 - val_accuracy: 0.9192\n","Epoch 129/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9831\n","Epoch 129: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 593s 753ms/step - loss: 0.0697 - accuracy: 0.9831 - val_loss: 26.3909 - val_accuracy: 0.9489\n","Epoch 130/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9841\n","Epoch 130: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 584s 742ms/step - loss: 0.0727 - accuracy: 0.9841 - val_loss: 0.7932 - val_accuracy: 0.9456\n","Epoch 131/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9802\n","Epoch 131: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 556s 706ms/step - loss: 0.0777 - accuracy: 0.9802 - val_loss: 0.8375 - val_accuracy: 0.9439\n","Epoch 132/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9809\n","Epoch 132: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 568s 722ms/step - loss: 0.0771 - accuracy: 0.9809 - val_loss: 0.8786 - val_accuracy: 0.9461\n","Epoch 133/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9849\n","Epoch 133: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 572s 727ms/step - loss: 0.0649 - accuracy: 0.9849 - val_loss: 0.6426 - val_accuracy: 0.9416\n","Epoch 134/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9887\n","Epoch 134: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 574s 729ms/step - loss: 0.0499 - accuracy: 0.9887 - val_loss: 0.3951 - val_accuracy: 0.9568\n","Epoch 135/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9837\n","Epoch 135: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 569s 723ms/step - loss: 0.0668 - accuracy: 0.9837 - val_loss: 2.1748 - val_accuracy: 0.9388\n","Epoch 136/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9844\n","Epoch 136: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 558s 708ms/step - loss: 0.0728 - accuracy: 0.9844 - val_loss: 0.3911 - val_accuracy: 0.9304\n","Epoch 137/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9788\n","Epoch 137: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 583s 741ms/step - loss: 0.0793 - accuracy: 0.9788 - val_loss: 2.6409 - val_accuracy: 0.9383\n","Epoch 138/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9844\n","Epoch 138: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 558s 709ms/step - loss: 0.0600 - accuracy: 0.9844 - val_loss: 1.4142 - val_accuracy: 0.9484\n","Epoch 139/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9818\n","Epoch 139: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 572s 727ms/step - loss: 0.0777 - accuracy: 0.9818 - val_loss: 0.2441 - val_accuracy: 0.9506\n","Epoch 140/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9818\n","Epoch 140: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 575s 730ms/step - loss: 0.0704 - accuracy: 0.9818 - val_loss: 0.3436 - val_accuracy: 0.9422\n","Epoch 141/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9869\n","Epoch 141: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 581s 738ms/step - loss: 0.0495 - accuracy: 0.9869 - val_loss: 0.2298 - val_accuracy: 0.9579\n","Epoch 142/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9883\n","Epoch 142: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 570s 725ms/step - loss: 0.0459 - accuracy: 0.9883 - val_loss: 0.7687 - val_accuracy: 0.9501\n","Epoch 143/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9842\n","Epoch 143: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 549s 698ms/step - loss: 0.0614 - accuracy: 0.9842 - val_loss: 0.4450 - val_accuracy: 0.9489\n","Epoch 144/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9865\n","Epoch 144: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 590s 750ms/step - loss: 0.0544 - accuracy: 0.9865 - val_loss: 0.3371 - val_accuracy: 0.9534\n","Epoch 145/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9772\n","Epoch 145: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 553s 703ms/step - loss: 0.0937 - accuracy: 0.9772 - val_loss: 0.2798 - val_accuracy: 0.9501\n","Epoch 146/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9846\n","Epoch 146: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 564s 716ms/step - loss: 0.0608 - accuracy: 0.9846 - val_loss: 1.5181 - val_accuracy: 0.9501\n","Epoch 147/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9863\n","Epoch 147: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 583s 741ms/step - loss: 0.0542 - accuracy: 0.9863 - val_loss: 1.0356 - val_accuracy: 0.9461\n","Epoch 148/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9822\n","Epoch 148: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 572s 727ms/step - loss: 0.0661 - accuracy: 0.9822 - val_loss: 1.1244 - val_accuracy: 0.9557\n","Epoch 149/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9878\n","Epoch 149: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 569s 723ms/step - loss: 0.0487 - accuracy: 0.9878 - val_loss: 0.4724 - val_accuracy: 0.9501\n","Epoch 150/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9877\n","Epoch 150: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 574s 729ms/step - loss: 0.0472 - accuracy: 0.9877 - val_loss: 41.2961 - val_accuracy: 0.9371\n","Epoch 151/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9888\n","Epoch 151: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 680s 865ms/step - loss: 0.0487 - accuracy: 0.9888 - val_loss: 165.9332 - val_accuracy: 0.8782\n","Epoch 152/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9846\n","Epoch 152: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 634s 806ms/step - loss: 0.0633 - accuracy: 0.9846 - val_loss: 26.2509 - val_accuracy: 0.9315\n","Epoch 153/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9897\n","Epoch 153: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 521s 662ms/step - loss: 0.0471 - accuracy: 0.9897 - val_loss: 12.1361 - val_accuracy: 0.9422\n","Epoch 154/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9896\n","Epoch 154: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 540s 686ms/step - loss: 0.0419 - accuracy: 0.9896 - val_loss: 167.0963 - val_accuracy: 0.8681\n","Epoch 155/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9874\n","Epoch 155: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 519s 660ms/step - loss: 0.0521 - accuracy: 0.9874 - val_loss: 10.7395 - val_accuracy: 0.9040\n","Epoch 156/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9846\n","Epoch 156: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 658s 836ms/step - loss: 0.0692 - accuracy: 0.9846 - val_loss: 1.0659 - val_accuracy: 0.9360\n","Epoch 157/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9850\n","Epoch 157: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 537s 682ms/step - loss: 0.0659 - accuracy: 0.9850 - val_loss: 9.1601 - val_accuracy: 0.8906\n","Epoch 158/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9889\n","Epoch 158: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 256s 325ms/step - loss: 0.0500 - accuracy: 0.9889 - val_loss: 1.5937 - val_accuracy: 0.9327\n","Epoch 159/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9893\n","Epoch 159: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 246s 313ms/step - loss: 0.0442 - accuracy: 0.9893 - val_loss: 35.2964 - val_accuracy: 0.8749\n","Epoch 160/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9884\n","Epoch 160: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 246s 313ms/step - loss: 0.0464 - accuracy: 0.9884 - val_loss: 11.9746 - val_accuracy: 0.8973\n","Epoch 161/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9907\n","Epoch 161: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 238s 302ms/step - loss: 0.0425 - accuracy: 0.9907 - val_loss: 7.5487 - val_accuracy: 0.9029\n","Epoch 162/250\n","787/787 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9743\n","Epoch 162: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 237s 301ms/step - loss: 0.1045 - accuracy: 0.9743 - val_loss: 15.6840 - val_accuracy: 0.8620\n","Epoch 163/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9835\n","Epoch 163: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 240s 304ms/step - loss: 0.0649 - accuracy: 0.9835 - val_loss: 82.6300 - val_accuracy: 0.8676\n","Epoch 164/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9878\n","Epoch 164: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 244s 309ms/step - loss: 0.0493 - accuracy: 0.9878 - val_loss: 41.2288 - val_accuracy: 0.8603\n","Epoch 165/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9916\n","Epoch 165: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 237s 301ms/step - loss: 0.0387 - accuracy: 0.9916 - val_loss: 45.4025 - val_accuracy: 0.8805\n","Epoch 166/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9908\n","Epoch 166: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 235s 298ms/step - loss: 0.0334 - accuracy: 0.9908 - val_loss: 10.7565 - val_accuracy: 0.9052\n","Epoch 167/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.9847\n","Epoch 167: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 239s 304ms/step - loss: 0.0607 - accuracy: 0.9847 - val_loss: 17.5148 - val_accuracy: 0.8805\n","Epoch 168/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9877\n","Epoch 168: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 242s 308ms/step - loss: 0.0547 - accuracy: 0.9877 - val_loss: 10.6869 - val_accuracy: 0.9085\n","Epoch 169/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9893\n","Epoch 169: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 245s 311ms/step - loss: 0.0442 - accuracy: 0.9893 - val_loss: 3.6597 - val_accuracy: 0.9540\n","Epoch 170/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9896\n","Epoch 170: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 244s 310ms/step - loss: 0.0435 - accuracy: 0.9896 - val_loss: 1.6686 - val_accuracy: 0.9377\n","Epoch 171/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9863\n","Epoch 171: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 252s 320ms/step - loss: 0.0458 - accuracy: 0.9863 - val_loss: 3.6181 - val_accuracy: 0.9388\n","Epoch 172/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9861\n","Epoch 172: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 241s 307ms/step - loss: 0.0567 - accuracy: 0.9861 - val_loss: 14.1135 - val_accuracy: 0.9456\n","Epoch 173/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9869\n","Epoch 173: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 286ms/step - loss: 0.0542 - accuracy: 0.9869 - val_loss: 26.0879 - val_accuracy: 0.9085\n","Epoch 174/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9800\n","Epoch 174: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 287ms/step - loss: 0.0841 - accuracy: 0.9800 - val_loss: 0.8276 - val_accuracy: 0.9506\n","Epoch 175/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9868\n","Epoch 175: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 289ms/step - loss: 0.0596 - accuracy: 0.9868 - val_loss: 0.5522 - val_accuracy: 0.9349\n","Epoch 176/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9875\n","Epoch 176: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 285ms/step - loss: 0.0534 - accuracy: 0.9875 - val_loss: 2.7556 - val_accuracy: 0.9355\n","Epoch 177/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9859\n","Epoch 177: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 285ms/step - loss: 0.0588 - accuracy: 0.9859 - val_loss: 0.7222 - val_accuracy: 0.9501\n","Epoch 178/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9869\n","Epoch 178: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 285ms/step - loss: 0.0590 - accuracy: 0.9869 - val_loss: 0.4382 - val_accuracy: 0.9557\n","Epoch 179/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9819\n","Epoch 179: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 285ms/step - loss: 0.0770 - accuracy: 0.9819 - val_loss: 0.7912 - val_accuracy: 0.9506\n","Epoch 180/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9855\n","Epoch 180: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 286ms/step - loss: 0.0618 - accuracy: 0.9855 - val_loss: 0.5621 - val_accuracy: 0.9360\n","Epoch 181/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9879\n","Epoch 181: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 286ms/step - loss: 0.0529 - accuracy: 0.9879 - val_loss: 0.5822 - val_accuracy: 0.9489\n","Epoch 182/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9875\n","Epoch 182: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 287ms/step - loss: 0.0575 - accuracy: 0.9875 - val_loss: 4.2672 - val_accuracy: 0.9293\n","Epoch 183/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9812\n","Epoch 183: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 289ms/step - loss: 0.0812 - accuracy: 0.9812 - val_loss: 3.3003 - val_accuracy: 0.9226\n","Epoch 184/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9853\n","Epoch 184: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 224s 284ms/step - loss: 0.0523 - accuracy: 0.9853 - val_loss: 2.3191 - val_accuracy: 0.9366\n","Epoch 185/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9859\n","Epoch 185: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 224s 284ms/step - loss: 0.0539 - accuracy: 0.9859 - val_loss: 1.3186 - val_accuracy: 0.9310\n","Epoch 186/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9896\n","Epoch 186: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 287ms/step - loss: 0.0434 - accuracy: 0.9896 - val_loss: 0.6334 - val_accuracy: 0.9405\n","Epoch 187/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9840\n","Epoch 187: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0679 - accuracy: 0.9840 - val_loss: 8.8366 - val_accuracy: 0.9018\n","Epoch 188/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9883\n","Epoch 188: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 287ms/step - loss: 0.0459 - accuracy: 0.9883 - val_loss: 19.7113 - val_accuracy: 0.8507\n","Epoch 189/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9874\n","Epoch 189: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0437 - accuracy: 0.9874 - val_loss: 2.6292 - val_accuracy: 0.9226\n","Epoch 190/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9900\n","Epoch 190: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0405 - accuracy: 0.9900 - val_loss: 44.3472 - val_accuracy: 0.8636\n","Epoch 191/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9881\n","Epoch 191: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 231s 293ms/step - loss: 0.0551 - accuracy: 0.9881 - val_loss: 18.5224 - val_accuracy: 0.8810\n","Epoch 192/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9917\n","Epoch 192: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 285ms/step - loss: 0.0316 - accuracy: 0.9917 - val_loss: 269.8829 - val_accuracy: 0.8311\n","Epoch 193/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9911\n","Epoch 193: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 286ms/step - loss: 0.0393 - accuracy: 0.9911 - val_loss: 233.0219 - val_accuracy: 0.8210\n","Epoch 194/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9860\n","Epoch 194: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 286ms/step - loss: 0.0643 - accuracy: 0.9860 - val_loss: 646.4337 - val_accuracy: 0.8288\n","Epoch 195/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9891\n","Epoch 195: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0430 - accuracy: 0.9891 - val_loss: 18.7584 - val_accuracy: 0.8687\n","Epoch 196/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9877\n","Epoch 196: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0497 - accuracy: 0.9877 - val_loss: 77.9337 - val_accuracy: 0.8462\n","Epoch 197/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9865\n","Epoch 197: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 289ms/step - loss: 0.0529 - accuracy: 0.9865 - val_loss: 24.2407 - val_accuracy: 0.9024\n","Epoch 198/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9894\n","Epoch 198: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0409 - accuracy: 0.9894 - val_loss: 14.0847 - val_accuracy: 0.9074\n","Epoch 199/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9921\n","Epoch 199: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 228s 289ms/step - loss: 0.0338 - accuracy: 0.9921 - val_loss: 1.7803 - val_accuracy: 0.9405\n","Epoch 200/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9887\n","Epoch 200: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 224s 285ms/step - loss: 0.0480 - accuracy: 0.9887 - val_loss: 4.0304 - val_accuracy: 0.9411\n","Epoch 201/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9921\n","Epoch 201: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 285ms/step - loss: 0.0318 - accuracy: 0.9921 - val_loss: 1.7931 - val_accuracy: 0.9529\n","Epoch 202/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9863\n","Epoch 202: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 224s 285ms/step - loss: 0.0553 - accuracy: 0.9863 - val_loss: 24.3645 - val_accuracy: 0.9169\n","Epoch 203/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9898\n","Epoch 203: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 288ms/step - loss: 0.0413 - accuracy: 0.9898 - val_loss: 21.5098 - val_accuracy: 0.8793\n","Epoch 204/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.9889\n","Epoch 204: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0546 - accuracy: 0.9889 - val_loss: 133.1872 - val_accuracy: 0.8844\n","Epoch 205/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9893\n","Epoch 205: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 287ms/step - loss: 0.0373 - accuracy: 0.9893 - val_loss: 17.6406 - val_accuracy: 0.9147\n","Epoch 206/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9813\n","Epoch 206: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 228s 290ms/step - loss: 0.0865 - accuracy: 0.9813 - val_loss: 20.5112 - val_accuracy: 0.8378\n","Epoch 207/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9872\n","Epoch 207: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 233s 296ms/step - loss: 0.0526 - accuracy: 0.9872 - val_loss: 82.2136 - val_accuracy: 0.8378\n","Epoch 208/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9835\n","Epoch 208: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 231s 293ms/step - loss: 0.0701 - accuracy: 0.9835 - val_loss: 76.0946 - val_accuracy: 0.8580\n","Epoch 209/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9900\n","Epoch 209: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 230s 292ms/step - loss: 0.0453 - accuracy: 0.9900 - val_loss: 124.6841 - val_accuracy: 0.8490\n","Epoch 210/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9907\n","Epoch 210: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 230s 293ms/step - loss: 0.0418 - accuracy: 0.9907 - val_loss: 26.2398 - val_accuracy: 0.8973\n","Epoch 211/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9940\n","Epoch 211: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 231s 294ms/step - loss: 0.0260 - accuracy: 0.9940 - val_loss: 46.9216 - val_accuracy: 0.8861\n","Epoch 212/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9847\n","Epoch 212: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 231s 293ms/step - loss: 0.0666 - accuracy: 0.9847 - val_loss: 220.3842 - val_accuracy: 0.8496\n","Epoch 213/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9881\n","Epoch 213: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 231s 293ms/step - loss: 0.0484 - accuracy: 0.9881 - val_loss: 212.4209 - val_accuracy: 0.8378\n","Epoch 214/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9903\n","Epoch 214: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 233s 295ms/step - loss: 0.0450 - accuracy: 0.9903 - val_loss: 152.3308 - val_accuracy: 0.8412\n","Epoch 215/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9874\n","Epoch 215: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 233s 296ms/step - loss: 0.0490 - accuracy: 0.9874 - val_loss: 201.5735 - val_accuracy: 0.8681\n","Epoch 216/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9844\n","Epoch 216: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 229s 291ms/step - loss: 0.0616 - accuracy: 0.9844 - val_loss: 13.2342 - val_accuracy: 0.9018\n","Epoch 217/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9860\n","Epoch 217: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 231s 293ms/step - loss: 0.0609 - accuracy: 0.9860 - val_loss: 18.2061 - val_accuracy: 0.9130\n","Epoch 218/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9874\n","Epoch 218: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 231s 293ms/step - loss: 0.0517 - accuracy: 0.9874 - val_loss: 167.3104 - val_accuracy: 0.8519\n","Epoch 219/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9896\n","Epoch 219: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 230s 292ms/step - loss: 0.0390 - accuracy: 0.9896 - val_loss: 282.5333 - val_accuracy: 0.8580\n","Epoch 220/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9894\n","Epoch 220: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 230s 292ms/step - loss: 0.0443 - accuracy: 0.9894 - val_loss: 476.8027 - val_accuracy: 0.8328\n","Epoch 221/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9906\n","Epoch 221: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 229s 291ms/step - loss: 0.0400 - accuracy: 0.9906 - val_loss: 540.9406 - val_accuracy: 0.8547\n","Epoch 222/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9910\n","Epoch 222: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 232s 295ms/step - loss: 0.0322 - accuracy: 0.9910 - val_loss: 601.0945 - val_accuracy: 0.8277\n","Epoch 223/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9938\n","Epoch 223: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 287ms/step - loss: 0.0256 - accuracy: 0.9938 - val_loss: 455.4766 - val_accuracy: 0.8187\n","Epoch 224/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9919\n","Epoch 224: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 288ms/step - loss: 0.0349 - accuracy: 0.9919 - val_loss: 775.6024 - val_accuracy: 0.8384\n","Epoch 225/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9919\n","Epoch 225: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 287ms/step - loss: 0.0349 - accuracy: 0.9919 - val_loss: 457.3839 - val_accuracy: 0.8154\n","Epoch 226/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9901\n","Epoch 226: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 228s 290ms/step - loss: 0.0436 - accuracy: 0.9901 - val_loss: 419.6630 - val_accuracy: 0.8288\n","Epoch 227/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9900\n","Epoch 227: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 289ms/step - loss: 0.0456 - accuracy: 0.9900 - val_loss: 227.4333 - val_accuracy: 0.7929\n","Epoch 228/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9873\n","Epoch 228: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 224s 285ms/step - loss: 0.0491 - accuracy: 0.9873 - val_loss: 420.5501 - val_accuracy: 0.8300\n","Epoch 229/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9854\n","Epoch 229: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 287ms/step - loss: 0.0714 - accuracy: 0.9854 - val_loss: 19.1997 - val_accuracy: 0.8496\n","Epoch 230/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9893\n","Epoch 230: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 228s 289ms/step - loss: 0.0447 - accuracy: 0.9893 - val_loss: 88.1896 - val_accuracy: 0.8137\n","Epoch 231/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9910\n","Epoch 231: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 286ms/step - loss: 0.0366 - accuracy: 0.9910 - val_loss: 73.0134 - val_accuracy: 0.8367\n","Epoch 232/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9872\n","Epoch 232: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 285ms/step - loss: 0.0510 - accuracy: 0.9872 - val_loss: 62.6917 - val_accuracy: 0.8569\n","Epoch 233/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9875\n","Epoch 233: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 224s 285ms/step - loss: 0.0476 - accuracy: 0.9875 - val_loss: 165.0387 - val_accuracy: 0.8563\n","Epoch 234/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9922\n","Epoch 234: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0366 - accuracy: 0.9922 - val_loss: 43.2607 - val_accuracy: 0.8535\n","Epoch 235/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9935\n","Epoch 235: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 286ms/step - loss: 0.0276 - accuracy: 0.9935 - val_loss: 101.7962 - val_accuracy: 0.8838\n","Epoch 236/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9957\n","Epoch 236: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 286ms/step - loss: 0.0173 - accuracy: 0.9957 - val_loss: 98.0701 - val_accuracy: 0.8631\n","Epoch 237/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9833\n","Epoch 237: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 225s 286ms/step - loss: 0.0667 - accuracy: 0.9833 - val_loss: 75.7878 - val_accuracy: 0.8962\n","Epoch 238/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9897\n","Epoch 238: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 228s 289ms/step - loss: 0.0388 - accuracy: 0.9897 - val_loss: 49.7305 - val_accuracy: 0.9080\n","Epoch 239/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9822\n","Epoch 239: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0946 - accuracy: 0.9822 - val_loss: 63.3344 - val_accuracy: 0.8939\n","Epoch 240/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9888\n","Epoch 240: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0490 - accuracy: 0.9888 - val_loss: 145.1689 - val_accuracy: 0.8805\n","Epoch 241/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9897\n","Epoch 241: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 287ms/step - loss: 0.0403 - accuracy: 0.9897 - val_loss: 42.0037 - val_accuracy: 0.9119\n","Epoch 242/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9906\n","Epoch 242: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0388 - accuracy: 0.9906 - val_loss: 192.5745 - val_accuracy: 0.8367\n","Epoch 243/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9939\n","Epoch 243: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 289ms/step - loss: 0.0354 - accuracy: 0.9939 - val_loss: 56.8753 - val_accuracy: 0.9186\n","Epoch 244/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9919\n","Epoch 244: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 228s 290ms/step - loss: 0.0292 - accuracy: 0.9919 - val_loss: 1129.7581 - val_accuracy: 0.8872\n","Epoch 245/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9912\n","Epoch 245: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 232s 295ms/step - loss: 0.0303 - accuracy: 0.9912 - val_loss: 1286.6637 - val_accuracy: 0.8614\n","Epoch 246/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9906\n","Epoch 246: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0434 - accuracy: 0.9906 - val_loss: 1793.4568 - val_accuracy: 0.8563\n","Epoch 247/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9902\n","Epoch 247: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 226s 287ms/step - loss: 0.0381 - accuracy: 0.9902 - val_loss: 889.2518 - val_accuracy: 0.8575\n","Epoch 248/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9896\n","Epoch 248: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0378 - accuracy: 0.9896 - val_loss: 741.7986 - val_accuracy: 0.8726\n","Epoch 249/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9920\n","Epoch 249: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0361 - accuracy: 0.9920 - val_loss: 552.2676 - val_accuracy: 0.8575\n","Epoch 250/250\n","787/787 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9928\n","Epoch 250: val_accuracy did not improve from 0.96072\n","787/787 [==============================] - 227s 288ms/step - loss: 0.0260 - accuracy: 0.9928 - val_loss: 2280.8733 - val_accuracy: 0.8698\n","66/66 [==============================] - 15s 217ms/step\n","[[7.9628720e-04 6.8757785e-05 6.7158864e-05 ... 4.9177079e-11\n","  9.9904138e-01 2.1242361e-08]\n"," [1.3169889e-04 1.3487451e-04 3.8919501e-05 ... 7.7774188e-05\n","  8.3931547e-05 9.9786842e-01]\n"," [3.4082479e-10 3.7276325e-06 4.5536950e-08 ... 5.9724336e-08\n","  3.5745362e-08 4.2563031e-32]\n"," ...\n"," [1.6620243e-10 9.7190658e-12 1.5599302e-15 ... 5.7178309e-26\n","  1.0000000e+00 0.0000000e+00]\n"," [9.9999738e-01 1.3898914e-06 6.7234475e-11 ... 2.8910515e-07\n","  2.9623539e-08 1.8757978e-21]\n"," [9.3632210e-05 5.9336342e-04 5.0240103e-04 ... 1.1699909e-04\n","  9.3027949e-05 9.5760814e-05]]\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Alexnet_code.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1ST_yrafE2p_4JHcbvSpBBYMW1lWIw5hD\n","\"\"\"\n","\n","from __future__ import print_function\n","import keras\n","import pandas as pd\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import os\n","import time\n","from openpyxl import load_workbook\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n","from keras.layers import BatchNormalization\n","from PIL import Image\n","import tensorflow as tf\n","\n","import xlsxwriter\n","import cv2\n","from random import shuffle\n","\n","\n","\n","\n","start1= time.time()\n","\n","# Training parameters\n","batch_size = 10  # orig paper trained all networks with batch_size=128\n","epochs = 250\n","data_augmentation = False\n","num_classes = 12\n","test_img_num = 2083\n","result_excel_link = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Result/fruit_batch10_epoch250.xlsx'\n","Train_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TRAIN'\n","TEST_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TEST'\n","label_link= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL_encoded_file.xlsx'\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 2\n","n = 2\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'AlexNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","##(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","\n","\n","def label_img(name):\n","    # print(name)\n","    filename1 = os.fsdecode(name)\n","    # print(filename1)\n","    s=[]\n","    s= filename1.split('_')\n","    # print(s)\n","    # print(int(s[0]))\n","    #word_label = name.split('-')[0]\n","    #if word_label == 'golden_retriever': return np.array([1, 0])\n","    #elif word_label == 'shetland_sheepdog' : return np.array([0, 1])\n","    wb = load_workbook(label_link)\n","    ws = wb.active\n","    #for cell in ws.columns[1]:  #here column 9 is value is what i am testing which is Column X of my example.\n","    for x in range(2, 18454):\n","        #for y in range(1,):\n","        cell= ws.cell(row=x, column=1)\n","        # print(cell.value)\n","        # print(cell.value)\n","        if cell.value == int(s[0]) :\n","               #print ws.cell(column=12).value\n","                #a= ws.columns(row).value\n","                #b= ws.columns(col).value\n","                cell2= ws.cell(row=x, column=2)\n","\n","                label1= cell2.value\n","                # print(label1)\n","                break\n","        #else:\n","                #print('hi')\n","    return label1\n","\n","def load_training_data():\n","    train_data = []\n","    for img in os.listdir(Train_DIR):\n","        #print(img)\n","        temp2 = img.replace('.png','')\n","        label = label_img(temp2)\n","        #print(label)\n","        path = os.path.join(Train_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        # print(path)\n","        # print(label)\n","        img2= cv2.imread(path)\n","        # cv2_imshow(img2)\n","        #img = img.convert('L')\n","        # img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        #path3= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/training_441_segmented_polar_A1_A2_A4'\n","        resized_img1 = cv2.resize(img2, (224,224), interpolation = cv2.INTER_AREA)\n","        #cv2.imwrite(path3+ '/' + temp2 , resized_img1 )\n","        train_data.append([np.array(resized_img1), label])\n","\n","    shuffle(train_data)\n","    return train_data\n","\n","\n","train_data = load_training_data()\n","\n","# s = [i[1] for i in train_data]\n","# print(train_data)\n","\n","# df2 = pd.DataFrame(np.array(df2),columns=['Label'])\n","# df2['Label'] = pd.Categorical(df2.Label)\n","IMG_SIZE = 224\n","# trainImages = np.array([i[0] for i in train_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","trainImages = np.array([i[0] for i in train_data])\n","# df = pd.read_csv()\n","# trainLabels = tf.convert_to_tensor(np.array([i[1] for i in train_data]).astype(np.int_))\n","trainLabels = np.array([i[1] for i in train_data])\n","print(trainLabels[2]) #successful till here\n","#print(trainLabels)\n","# trainLabels = to_categorical(trainLabels)                                                          #problem here\n","trainLabels = keras.utils.to_categorical(trainLabels, num_classes)\n","print(trainLabels[2])\n","\n","\n","\n","def load_test_data():\n","    test_data = []\n","    for img in os.listdir(TEST_DIR):\n","        temp1 = img.replace('.png','')\n","        # print(temp1)\n","        label = label_img(temp1)\n","        path = os.path.join(TEST_DIR, img)\n","        # temp1 = img\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img1= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        resized_img2 = cv2.resize(img1, (224,224), interpolation = cv2.INTER_AREA)\n","        #path2= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/testing folder_segmented_resized'\n","        #cv2.imwrite(path2+ '/' + temp1 , resized_img2 )\n","        test_data.append([np.array(resized_img2), label,temp1])\n","        # print(label)\n","\n","    shuffle(test_data)\n","    return test_data\n","\n","\n","test_data = load_test_data()\n","#plt.imshow(test_data[10][0], cmap = 'gist_gray')\n","\n","\n","# # convert it to tensorflow              ########################\n","# tensor1 = tf.convert_to_tensor(numpy_array)\n","# print(tensor1)\n","\n","# testImages = np.array([i[0] for i in test_data],dtype=object) #.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","# testLabels = tf.convert_to_tensor(np.array([i[1] for i in test_data]).astype(np.int_))\n","\n","testImages = np.array([i[0] for i in test_data])\n","testLabels = np.array([i[1] for i in test_data])\n","test_img_name = np.array([i[2] for i in test_data])\n","\n","\n","#testLabels = keras.utils.to_categorical(testLabels, num_classes)\n","\n","# Input image dimensions.\n","input_shape = trainImages.shape[1:]\n","\n","# Normalize data.\n","##trainImages = trainImages.astype('float32') / 255\n","##testImages = testImages.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","##if subtract_pixel_mean:\n","    ##trainImages_mean = np.mean(trainImages, axis=0)\n","    ##trainImages -= trainImages_mean\n","    ##testImages -= trainImages_mean\n","\n","#print('x_train shape:', x_train.shape)\n","#print(x_train.shape[0], 'train samples')\n","#print(x_test.shape[0], 'test samples')\n","#print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","#y_train = keras.utils.to_categorical(y_train, num_classes)\n","#y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","model = Sequential()\n","\n","# 1st Convolutional Layer\n","model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),strides=(4,4), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation before passing it to the next layer\n","model.add(BatchNormalization())\n","\n","# 2nd Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 3rd Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 5th Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# Passing it to a dense layer\n","model.add(Flatten())\n","# 1st Dense Layer\n","model.add(Dense(4096, input_shape=(224*224*3,)))\n","model.add(Activation('relu'))\n","# Add Dropout to prevent overfitting\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 2nd Dense Layer\n","model.add(Dense(4096))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 3rd Dense Layer\n","model.add(Dense(1000))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Dense Layer\n","model.add(Dense(100))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","\n","# Output Layer\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))\n","\n","model.summary()\n","model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","\n","# Prepare model model saving directory.\n","save_dir = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Alexnet_Model'\n","model_name = 'Alexnet_model_fNet_batch10_epoch250.h5'\n","saved_model= save_dir + '/' + model_name\n","\n","# saved_model= save_dir + '/' + model_name\n","# saved_model_final = load_model(saved_model)\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200) #EPOCH CHANGING HERE\n","mc = ModelCheckpoint(saved_model , monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","\n","# print(type(trainImages))\n","# print(type(trainLabels))\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainImages= np.asarray(trainImages).astype(np.float32)\n","# trainLabels= np.asarray(trainLabels).astype(np.float32)\n","model.fit(trainImages, trainLabels, batch_size= batch_size, epochs = epochs, verbose=1, validation_split=0.1846, shuffle=True, callbacks=[es, mc])\n","\n","saved_model_final = load_model(saved_model)\n","\n","\n","# Score trained model.\n","preds = saved_model_final.predict(testImages)\n","print(preds)\n","\n","end1= time.time()\n","time_taken= end1- start1\n","\n","workbook = xlsxwriter.Workbook(result_excel_link)\n","worksheet = workbook.add_worksheet()\n","\n","for i in range(0,test_img_num):\n","        #wb1 = load_workbook('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        #ws1 = wb1.active\n","        #a = ws1.cell(row= i+1, column=1)\n","        #a.value = testLabels[i]\n","        #b = ws1.cell(row= i+1, column=2)\n","        #b.value = np.argmax(preds [i])\n","        #ws1.save('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        name = test_img_name[i]\n","        worksheet.write(i+1, 0 , name)\n","        a = testLabels[i]\n","        worksheet.write(i+1, 1 , a)\n","        b = np.argmax(preds[i])\n","        worksheet.write(i+1, 2 , b)\n","        if a==b:\n","             worksheet.write(i+1, 3 , 1)\n","        else:\n","             worksheet.write(i+1, 3 , 0)\n","\n","# worksheet.write(1, 4 , time_taken)\n","\n","workbook.close()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPna1/v8hiT2BYXsO9xwD+E","mount_file_id":"1imHtM52REtMkZJ4Tl7xqpdh0rM1en9Yk","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
