{"cells":[{"cell_type":"markdown","metadata":{},"source":["Epoch 50"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Learning rate:  0.001\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_3 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n","                                                                                                  \n"," conv2d_26 (Conv2D)          (None, 224, 224, 16)         448       ['input_3[0][0]']             \n","                                                                                                  \n"," batch_normalization_20 (Ba  (None, 224, 224, 16)         64        ['conv2d_26[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_20 (Activation)  (None, 224, 224, 16)         0         ['batch_normalization_20[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_27 (Conv2D)          (None, 224, 224, 16)         272       ['activation_20[0][0]']       \n","                                                                                                  \n"," batch_normalization_21 (Ba  (None, 224, 224, 16)         64        ['conv2d_27[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_21 (Activation)  (None, 224, 224, 16)         0         ['batch_normalization_21[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_28 (Conv2D)          (None, 224, 224, 16)         2320      ['activation_21[0][0]']       \n","                                                                                                  \n"," batch_normalization_22 (Ba  (None, 224, 224, 16)         64        ['conv2d_28[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_22 (Activation)  (None, 224, 224, 16)         0         ['batch_normalization_22[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_30 (Conv2D)          (None, 224, 224, 64)         1088      ['activation_20[0][0]']       \n","                                                                                                  \n"," conv2d_29 (Conv2D)          (None, 224, 224, 64)         1088      ['activation_22[0][0]']       \n","                                                                                                  \n"," add_6 (Add)                 (None, 224, 224, 64)         0         ['conv2d_30[0][0]',           \n","                                                                     'conv2d_29[0][0]']           \n","                                                                                                  \n"," batch_normalization_23 (Ba  (None, 224, 224, 64)         256       ['add_6[0][0]']               \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_23 (Activation)  (None, 224, 224, 64)         0         ['batch_normalization_23[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_31 (Conv2D)          (None, 112, 112, 64)         4160      ['activation_23[0][0]']       \n","                                                                                                  \n"," batch_normalization_24 (Ba  (None, 112, 112, 64)         256       ['conv2d_31[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_24 (Activation)  (None, 112, 112, 64)         0         ['batch_normalization_24[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_32 (Conv2D)          (None, 112, 112, 64)         36928     ['activation_24[0][0]']       \n","                                                                                                  \n"," batch_normalization_25 (Ba  (None, 112, 112, 64)         256       ['conv2d_32[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_25 (Activation)  (None, 112, 112, 64)         0         ['batch_normalization_25[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_34 (Conv2D)          (None, 112, 112, 128)        8320      ['add_6[0][0]']               \n","                                                                                                  \n"," conv2d_33 (Conv2D)          (None, 112, 112, 128)        8320      ['activation_25[0][0]']       \n","                                                                                                  \n"," add_7 (Add)                 (None, 112, 112, 128)        0         ['conv2d_34[0][0]',           \n","                                                                     'conv2d_33[0][0]']           \n","                                                                                                  \n"," batch_normalization_26 (Ba  (None, 112, 112, 128)        512       ['add_7[0][0]']               \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_26 (Activation)  (None, 112, 112, 128)        0         ['batch_normalization_26[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_35 (Conv2D)          (None, 56, 56, 128)          16512     ['activation_26[0][0]']       \n","                                                                                                  \n"," batch_normalization_27 (Ba  (None, 56, 56, 128)          512       ['conv2d_35[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_27 (Activation)  (None, 56, 56, 128)          0         ['batch_normalization_27[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_36 (Conv2D)          (None, 56, 56, 128)          147584    ['activation_27[0][0]']       \n","                                                                                                  \n"," batch_normalization_28 (Ba  (None, 56, 56, 128)          512       ['conv2d_36[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_28 (Activation)  (None, 56, 56, 128)          0         ['batch_normalization_28[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_38 (Conv2D)          (None, 56, 56, 256)          33024     ['add_7[0][0]']               \n","                                                                                                  \n"," conv2d_37 (Conv2D)          (None, 56, 56, 256)          33024     ['activation_28[0][0]']       \n","                                                                                                  \n"," add_8 (Add)                 (None, 56, 56, 256)          0         ['conv2d_38[0][0]',           \n","                                                                     'conv2d_37[0][0]']           \n","                                                                                                  \n"," batch_normalization_29 (Ba  (None, 56, 56, 256)          1024      ['add_8[0][0]']               \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_29 (Activation)  (None, 56, 56, 256)          0         ['batch_normalization_29[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," average_pooling2d_2 (Avera  (None, 7, 7, 256)            0         ['activation_29[0][0]']       \n"," gePooling2D)                                                                                     \n","                                                                                                  \n"," flatten_2 (Flatten)         (None, 12544)                0         ['average_pooling2d_2[0][0]'] \n","                                                                                                  \n"," dense_2 (Dense)             (None, 12)                   150540    ['flatten_2[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 447148 (1.71 MB)\n","Trainable params: 445388 (1.70 MB)\n","Non-trainable params: 1760 (6.88 KB)\n","__________________________________________________________________________________________________\n","ResNet11v2\n","Epoch 1/50\n","772/772 [==============================] - ETA: 0s - loss: 1.2203 - accuracy: 0.7237\n","Epoch 1: val_accuracy improved from -inf to 0.81813, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 1518s 2s/step - loss: 1.2203 - accuracy: 0.7237 - val_loss: 0.9048 - val_accuracy: 0.8181\n","Epoch 2/50\n","772/772 [==============================] - ETA: 0s - loss: 0.7136 - accuracy: 0.8499\n","Epoch 2: val_accuracy improved from 0.81813 to 0.82850, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 2166s 3s/step - loss: 0.7136 - accuracy: 0.8499 - val_loss: 0.7569 - val_accuracy: 0.8285\n","Epoch 3/50\n","772/772 [==============================] - ETA: 0s - loss: 0.5884 - accuracy: 0.8784\n","Epoch 3: val_accuracy improved from 0.82850 to 0.87202, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 2403s 3s/step - loss: 0.5884 - accuracy: 0.8784 - val_loss: 0.6513 - val_accuracy: 0.8720\n","Epoch 4/50\n","772/772 [==============================] - ETA: 0s - loss: 0.4994 - accuracy: 0.9005\n","Epoch 4: val_accuracy improved from 0.87202 to 0.87876, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 2409s 3s/step - loss: 0.4994 - accuracy: 0.9005 - val_loss: 0.6322 - val_accuracy: 0.8788\n","Epoch 5/50\n","772/772 [==============================] - ETA: 0s - loss: 0.4504 - accuracy: 0.9113\n","Epoch 5: val_accuracy improved from 0.87876 to 0.89430, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 2371s 3s/step - loss: 0.4504 - accuracy: 0.9113 - val_loss: 0.5562 - val_accuracy: 0.8943\n","Epoch 6/50\n","772/772 [==============================] - ETA: 0s - loss: 0.3939 - accuracy: 0.9194\n","Epoch 6: val_accuracy improved from 0.89430 to 0.91244, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 2360s 3s/step - loss: 0.3939 - accuracy: 0.9194 - val_loss: 0.4602 - val_accuracy: 0.9124\n","Epoch 7/50\n","772/772 [==============================] - ETA: 0s - loss: 0.3392 - accuracy: 0.9387\n","Epoch 7: val_accuracy did not improve from 0.91244\n","772/772 [==============================] - 2342s 3s/step - loss: 0.3392 - accuracy: 0.9387 - val_loss: 0.5070 - val_accuracy: 0.9041\n","Epoch 8/50\n","772/772 [==============================] - ETA: 0s - loss: 0.3316 - accuracy: 0.9354\n","Epoch 8: val_accuracy improved from 0.91244 to 0.92073, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 2364s 3s/step - loss: 0.3316 - accuracy: 0.9354 - val_loss: 0.4454 - val_accuracy: 0.9207\n","Epoch 9/50\n","772/772 [==============================] - ETA: 0s - loss: 0.3048 - accuracy: 0.9409\n","Epoch 9: val_accuracy improved from 0.92073 to 0.92539, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 2392s 3s/step - loss: 0.3048 - accuracy: 0.9409 - val_loss: 0.4662 - val_accuracy: 0.9254\n","Epoch 10/50\n","772/772 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.9509\n","Epoch 10: val_accuracy did not improve from 0.92539\n","772/772 [==============================] - 2392s 3s/step - loss: 0.2777 - accuracy: 0.9509 - val_loss: 0.4700 - val_accuracy: 0.9057\n","Epoch 11/50\n","772/772 [==============================] - ETA: 0s - loss: 0.2612 - accuracy: 0.9525\n","Epoch 11: val_accuracy did not improve from 0.92539\n","772/772 [==============================] - 2383s 3s/step - loss: 0.2612 - accuracy: 0.9525 - val_loss: 0.4423 - val_accuracy: 0.9021\n","Epoch 12/50\n","772/772 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9566\n","Epoch 12: val_accuracy improved from 0.92539 to 0.93057, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 2390s 3s/step - loss: 0.2409 - accuracy: 0.9566 - val_loss: 0.4114 - val_accuracy: 0.9306\n","Epoch 13/50\n","772/772 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.9535\n","Epoch 13: val_accuracy did not improve from 0.93057\n","772/772 [==============================] - 2410s 3s/step - loss: 0.2438 - accuracy: 0.9535 - val_loss: 0.3750 - val_accuracy: 0.9301\n","Epoch 14/50\n","772/772 [==============================] - ETA: 0s - loss: 0.2262 - accuracy: 0.9580\n","Epoch 14: val_accuracy improved from 0.93057 to 0.93523, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 2401s 3s/step - loss: 0.2262 - accuracy: 0.9580 - val_loss: 0.3890 - val_accuracy: 0.9352\n","Epoch 15/50\n","772/772 [==============================] - ETA: 0s - loss: 0.2154 - accuracy: 0.9613\n","Epoch 15: val_accuracy improved from 0.93523 to 0.93679, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 1768s 2s/step - loss: 0.2154 - accuracy: 0.9613 - val_loss: 0.3807 - val_accuracy: 0.9368\n","Epoch 16/50\n","772/772 [==============================] - ETA: 0s - loss: 0.2021 - accuracy: 0.9646\n","Epoch 16: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 1532s 2s/step - loss: 0.2021 - accuracy: 0.9646 - val_loss: 0.4706 - val_accuracy: 0.9244\n","Epoch 17/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9734\n","Epoch 17: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 1754s 2s/step - loss: 0.1774 - accuracy: 0.9734 - val_loss: 0.3980 - val_accuracy: 0.9342\n","Epoch 18/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9701\n","Epoch 18: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 1791s 2s/step - loss: 0.1768 - accuracy: 0.9701 - val_loss: 0.4052 - val_accuracy: 0.9218\n","Epoch 19/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.9698\n","Epoch 19: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 1787s 2s/step - loss: 0.1836 - accuracy: 0.9698 - val_loss: 0.4237 - val_accuracy: 0.9249\n","Epoch 20/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9706\n","Epoch 20: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 1796s 2s/step - loss: 0.1777 - accuracy: 0.9706 - val_loss: 0.4886 - val_accuracy: 0.9067\n","Epoch 21/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9764\n","Epoch 21: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 1768s 2s/step - loss: 0.1529 - accuracy: 0.9764 - val_loss: 0.4736 - val_accuracy: 0.9233\n","Epoch 22/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9702\n","Epoch 22: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 1783s 2s/step - loss: 0.1712 - accuracy: 0.9702 - val_loss: 0.4157 - val_accuracy: 0.9228\n","Epoch 23/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9727\n","Epoch 23: val_accuracy improved from 0.93679 to 0.94093, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 1774s 2s/step - loss: 0.1599 - accuracy: 0.9727 - val_loss: 0.4007 - val_accuracy: 0.9409\n","Epoch 24/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9795\n","Epoch 24: val_accuracy did not improve from 0.94093\n","772/772 [==============================] - 1761s 2s/step - loss: 0.1406 - accuracy: 0.9795 - val_loss: 0.3678 - val_accuracy: 0.9404\n","Epoch 25/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9789\n","Epoch 25: val_accuracy did not improve from 0.94093\n","772/772 [==============================] - 1778s 2s/step - loss: 0.1396 - accuracy: 0.9789 - val_loss: 0.4362 - val_accuracy: 0.9295\n","Epoch 26/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.9786\n","Epoch 26: val_accuracy did not improve from 0.94093\n","772/772 [==============================] - 1941s 3s/step - loss: 0.1428 - accuracy: 0.9786 - val_loss: 0.4806 - val_accuracy: 0.9218\n","Epoch 27/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1476 - accuracy: 0.9767\n","Epoch 27: val_accuracy did not improve from 0.94093\n","772/772 [==============================] - 2375s 3s/step - loss: 0.1476 - accuracy: 0.9767 - val_loss: 0.3898 - val_accuracy: 0.9332\n","Epoch 28/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.9856\n","Epoch 28: val_accuracy did not improve from 0.94093\n","772/772 [==============================] - 2350s 3s/step - loss: 0.1176 - accuracy: 0.9856 - val_loss: 0.4068 - val_accuracy: 0.9358\n","Epoch 29/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1347 - accuracy: 0.9791\n","Epoch 29: val_accuracy did not improve from 0.94093\n","772/772 [==============================] - 2383s 3s/step - loss: 0.1347 - accuracy: 0.9791 - val_loss: 0.4459 - val_accuracy: 0.9218\n","Epoch 30/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9821\n","Epoch 30: val_accuracy did not improve from 0.94093\n","772/772 [==============================] - 2393s 3s/step - loss: 0.1282 - accuracy: 0.9821 - val_loss: 0.4147 - val_accuracy: 0.9347\n","Epoch 31/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9824\n","Epoch 31: val_accuracy did not improve from 0.94093\n","772/772 [==============================] - 2385s 3s/step - loss: 0.1301 - accuracy: 0.9824 - val_loss: 0.4063 - val_accuracy: 0.9383\n","Epoch 32/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9815\n","Epoch 32: val_accuracy did not improve from 0.94093\n","772/772 [==============================] - 2409s 3s/step - loss: 0.1249 - accuracy: 0.9815 - val_loss: 0.4305 - val_accuracy: 0.9363\n","Epoch 33/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9829\n","Epoch 33: val_accuracy did not improve from 0.94093\n","772/772 [==============================] - 2471s 3s/step - loss: 0.1244 - accuracy: 0.9829 - val_loss: 0.3840 - val_accuracy: 0.9332\n","Epoch 34/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9860\n","Epoch 34: val_accuracy did not improve from 0.94093\n","772/772 [==============================] - 2378s 3s/step - loss: 0.1108 - accuracy: 0.9860 - val_loss: 0.6048 - val_accuracy: 0.9104\n","Epoch 35/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9798\n","Epoch 35: val_accuracy improved from 0.94093 to 0.94197, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 2420s 3s/step - loss: 0.1272 - accuracy: 0.9798 - val_loss: 0.3834 - val_accuracy: 0.9420\n","Epoch 36/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.9832\n","Epoch 36: val_accuracy did not improve from 0.94197\n","772/772 [==============================] - 2413s 3s/step - loss: 0.1141 - accuracy: 0.9832 - val_loss: 0.4232 - val_accuracy: 0.9295\n","Epoch 37/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9869\n","Epoch 37: val_accuracy did not improve from 0.94197\n","772/772 [==============================] - 2388s 3s/step - loss: 0.1033 - accuracy: 0.9869 - val_loss: 0.4156 - val_accuracy: 0.9383\n","Epoch 38/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9781\n","Epoch 38: val_accuracy did not improve from 0.94197\n","772/772 [==============================] - 2453s 3s/step - loss: 0.1243 - accuracy: 0.9781 - val_loss: 0.4181 - val_accuracy: 0.9358\n","Epoch 39/50\n","772/772 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9872\n","Epoch 39: val_accuracy did not improve from 0.94197\n","772/772 [==============================] - 2450s 3s/step - loss: 0.0978 - accuracy: 0.9872 - val_loss: 0.3867 - val_accuracy: 0.9420\n","Epoch 40/50\n","772/772 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9880\n","Epoch 40: val_accuracy did not improve from 0.94197\n","772/772 [==============================] - 2408s 3s/step - loss: 0.0993 - accuracy: 0.9880 - val_loss: 0.4394 - val_accuracy: 0.9326\n","Epoch 41/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9845\n","Epoch 41: val_accuracy did not improve from 0.94197\n","772/772 [==============================] - 2415s 3s/step - loss: 0.1109 - accuracy: 0.9845 - val_loss: 0.6086 - val_accuracy: 0.9083\n","Epoch 42/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9851\n","Epoch 42: val_accuracy did not improve from 0.94197\n","772/772 [==============================] - 2437s 3s/step - loss: 0.1071 - accuracy: 0.9851 - val_loss: 0.6519 - val_accuracy: 0.8803\n","Epoch 43/50\n","772/772 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9911\n","Epoch 43: val_accuracy did not improve from 0.94197\n","772/772 [==============================] - 2433s 3s/step - loss: 0.0880 - accuracy: 0.9911 - val_loss: 0.4106 - val_accuracy: 0.9363\n","Epoch 44/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1063 - accuracy: 0.9839\n","Epoch 44: val_accuracy improved from 0.94197 to 0.94611, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch50.h5\n","772/772 [==============================] - 2417s 3s/step - loss: 0.1063 - accuracy: 0.9839 - val_loss: 0.4043 - val_accuracy: 0.9461\n","Epoch 45/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9870\n","Epoch 45: val_accuracy did not improve from 0.94611\n","772/772 [==============================] - 2435s 3s/step - loss: 0.1016 - accuracy: 0.9870 - val_loss: 0.4830 - val_accuracy: 0.9181\n","Epoch 46/50\n","772/772 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9898\n","Epoch 46: val_accuracy did not improve from 0.94611\n","772/772 [==============================] - 2438s 3s/step - loss: 0.0913 - accuracy: 0.9898 - val_loss: 0.4699 - val_accuracy: 0.9259\n","Epoch 47/50\n","772/772 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9894\n","Epoch 47: val_accuracy did not improve from 0.94611\n","772/772 [==============================] - 2433s 3s/step - loss: 0.0911 - accuracy: 0.9894 - val_loss: 0.4463 - val_accuracy: 0.9358\n","Epoch 48/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9865\n","Epoch 48: val_accuracy did not improve from 0.94611\n","772/772 [==============================] - 2433s 3s/step - loss: 0.1061 - accuracy: 0.9865 - val_loss: 0.4110 - val_accuracy: 0.9430\n","Epoch 49/50\n","772/772 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9850\n","Epoch 49: val_accuracy did not improve from 0.94611\n","772/772 [==============================] - 2389s 3s/step - loss: 0.1017 - accuracy: 0.9850 - val_loss: 0.4171 - val_accuracy: 0.9352\n","Epoch 50/50\n","772/772 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9894\n","Epoch 50: val_accuracy did not improve from 0.94611\n","772/772 [==============================] - 2351s 3s/step - loss: 0.0919 - accuracy: 0.9894 - val_loss: 0.3672 - val_accuracy: 0.9420\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["66/66 [==============================] - 146s 2s/step\n","[[8.4893503e-07 1.3724293e-12 0.0000000e+00 ... 2.6070281e-34\n","  1.9403239e-10 9.9999917e-01]\n"," [5.0499630e-15 3.1129332e-10 0.0000000e+00 ... 2.5237270e-35\n","  3.4786624e-13 1.0000000e+00]\n"," [9.8694978e-13 3.6583652e-04 5.8633735e-33 ... 4.2652381e-28\n","  4.5551898e-13 9.9963415e-01]\n"," ...\n"," [1.6418337e-33 3.7916079e-23 1.2026554e-23 ... 3.7622409e-25\n","  5.3447087e-27 0.0000000e+00]\n"," [9.7111206e-26 1.2307850e-13 0.0000000e+00 ... 1.7927700e-25\n","  2.8672202e-11 1.0000000e+00]\n"," [2.4003631e-26 4.3397381e-14 0.0000000e+00 ... 8.7049100e-37\n","  1.6830968e-19 1.0000000e+00]]\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"resnet_overfitting_included.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1x-dq99w3wIjofB1NVMmbTqd-4Eh7O4OE\n","\"\"\"\n","\n","from __future__ import print_function\n","import keras\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import os\n","import time\n","from openpyxl import load_workbook\n","\n","import xlsxwriter\n","import cv2\n","from random import shuffle\n","\n","\n","\n","\n","start1= time.time()\n","\n","# Training parameters\n","batch_size = 10  # orig paper trained all networks with batch_size=128\n","epochs = 50\n","data_augmentation = False\n","num_classes = 12\n","test_img_num = 2083\n","result_excel_link = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Result/fruit_batch10_epoch50.xlsx'\n","Train_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TRAIN'\n","TEST_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TEST'\n","label_link= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL_encoded_file.xlsx'\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 2\n","n = 1\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'ResNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","##(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","\n","\n","def label_img(name):\n","    #print(name)\n","    filename1 = os.fsdecode(name)\n","    #print(filename1)\n","    s=[]\n","    s= filename1.split('_')\n","    #print(int(s[0]))\n","    #word_label = name.split('-')[0]\n","    #if word_label == 'golden_retriever': return np.array([1, 0])\n","    #elif word_label == 'shetland_sheepdog' : return np.array([0, 1])\n","    wb = load_workbook(label_link)\n","    ws = wb.active\n","    #for cell in ws.columns[1]:  #here column 9 is value is what i am testing which is Column X of my example.\n","    for x in range(2, 18454):\n","        #for y in range(1,):\n","        cell= ws.cell(row=x, column=1)\n","        #print(cell.value)\n","        if cell.value == int(s[0]) :\n","               #print ws.cell(column=12).value\n","                #a= ws.columns(row).value\n","                #b= ws.columns(col).value\n","                cell2= ws.cell(row=x, column=2)\n","\n","                label1= cell2.value\n","                #print(label1)\n","                break\n","        #else:\n","                #print('hi')\n","    return label1\n","\n","def load_training_data():\n","    train_data = []\n","    for img in os.listdir(Train_DIR):\n","        #print(img)\n","        temp2 = img.replace('.png','')\n","        label = label_img(temp2)\n","        #print(label)\n","        path = os.path.join(Train_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img2= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        #path3= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/training_441_segmented_polar_A1_A2_A4'\n","        resized_img1 = cv2.resize(img2, (224,224), interpolation = cv2.INTER_AREA)\n","        #cv2.imwrite(path3+ '/' + temp2 , resized_img1 )\n","        train_data.append([np.array(resized_img1), label])\n","\n","    shuffle(train_data)\n","    return train_data\n","\n","\n","train_data = load_training_data()\n","\n","trainImages = np.array([i[0] for i in train_data])#.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","trainLabels = np.array([i[1] for i in train_data])\n","# print(trainLabels[5])\n","#print(trainLabels)\n","trainLabels = to_categorical(trainLabels)\n","# #trainLabels = keras.utils.to_categorical(trainLabels, num_classes)\n","# print('now')\n","# print(trainLabels[5])\n","\n","\n","\n","def load_test_data():\n","    test_data = []\n","    for img in os.listdir(TEST_DIR):\n","        temp1 = img.replace('.png','')\n","        label = label_img(temp1)\n","        path = os.path.join(TEST_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img1= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        resized_img2 = cv2.resize(img1, (224,224), interpolation = cv2.INTER_AREA)\n","        #path2= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/testing folder_segmented_resized'\n","        #cv2.imwrite(path2+ '/' + temp1 , resized_img2 )\n","        test_data.append([np.array(resized_img2), label,temp1])\n","    shuffle(test_data)\n","    return test_data\n","\n","\n","test_data = load_test_data()\n","#plt.imshow(test_data[10][0], cmap = 'gist_gray')\n","\n","testImages = np.array([i[0] for i in test_data])#.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","testLabels = np.array([i[1] for i in test_data])\n","test_img_name = np.array([i[2] for i in test_data])\n","#testLabels = keras.utils.to_categorical(testLabels, num_classes)\n","\n","# Input image dimensions.\n","input_shape = trainImages.shape[1:]\n","\n","# Normalize data.\n","##trainImages = trainImages.astype('float32') / 255\n","##testImages = testImages.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","##if subtract_pixel_mean:\n","    ##trainImages_mean = np.mean(trainImages, axis=0)\n","    ##trainImages -= trainImages_mean\n","    ##testImages -= trainImages_mean\n","\n","#print('x_train shape:', x_train.shape)\n","#print(x_train.shape[0], 'train samples')\n","#print(x_test.shape[0], 'test samples')\n","#print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","#y_train = keras.utils.to_categorical(y_train, num_classes)\n","#y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","def resnet_layer(inputs,\n","                 num_filters=16,\n","                 kernel_size=3,\n","                 strides=1,\n","                 activation='relu',\n","                 batch_normalization=True,\n","                 conv_first=True):\n","    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n","\n","    # Arguments\n","        inputs (tensor): input tensor from input image or previous layer\n","        num_filters (int): Conv2D number of filters\n","        kernel_size (int): Conv2D square kernel dimensions\n","        strides (int): Conv2D square stride dimensions\n","        activation (string): activation name\n","        batch_normalization (bool): whether to include batch normalization\n","        conv_first (bool): conv-bn-activation (True) or\n","            bn-activation-conv (False)\n","\n","    # Returns\n","        x (tensor): tensor as input to the next layer\n","    \"\"\"\n","    conv = Conv2D(num_filters,\n","                  kernel_size=kernel_size,\n","                  strides=strides,\n","                  padding='same',\n","                  kernel_initializer='he_normal',\n","                  kernel_regularizer=l2(1e-4))\n","\n","    x = inputs\n","    if conv_first:\n","        x = conv(x)\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","    else:\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","        x = conv(x)\n","    return x\n","\n","\n","\n","\n","\n","def resnet_v2(input_shape, depth, num_classes):\n","    \"\"\"ResNet Version 2 Model builder [b]\n","\n","    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n","    bottleneck layer\n","    First shortcut connection per layer is 1 x 1 Conv2D.\n","    Second and onwards shortcut connection is identity.\n","    At the beginning of each stage, the feature map size is halved (downsampled)\n","    by a convolutional layer with strides=2, while the number of filter maps is\n","    doubled. Within each stage, the layers have the same number filters and the\n","    same filter map sizes.\n","    Features maps sizes:\n","    conv1  : 32x32,  16\n","    stage 0: 32x32,  64\n","    stage 1: 16x16, 128\n","    stage 2:  8x8,  256\n","\n","    # Arguments\n","        input_shape (tensor): shape of input image tensor\n","        depth (int): number of core convolutional layers\n","        num_classes (int): number of classes (CIFAR10 has 10)\n","\n","    # Returns\n","        model (Model): Keras model instance\n","    \"\"\"\n","    if (depth - 2) % 9 != 0:\n","        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n","    # Start model definition.\n","    num_filters_in = 16\n","    num_res_blocks = int((depth - 2) / 9)\n","\n","    inputs = Input(shape=input_shape)\n","    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n","    x = resnet_layer(inputs=inputs,\n","                     num_filters=num_filters_in,\n","                     conv_first=True)\n","\n","    # Instantiate the stack of residual units\n","    for stage in range(3):\n","        for res_block in range(num_res_blocks):\n","            activation = 'relu'\n","            batch_normalization = True\n","            strides = 1\n","            if stage == 0:\n","                num_filters_out = num_filters_in * 4\n","                if res_block == 0:  # first layer and first stage\n","                    activation = None\n","                    batch_normalization = False\n","            else:\n","                num_filters_out = num_filters_in * 2\n","                if res_block == 0:  # first layer but not first stage\n","                    strides = 2    # downsample\n","\n","            # bottleneck residual unit\n","            y = resnet_layer(inputs=x,\n","                             num_filters=num_filters_in,\n","                             kernel_size=1,\n","                             strides=strides,\n","                             activation=activation,\n","                             batch_normalization=batch_normalization,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_in,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_out,\n","                             kernel_size=1,\n","                             conv_first=False)\n","            if res_block == 0:\n","                # linear projection residual shortcut connection to match\n","                # changed dims\n","                x = resnet_layer(inputs=x,\n","                                 num_filters=num_filters_out,\n","                                 kernel_size=1,\n","                                 strides=strides,\n","                                 activation=None,\n","                                 batch_normalization=False)\n","            x = keras.layers.add([x, y])\n","\n","        num_filters_in = num_filters_out\n","\n","    # Add classifier on top.\n","    # v2 has BN-ReLU before Pooling\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = AveragePooling2D(pool_size=8)(x)\n","    y = Flatten()(x)\n","    outputs = Dense(num_classes,\n","                    activation='softmax',\n","                    kernel_initializer='he_normal')(y)\n","\n","    # Instantiate model.\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","model = resnet_v2(input_shape=input_shape, depth=depth, num_classes= num_classes)\n","# if version == 2:\n","#     model = resnet_v2(input_shape=input_shape, depth=depth, num_classes= num_classes)\n","# else:\n","#     model = resnet_v1(input_shape=input_shape, depth=depth)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(learning_rate=lr_schedule(0)),\n","              metrics=['accuracy'])\n","model.summary()\n","print(model_type)\n","\n","# Prepare model model saving directory.\n","save_dir = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model'\n","model_name = 'Resnet_model_fNet_batch10_epoch50.h5'\n","saved_model= save_dir + '/' + model_name\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n","mc = ModelCheckpoint(saved_model , monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","\n","\n","\n","model.fit(trainImages, trainLabels, batch_size= batch_size, epochs = epochs, verbose=1, validation_split=0.2, shuffle=True, callbacks=[es, mc])\n","\n","\n","saved_model_final = load_model(saved_model)\n","\n","# Score trained model.\n","preds = saved_model_final.predict(testImages)\n","print(preds)\n","\n","end1= time.time()\n","time_taken= end1- start1\n","\n","workbook = xlsxwriter.Workbook(result_excel_link)\n","worksheet = workbook.add_worksheet()\n","\n","for i in range(0,test_img_num):\n","        #wb1 = load_workbook('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        #ws1 = wb1.active\n","        #a = ws1.cell(row= i+1, column=1)\n","        #a.value = testLabels[i]\n","        #b = ws1.cell(row= i+1, column=2)\n","        #b.value = np.argmax(preds [i])\n","        #ws1.save('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        name = test_img_name[i]\n","        worksheet.write(i+1, 0 , name)\n","        a = testLabels[i]\n","        worksheet.write(i+1, 1 , a)\n","        b = np.argmax(preds[i])\n","        worksheet.write(i+1, 2 , b)\n","        if a==b:\n","             worksheet.write(i+1, 3 , 1)\n","        else:\n","             worksheet.write(i+1, 3 , 0)\n","\n","# worksheet.write(1, 4 , time_taken)\n","\n","workbook.close()"]},{"cell_type":"markdown","metadata":{},"source":["Epoch 75"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Learning rate:  0.001\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_4 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n","                                                                                                  \n"," conv2d_39 (Conv2D)          (None, 224, 224, 16)         448       ['input_4[0][0]']             \n","                                                                                                  \n"," batch_normalization_30 (Ba  (None, 224, 224, 16)         64        ['conv2d_39[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_30 (Activation)  (None, 224, 224, 16)         0         ['batch_normalization_30[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_40 (Conv2D)          (None, 224, 224, 16)         272       ['activation_30[0][0]']       \n","                                                                                                  \n"," batch_normalization_31 (Ba  (None, 224, 224, 16)         64        ['conv2d_40[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_31 (Activation)  (None, 224, 224, 16)         0         ['batch_normalization_31[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_41 (Conv2D)          (None, 224, 224, 16)         2320      ['activation_31[0][0]']       \n","                                                                                                  \n"," batch_normalization_32 (Ba  (None, 224, 224, 16)         64        ['conv2d_41[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_32 (Activation)  (None, 224, 224, 16)         0         ['batch_normalization_32[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_43 (Conv2D)          (None, 224, 224, 64)         1088      ['activation_30[0][0]']       \n","                                                                                                  \n"," conv2d_42 (Conv2D)          (None, 224, 224, 64)         1088      ['activation_32[0][0]']       \n","                                                                                                  \n"," add_9 (Add)                 (None, 224, 224, 64)         0         ['conv2d_43[0][0]',           \n","                                                                     'conv2d_42[0][0]']           \n","                                                                                                  \n"," batch_normalization_33 (Ba  (None, 224, 224, 64)         256       ['add_9[0][0]']               \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_33 (Activation)  (None, 224, 224, 64)         0         ['batch_normalization_33[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_44 (Conv2D)          (None, 112, 112, 64)         4160      ['activation_33[0][0]']       \n","                                                                                                  \n"," batch_normalization_34 (Ba  (None, 112, 112, 64)         256       ['conv2d_44[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_34 (Activation)  (None, 112, 112, 64)         0         ['batch_normalization_34[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_45 (Conv2D)          (None, 112, 112, 64)         36928     ['activation_34[0][0]']       \n","                                                                                                  \n"," batch_normalization_35 (Ba  (None, 112, 112, 64)         256       ['conv2d_45[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_35 (Activation)  (None, 112, 112, 64)         0         ['batch_normalization_35[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_47 (Conv2D)          (None, 112, 112, 128)        8320      ['add_9[0][0]']               \n","                                                                                                  \n"," conv2d_46 (Conv2D)          (None, 112, 112, 128)        8320      ['activation_35[0][0]']       \n","                                                                                                  \n"," add_10 (Add)                (None, 112, 112, 128)        0         ['conv2d_47[0][0]',           \n","                                                                     'conv2d_46[0][0]']           \n","                                                                                                  \n"," batch_normalization_36 (Ba  (None, 112, 112, 128)        512       ['add_10[0][0]']              \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_36 (Activation)  (None, 112, 112, 128)        0         ['batch_normalization_36[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_48 (Conv2D)          (None, 56, 56, 128)          16512     ['activation_36[0][0]']       \n","                                                                                                  \n"," batch_normalization_37 (Ba  (None, 56, 56, 128)          512       ['conv2d_48[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_37 (Activation)  (None, 56, 56, 128)          0         ['batch_normalization_37[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_49 (Conv2D)          (None, 56, 56, 128)          147584    ['activation_37[0][0]']       \n","                                                                                                  \n"," batch_normalization_38 (Ba  (None, 56, 56, 128)          512       ['conv2d_49[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_38 (Activation)  (None, 56, 56, 128)          0         ['batch_normalization_38[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_51 (Conv2D)          (None, 56, 56, 256)          33024     ['add_10[0][0]']              \n","                                                                                                  \n"," conv2d_50 (Conv2D)          (None, 56, 56, 256)          33024     ['activation_38[0][0]']       \n","                                                                                                  \n"," add_11 (Add)                (None, 56, 56, 256)          0         ['conv2d_51[0][0]',           \n","                                                                     'conv2d_50[0][0]']           \n","                                                                                                  \n"," batch_normalization_39 (Ba  (None, 56, 56, 256)          1024      ['add_11[0][0]']              \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_39 (Activation)  (None, 56, 56, 256)          0         ['batch_normalization_39[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," average_pooling2d_3 (Avera  (None, 7, 7, 256)            0         ['activation_39[0][0]']       \n"," gePooling2D)                                                                                     \n","                                                                                                  \n"," flatten_3 (Flatten)         (None, 12544)                0         ['average_pooling2d_3[0][0]'] \n","                                                                                                  \n"," dense_3 (Dense)             (None, 12)                   150540    ['flatten_3[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 447148 (1.71 MB)\n","Trainable params: 445388 (1.70 MB)\n","Non-trainable params: 1760 (6.88 KB)\n","__________________________________________________________________________________________________\n","ResNet11v2\n","Epoch 1/75\n","772/772 [==============================] - ETA: 0s - loss: 1.0977 - accuracy: 0.7325\n","Epoch 1: val_accuracy improved from -inf to 0.86218, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch75.h5\n","772/772 [==============================] - 2458s 3s/step - loss: 1.0977 - accuracy: 0.7325 - val_loss: 0.6657 - val_accuracy: 0.8622\n","Epoch 2/75\n","772/772 [==============================] - ETA: 0s - loss: 0.6735 - accuracy: 0.8533\n","Epoch 2: val_accuracy did not improve from 0.86218\n","772/772 [==============================] - 2432s 3s/step - loss: 0.6735 - accuracy: 0.8533 - val_loss: 0.6630 - val_accuracy: 0.8420\n","Epoch 3/75\n","772/772 [==============================] - ETA: 0s - loss: 0.5240 - accuracy: 0.8913\n","Epoch 3: val_accuracy improved from 0.86218 to 0.89689, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch75.h5\n","772/772 [==============================] - 2468s 3s/step - loss: 0.5240 - accuracy: 0.8913 - val_loss: 0.4800 - val_accuracy: 0.8969\n","Epoch 4/75\n","772/772 [==============================] - ETA: 0s - loss: 0.4564 - accuracy: 0.9067\n","Epoch 4: val_accuracy did not improve from 0.89689\n","772/772 [==============================] - 2442s 3s/step - loss: 0.4564 - accuracy: 0.9067 - val_loss: 0.6666 - val_accuracy: 0.8389\n","Epoch 5/75\n","772/772 [==============================] - ETA: 0s - loss: 0.3928 - accuracy: 0.9203\n","Epoch 5: val_accuracy did not improve from 0.89689\n","772/772 [==============================] - 2431s 3s/step - loss: 0.3928 - accuracy: 0.9203 - val_loss: 0.4968 - val_accuracy: 0.8964\n","Epoch 6/75\n","772/772 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.9335\n","Epoch 6: val_accuracy improved from 0.89689 to 0.90984, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch75.h5\n","772/772 [==============================] - 2427s 3s/step - loss: 0.3503 - accuracy: 0.9335 - val_loss: 0.4142 - val_accuracy: 0.9098\n","Epoch 7/75\n","772/772 [==============================] - ETA: 0s - loss: 0.3357 - accuracy: 0.9348\n","Epoch 7: val_accuracy improved from 0.90984 to 0.91813, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch75.h5\n","772/772 [==============================] - 2488s 3s/step - loss: 0.3357 - accuracy: 0.9348 - val_loss: 0.4085 - val_accuracy: 0.9181\n","Epoch 8/75\n","772/772 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.9479\n","Epoch 8: val_accuracy improved from 0.91813 to 0.91865, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch75.h5\n","772/772 [==============================] - 2551s 3s/step - loss: 0.2866 - accuracy: 0.9479 - val_loss: 0.3921 - val_accuracy: 0.9187\n","Epoch 9/75\n","772/772 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9420\n","Epoch 9: val_accuracy improved from 0.91865 to 0.92850, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch75.h5\n","772/772 [==============================] - 2509s 3s/step - loss: 0.2863 - accuracy: 0.9420 - val_loss: 0.3587 - val_accuracy: 0.9285\n","Epoch 10/75\n","772/772 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.9569\n","Epoch 10: val_accuracy did not improve from 0.92850\n","772/772 [==============================] - 2518s 3s/step - loss: 0.2503 - accuracy: 0.9569 - val_loss: 0.3576 - val_accuracy: 0.9280\n","Epoch 11/75\n","772/772 [==============================] - ETA: 0s - loss: 0.2329 - accuracy: 0.9595\n","Epoch 11: val_accuracy did not improve from 0.92850\n","772/772 [==============================] - 2660s 3s/step - loss: 0.2329 - accuracy: 0.9595 - val_loss: 0.3941 - val_accuracy: 0.9109\n","Epoch 12/75\n","772/772 [==============================] - ETA: 0s - loss: 0.2165 - accuracy: 0.9629\n","Epoch 12: val_accuracy improved from 0.92850 to 0.93472, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch75.h5\n","772/772 [==============================] - 2810s 4s/step - loss: 0.2165 - accuracy: 0.9629 - val_loss: 0.3591 - val_accuracy: 0.9347\n","Epoch 13/75\n","772/772 [==============================] - ETA: 0s - loss: 0.2293 - accuracy: 0.9539\n","Epoch 13: val_accuracy did not improve from 0.93472\n","772/772 [==============================] - 2613s 3s/step - loss: 0.2293 - accuracy: 0.9539 - val_loss: 0.3995 - val_accuracy: 0.9166\n","Epoch 14/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1992 - accuracy: 0.9652\n","Epoch 14: val_accuracy did not improve from 0.93472\n","772/772 [==============================] - 2632s 3s/step - loss: 0.1992 - accuracy: 0.9652 - val_loss: 0.3159 - val_accuracy: 0.9290\n","Epoch 15/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.9658\n","Epoch 15: val_accuracy did not improve from 0.93472\n","772/772 [==============================] - 2999s 4s/step - loss: 0.1876 - accuracy: 0.9658 - val_loss: 0.3268 - val_accuracy: 0.9316\n","Epoch 16/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9646\n","Epoch 16: val_accuracy improved from 0.93472 to 0.94663, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch75.h5\n","772/772 [==============================] - 2847s 4s/step - loss: 0.1844 - accuracy: 0.9646 - val_loss: 0.2854 - val_accuracy: 0.9466\n","Epoch 17/75\n","772/772 [==============================] - ETA: 0s - loss: 0.2013 - accuracy: 0.9629\n","Epoch 17: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 2016s 3s/step - loss: 0.2013 - accuracy: 0.9629 - val_loss: 0.3244 - val_accuracy: 0.9368\n","Epoch 18/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.9790\n","Epoch 18: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1976s 3s/step - loss: 0.1514 - accuracy: 0.9790 - val_loss: 0.3478 - val_accuracy: 0.9332\n","Epoch 19/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9723\n","Epoch 19: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1976s 3s/step - loss: 0.1687 - accuracy: 0.9723 - val_loss: 0.2973 - val_accuracy: 0.9399\n","Epoch 20/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9742\n","Epoch 20: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1973s 3s/step - loss: 0.1569 - accuracy: 0.9742 - val_loss: 0.3432 - val_accuracy: 0.9280\n","Epoch 21/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1666 - accuracy: 0.9705\n","Epoch 21: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 2050s 3s/step - loss: 0.1666 - accuracy: 0.9705 - val_loss: 0.4248 - val_accuracy: 0.9135\n","Epoch 22/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9775\n","Epoch 22: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 2219s 3s/step - loss: 0.1481 - accuracy: 0.9775 - val_loss: 0.4551 - val_accuracy: 0.8995\n","Epoch 23/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.9813\n","Epoch 23: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 2233s 3s/step - loss: 0.1413 - accuracy: 0.9813 - val_loss: 0.3315 - val_accuracy: 0.9295\n","Epoch 24/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.9775\n","Epoch 24: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 3859s 5s/step - loss: 0.1428 - accuracy: 0.9775 - val_loss: 0.2835 - val_accuracy: 0.9425\n","Epoch 25/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9776\n","Epoch 25: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 4068s 5s/step - loss: 0.1406 - accuracy: 0.9776 - val_loss: 0.3575 - val_accuracy: 0.9316\n","Epoch 26/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9746\n","Epoch 26: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 4111s 5s/step - loss: 0.1512 - accuracy: 0.9746 - val_loss: 0.2865 - val_accuracy: 0.9451\n","Epoch 27/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9837\n","Epoch 27: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 4158s 5s/step - loss: 0.1189 - accuracy: 0.9837 - val_loss: 0.5547 - val_accuracy: 0.8984\n","Epoch 28/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9766\n","Epoch 28: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 4064s 5s/step - loss: 0.1396 - accuracy: 0.9766 - val_loss: 0.3635 - val_accuracy: 0.9321\n","Epoch 29/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9838\n","Epoch 29: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 4238s 5s/step - loss: 0.1216 - accuracy: 0.9838 - val_loss: 0.3393 - val_accuracy: 0.9358\n","Epoch 30/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9816\n","Epoch 30: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 4265s 6s/step - loss: 0.1219 - accuracy: 0.9816 - val_loss: 0.5394 - val_accuracy: 0.8845\n","Epoch 31/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9793\n","Epoch 31: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 4259s 6s/step - loss: 0.1300 - accuracy: 0.9793 - val_loss: 0.3476 - val_accuracy: 0.9306\n","Epoch 32/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9835\n","Epoch 32: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 4646s 6s/step - loss: 0.1202 - accuracy: 0.9835 - val_loss: 0.3373 - val_accuracy: 0.9352\n","Epoch 33/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1063 - accuracy: 0.9865\n","Epoch 33: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 4884s 6s/step - loss: 0.1063 - accuracy: 0.9865 - val_loss: 0.3751 - val_accuracy: 0.9259\n","Epoch 34/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9793\n","Epoch 34: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 5082s 7s/step - loss: 0.1271 - accuracy: 0.9793 - val_loss: 0.3207 - val_accuracy: 0.9420\n","Epoch 35/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9813\n","Epoch 35: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 6203s 8s/step - loss: 0.1242 - accuracy: 0.9813 - val_loss: 0.5651 - val_accuracy: 0.8974\n","Epoch 36/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9864\n","Epoch 36: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 5992s 8s/step - loss: 0.1005 - accuracy: 0.9864 - val_loss: 0.3112 - val_accuracy: 0.9451\n","Epoch 37/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9908\n","Epoch 37: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 4558s 6s/step - loss: 0.0927 - accuracy: 0.9908 - val_loss: 0.2810 - val_accuracy: 0.9420\n","Epoch 38/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1312 - accuracy: 0.9799\n","Epoch 38: val_accuracy improved from 0.94663 to 0.94767, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch75.h5\n","772/772 [==============================] - 4569s 6s/step - loss: 0.1312 - accuracy: 0.9799 - val_loss: 0.2975 - val_accuracy: 0.9477\n","Epoch 39/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9902\n","Epoch 39: val_accuracy did not improve from 0.94767\n","772/772 [==============================] - 4543s 6s/step - loss: 0.0953 - accuracy: 0.9902 - val_loss: 0.3206 - val_accuracy: 0.9409\n","Epoch 40/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1060 - accuracy: 0.9838\n","Epoch 40: val_accuracy did not improve from 0.94767\n","772/772 [==============================] - 4526s 6s/step - loss: 0.1060 - accuracy: 0.9838 - val_loss: 0.4180 - val_accuracy: 0.9166\n","Epoch 41/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9869\n","Epoch 41: val_accuracy did not improve from 0.94767\n","772/772 [==============================] - 4567s 6s/step - loss: 0.0985 - accuracy: 0.9869 - val_loss: 0.3203 - val_accuracy: 0.9425\n","Epoch 42/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9863\n","Epoch 42: val_accuracy did not improve from 0.94767\n","772/772 [==============================] - 4551s 6s/step - loss: 0.1089 - accuracy: 0.9863 - val_loss: 0.4134 - val_accuracy: 0.9062\n","Epoch 43/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9930\n","Epoch 43: val_accuracy did not improve from 0.94767\n","772/772 [==============================] - 4705s 6s/step - loss: 0.0836 - accuracy: 0.9930 - val_loss: 0.3494 - val_accuracy: 0.9358\n","Epoch 44/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9852\n","Epoch 44: val_accuracy did not improve from 0.94767\n","772/772 [==============================] - 4957s 6s/step - loss: 0.1075 - accuracy: 0.9852 - val_loss: 0.6569 - val_accuracy: 0.8902\n","Epoch 45/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0894 - accuracy: 0.9891\n","Epoch 45: val_accuracy did not improve from 0.94767\n","772/772 [==============================] - 5633s 7s/step - loss: 0.0894 - accuracy: 0.9891 - val_loss: 0.3122 - val_accuracy: 0.9461\n","Epoch 46/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9842\n","Epoch 46: val_accuracy did not improve from 0.94767\n","772/772 [==============================] - 4626s 6s/step - loss: 0.1012 - accuracy: 0.9842 - val_loss: 0.3321 - val_accuracy: 0.9389\n","Epoch 47/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9878\n","Epoch 47: val_accuracy improved from 0.94767 to 0.95026, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch75.h5\n","772/772 [==============================] - 4079s 5s/step - loss: 0.0973 - accuracy: 0.9878 - val_loss: 0.2671 - val_accuracy: 0.9503\n","Epoch 48/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9887\n","Epoch 48: val_accuracy did not improve from 0.95026\n","772/772 [==============================] - 3909s 5s/step - loss: 0.0864 - accuracy: 0.9887 - val_loss: 0.4015 - val_accuracy: 0.9223\n","Epoch 49/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9859\n","Epoch 49: val_accuracy did not improve from 0.95026\n","772/772 [==============================] - 3806s 5s/step - loss: 0.1004 - accuracy: 0.9859 - val_loss: 0.3673 - val_accuracy: 0.9332\n","Epoch 50/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9865\n","Epoch 50: val_accuracy did not improve from 0.95026\n","772/772 [==============================] - 3841s 5s/step - loss: 0.0950 - accuracy: 0.9865 - val_loss: 0.3592 - val_accuracy: 0.9440\n","Epoch 51/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9924\n","Epoch 51: val_accuracy did not improve from 0.95026\n","772/772 [==============================] - 3890s 5s/step - loss: 0.0788 - accuracy: 0.9924 - val_loss: 0.3441 - val_accuracy: 0.9358\n","Epoch 52/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9854\n","Epoch 52: val_accuracy did not improve from 0.95026\n","772/772 [==============================] - 3783s 5s/step - loss: 0.0969 - accuracy: 0.9854 - val_loss: 0.3713 - val_accuracy: 0.9306\n","Epoch 53/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9905\n","Epoch 53: val_accuracy improved from 0.95026 to 0.95181, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_batch10_epoch75.h5\n","772/772 [==============================] - 3863s 5s/step - loss: 0.0844 - accuracy: 0.9905 - val_loss: 0.2970 - val_accuracy: 0.9518\n","Epoch 54/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9933\n","Epoch 54: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 4011s 5s/step - loss: 0.0744 - accuracy: 0.9933 - val_loss: 0.6340 - val_accuracy: 0.8845\n","Epoch 55/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9839\n","Epoch 55: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 4089s 5s/step - loss: 0.0984 - accuracy: 0.9839 - val_loss: 0.3199 - val_accuracy: 0.9358\n","Epoch 56/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9905\n","Epoch 56: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3758s 5s/step - loss: 0.0817 - accuracy: 0.9905 - val_loss: 0.3182 - val_accuracy: 0.9425\n","Epoch 57/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9926\n","Epoch 57: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3760s 5s/step - loss: 0.0706 - accuracy: 0.9926 - val_loss: 0.3493 - val_accuracy: 0.9342\n","Epoch 58/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9869\n","Epoch 58: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3833s 5s/step - loss: 0.0885 - accuracy: 0.9869 - val_loss: 0.3872 - val_accuracy: 0.9383\n","Epoch 59/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9861\n","Epoch 59: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3750s 5s/step - loss: 0.0929 - accuracy: 0.9861 - val_loss: 0.3482 - val_accuracy: 0.9378\n","Epoch 60/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9929\n","Epoch 60: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3775s 5s/step - loss: 0.0714 - accuracy: 0.9929 - val_loss: 0.3245 - val_accuracy: 0.9383\n","Epoch 61/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9886\n","Epoch 61: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3779s 5s/step - loss: 0.0858 - accuracy: 0.9886 - val_loss: 0.4289 - val_accuracy: 0.9321\n","Epoch 62/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9952\n","Epoch 62: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3821s 5s/step - loss: 0.0654 - accuracy: 0.9952 - val_loss: 0.4005 - val_accuracy: 0.9373\n","Epoch 63/75\n","772/772 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9829\n","Epoch 63: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3793s 5s/step - loss: 0.1086 - accuracy: 0.9829 - val_loss: 0.3460 - val_accuracy: 0.9378\n","Epoch 64/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9920\n","Epoch 64: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3769s 5s/step - loss: 0.0802 - accuracy: 0.9920 - val_loss: 0.6947 - val_accuracy: 0.8876\n","Epoch 65/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9913\n","Epoch 65: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3700s 5s/step - loss: 0.0783 - accuracy: 0.9913 - val_loss: 0.5313 - val_accuracy: 0.9041\n","Epoch 66/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0843 - accuracy: 0.9890\n","Epoch 66: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3620s 5s/step - loss: 0.0843 - accuracy: 0.9890 - val_loss: 0.3344 - val_accuracy: 0.9451\n","Epoch 67/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9930\n","Epoch 67: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3650s 5s/step - loss: 0.0744 - accuracy: 0.9930 - val_loss: 0.3069 - val_accuracy: 0.9477\n","Epoch 68/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9911\n","Epoch 68: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3659s 5s/step - loss: 0.0776 - accuracy: 0.9911 - val_loss: 0.3150 - val_accuracy: 0.9456\n","Epoch 69/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9883\n","Epoch 69: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3636s 5s/step - loss: 0.0804 - accuracy: 0.9883 - val_loss: 0.4525 - val_accuracy: 0.9228\n","Epoch 70/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9937\n","Epoch 70: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3734s 5s/step - loss: 0.0668 - accuracy: 0.9937 - val_loss: 0.3315 - val_accuracy: 0.9383\n","Epoch 71/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9969\n","Epoch 71: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3659s 5s/step - loss: 0.0565 - accuracy: 0.9969 - val_loss: 0.4001 - val_accuracy: 0.9337\n","Epoch 72/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0925 - accuracy: 0.9854\n","Epoch 72: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3652s 5s/step - loss: 0.0925 - accuracy: 0.9854 - val_loss: 0.4154 - val_accuracy: 0.9368\n","Epoch 73/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9907\n","Epoch 73: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3608s 5s/step - loss: 0.0776 - accuracy: 0.9907 - val_loss: 0.4129 - val_accuracy: 0.9306\n","Epoch 74/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9909\n","Epoch 74: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3671s 5s/step - loss: 0.0798 - accuracy: 0.9909 - val_loss: 0.5677 - val_accuracy: 0.9078\n","Epoch 75/75\n","772/772 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9894\n","Epoch 75: val_accuracy did not improve from 0.95181\n","772/772 [==============================] - 3599s 5s/step - loss: 0.0791 - accuracy: 0.9894 - val_loss: 0.3516 - val_accuracy: 0.9461\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["66/66 [==============================] - 201s 3s/step\n","[[2.30880493e-09 1.33385469e-10 2.50976196e-24 ... 4.03891261e-25\n","  4.00505269e-17 3.13981550e-24]\n"," [1.38226230e-08 1.91164862e-10 1.36556225e-02 ... 1.69580051e-12\n","  7.80309435e-08 4.67259611e-07]\n"," [9.99996066e-01 1.73108710e-08 4.35196936e-19 ... 1.10636875e-27\n","  3.72029990e-06 1.19064874e-17]\n"," ...\n"," [1.22776623e-06 4.29860108e-07 8.25177565e-33 ... 6.68998914e-26\n","  9.99998331e-01 6.40239382e-16]\n"," [2.54371946e-09 1.38717347e-12 9.71598956e-23 ... 0.00000000e+00\n","  1.00000000e+00 6.99166257e-13]\n"," [1.64281634e-13 1.44278274e-12 4.13234353e-08 ... 6.76243904e-37\n","  9.99998808e-01 1.25385308e-16]]\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"resnet_overfitting_included.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1x-dq99w3wIjofB1NVMmbTqd-4Eh7O4OE\n","\"\"\"\n","\n","from __future__ import print_function\n","import keras\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import os\n","import time\n","from openpyxl import load_workbook\n","\n","import xlsxwriter\n","import cv2\n","from random import shuffle\n","\n","\n","\n","\n","start1= time.time()\n","\n","# Training parameters\n","batch_size = 10  # orig paper trained all networks with batch_size=128\n","epochs = 75\n","data_augmentation = False\n","num_classes = 12\n","test_img_num = 2083\n","result_excel_link = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Result/fruit_batch10_epoch75.xlsx'\n","Train_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TRAIN'\n","TEST_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TEST'\n","label_link= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL_encoded_file.xlsx'\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 2\n","n = 1\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'ResNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","##(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","\n","\n","def label_img(name):\n","    #print(name)\n","    filename1 = os.fsdecode(name)\n","    #print(filename1)\n","    s=[]\n","    s= filename1.split('_')\n","    #print(int(s[0]))\n","    #word_label = name.split('-')[0]\n","    #if word_label == 'golden_retriever': return np.array([1, 0])\n","    #elif word_label == 'shetland_sheepdog' : return np.array([0, 1])\n","    wb = load_workbook(label_link)\n","    ws = wb.active\n","    #for cell in ws.columns[1]:  #here column 9 is value is what i am testing which is Column X of my example.\n","    for x in range(2, 18454):\n","        #for y in range(1,):\n","        cell= ws.cell(row=x, column=1)\n","        #print(cell.value)\n","        if cell.value == int(s[0]) :\n","               #print ws.cell(column=12).value\n","                #a= ws.columns(row).value\n","                #b= ws.columns(col).value\n","                cell2= ws.cell(row=x, column=2)\n","\n","                label1= cell2.value\n","                #print(label1)\n","                break\n","        #else:\n","                #print('hi')\n","    return label1\n","\n","def load_training_data():\n","    train_data = []\n","    for img in os.listdir(Train_DIR):\n","        #print(img)\n","        temp2 = img.replace('.png','')\n","        label = label_img(temp2)\n","        #print(label)\n","        path = os.path.join(Train_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img2= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        #path3= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/training_441_segmented_polar_A1_A2_A4'\n","        resized_img1 = cv2.resize(img2, (224,224), interpolation = cv2.INTER_AREA)\n","        #cv2.imwrite(path3+ '/' + temp2 , resized_img1 )\n","        train_data.append([np.array(resized_img1), label])\n","\n","    shuffle(train_data)\n","    return train_data\n","\n","\n","train_data = load_training_data()\n","\n","trainImages = np.array([i[0] for i in train_data])#.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","trainLabels = np.array([i[1] for i in train_data])\n","# print(trainLabels[5])\n","#print(trainLabels)\n","trainLabels = to_categorical(trainLabels)\n","# #trainLabels = keras.utils.to_categorical(trainLabels, num_classes)\n","# print('now')\n","# print(trainLabels[5])\n","\n","\n","\n","def load_test_data():\n","    test_data = []\n","    for img in os.listdir(TEST_DIR):\n","        temp1 = img.replace('.png','')\n","        label = label_img(temp1)\n","        path = os.path.join(TEST_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img1= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        resized_img2 = cv2.resize(img1, (224,224), interpolation = cv2.INTER_AREA)\n","        #path2= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/testing folder_segmented_resized'\n","        #cv2.imwrite(path2+ '/' + temp1 , resized_img2 )\n","        test_data.append([np.array(resized_img2), label,temp1])\n","    shuffle(test_data)\n","    return test_data\n","\n","\n","test_data = load_test_data()\n","#plt.imshow(test_data[10][0], cmap = 'gist_gray')\n","\n","testImages = np.array([i[0] for i in test_data])#.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","testLabels = np.array([i[1] for i in test_data])\n","test_img_name = np.array([i[2] for i in test_data])\n","#testLabels = keras.utils.to_categorical(testLabels, num_classes)\n","\n","# Input image dimensions.\n","input_shape = trainImages.shape[1:]\n","\n","# Normalize data.\n","##trainImages = trainImages.astype('float32') / 255\n","##testImages = testImages.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","##if subtract_pixel_mean:\n","    ##trainImages_mean = np.mean(trainImages, axis=0)\n","    ##trainImages -= trainImages_mean\n","    ##testImages -= trainImages_mean\n","\n","#print('x_train shape:', x_train.shape)\n","#print(x_train.shape[0], 'train samples')\n","#print(x_test.shape[0], 'test samples')\n","#print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","#y_train = keras.utils.to_categorical(y_train, num_classes)\n","#y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","def resnet_layer(inputs,\n","                 num_filters=16,\n","                 kernel_size=3,\n","                 strides=1,\n","                 activation='relu',\n","                 batch_normalization=True,\n","                 conv_first=True):\n","    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n","\n","    # Arguments\n","        inputs (tensor): input tensor from input image or previous layer\n","        num_filters (int): Conv2D number of filters\n","        kernel_size (int): Conv2D square kernel dimensions\n","        strides (int): Conv2D square stride dimensions\n","        activation (string): activation name\n","        batch_normalization (bool): whether to include batch normalization\n","        conv_first (bool): conv-bn-activation (True) or\n","            bn-activation-conv (False)\n","\n","    # Returns\n","        x (tensor): tensor as input to the next layer\n","    \"\"\"\n","    conv = Conv2D(num_filters,\n","                  kernel_size=kernel_size,\n","                  strides=strides,\n","                  padding='same',\n","                  kernel_initializer='he_normal',\n","                  kernel_regularizer=l2(1e-4))\n","\n","    x = inputs\n","    if conv_first:\n","        x = conv(x)\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","    else:\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","        x = conv(x)\n","    return x\n","\n","\n","\n","\n","\n","def resnet_v2(input_shape, depth, num_classes):\n","    \"\"\"ResNet Version 2 Model builder [b]\n","\n","    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n","    bottleneck layer\n","    First shortcut connection per layer is 1 x 1 Conv2D.\n","    Second and onwards shortcut connection is identity.\n","    At the beginning of each stage, the feature map size is halved (downsampled)\n","    by a convolutional layer with strides=2, while the number of filter maps is\n","    doubled. Within each stage, the layers have the same number filters and the\n","    same filter map sizes.\n","    Features maps sizes:\n","    conv1  : 32x32,  16\n","    stage 0: 32x32,  64\n","    stage 1: 16x16, 128\n","    stage 2:  8x8,  256\n","\n","    # Arguments\n","        input_shape (tensor): shape of input image tensor\n","        depth (int): number of core convolutional layers\n","        num_classes (int): number of classes (CIFAR10 has 10)\n","\n","    # Returns\n","        model (Model): Keras model instance\n","    \"\"\"\n","    if (depth - 2) % 9 != 0:\n","        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n","    # Start model definition.\n","    num_filters_in = 16\n","    num_res_blocks = int((depth - 2) / 9)\n","\n","    inputs = Input(shape=input_shape)\n","    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n","    x = resnet_layer(inputs=inputs,\n","                     num_filters=num_filters_in,\n","                     conv_first=True)\n","\n","    # Instantiate the stack of residual units\n","    for stage in range(3):\n","        for res_block in range(num_res_blocks):\n","            activation = 'relu'\n","            batch_normalization = True\n","            strides = 1\n","            if stage == 0:\n","                num_filters_out = num_filters_in * 4\n","                if res_block == 0:  # first layer and first stage\n","                    activation = None\n","                    batch_normalization = False\n","            else:\n","                num_filters_out = num_filters_in * 2\n","                if res_block == 0:  # first layer but not first stage\n","                    strides = 2    # downsample\n","\n","            # bottleneck residual unit\n","            y = resnet_layer(inputs=x,\n","                             num_filters=num_filters_in,\n","                             kernel_size=1,\n","                             strides=strides,\n","                             activation=activation,\n","                             batch_normalization=batch_normalization,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_in,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_out,\n","                             kernel_size=1,\n","                             conv_first=False)\n","            if res_block == 0:\n","                # linear projection residual shortcut connection to match\n","                # changed dims\n","                x = resnet_layer(inputs=x,\n","                                 num_filters=num_filters_out,\n","                                 kernel_size=1,\n","                                 strides=strides,\n","                                 activation=None,\n","                                 batch_normalization=False)\n","            x = keras.layers.add([x, y])\n","\n","        num_filters_in = num_filters_out\n","\n","    # Add classifier on top.\n","    # v2 has BN-ReLU before Pooling\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = AveragePooling2D(pool_size=8)(x)\n","    y = Flatten()(x)\n","    outputs = Dense(num_classes,\n","                    activation='softmax',\n","                    kernel_initializer='he_normal')(y)\n","\n","    # Instantiate model.\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","model = resnet_v2(input_shape=input_shape, depth=depth, num_classes= num_classes)\n","# if version == 2:\n","#     model = resnet_v2(input_shape=input_shape, depth=depth, num_classes= num_classes)\n","# else:\n","#     model = resnet_v1(input_shape=input_shape, depth=depth)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(learning_rate=lr_schedule(0)),\n","              metrics=['accuracy'])\n","model.summary()\n","print(model_type)\n","\n","# Prepare model model saving directory.\n","save_dir = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model'\n","model_name = 'Resnet_model_fNet_batch10_epoch75.h5'\n","saved_model= save_dir + '/' + model_name\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n","mc = ModelCheckpoint(saved_model , monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","\n","\n","\n","model.fit(trainImages, trainLabels, batch_size= batch_size, epochs = epochs, verbose=1, validation_split=0.2, shuffle=True, callbacks=[es, mc])\n","\n","\n","saved_model_final = load_model(saved_model)\n","\n","# Score trained model.\n","preds = saved_model_final.predict(testImages)\n","print(preds)\n","\n","end1= time.time()\n","time_taken= end1- start1\n","\n","workbook = xlsxwriter.Workbook(result_excel_link)\n","worksheet = workbook.add_worksheet()\n","\n","for i in range(0,test_img_num):\n","        #wb1 = load_workbook('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        #ws1 = wb1.active\n","        #a = ws1.cell(row= i+1, column=1)\n","        #a.value = testLabels[i]\n","        #b = ws1.cell(row= i+1, column=2)\n","        #b.value = np.argmax(preds [i])\n","        #ws1.save('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        name = test_img_name[i]\n","        worksheet.write(i+1, 0 , name)\n","        a = testLabels[i]\n","        worksheet.write(i+1, 1 , a)\n","        b = np.argmax(preds[i])\n","        worksheet.write(i+1, 2 , b)\n","        if a==b:\n","             worksheet.write(i+1, 3 , 1)\n","        else:\n","             worksheet.write(i+1, 3 , 0)\n","\n","# worksheet.write(1, 4 , time_taken)\n","\n","workbook.close()"]},{"cell_type":"markdown","metadata":{},"source":["Epoch 100"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"resnet_overfitting_included.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1x-dq99w3wIjofB1NVMmbTqd-4Eh7O4OE\n","\"\"\"\n","\n","from __future__ import print_function\n","import keras\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import os\n","import time\n","from openpyxl import load_workbook\n","\n","import xlsxwriter\n","import cv2\n","from random import shuffle\n","\n","\n","\n","\n","start1= time.time()\n","\n","# Training parameters\n","batch_size = 10  # orig paper trained all networks with batch_size=128\n","epochs = 100\n","data_augmentation = False\n","num_classes = 12\n","test_img_num = 2083\n","result_excel_link = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Result/fruit_batch10_epoch100.xlsx'\n","Train_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TRAIN'\n","TEST_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TEST'\n","label_link= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL_encoded_file.xlsx'\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 2\n","n = 1\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'ResNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","##(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","\n","\n","def label_img(name):\n","    #print(name)\n","    filename1 = os.fsdecode(name)\n","    #print(filename1)\n","    s=[]\n","    s= filename1.split('_')\n","    #print(int(s[0]))\n","    #word_label = name.split('-')[0]\n","    #if word_label == 'golden_retriever': return np.array([1, 0])\n","    #elif word_label == 'shetland_sheepdog' : return np.array([0, 1])\n","    wb = load_workbook(label_link)\n","    ws = wb.active\n","    #for cell in ws.columns[1]:  #here column 9 is value is what i am testing which is Column X of my example.\n","    for x in range(2, 18454):\n","        #for y in range(1,):\n","        cell= ws.cell(row=x, column=1)\n","        #print(cell.value)\n","        if cell.value == int(s[0]) :\n","               #print ws.cell(column=12).value\n","                #a= ws.columns(row).value\n","                #b= ws.columns(col).value\n","                cell2= ws.cell(row=x, column=2)\n","\n","                label1= cell2.value\n","                #print(label1)\n","                break\n","        #else:\n","                #print('hi')\n","    return label1\n","\n","def load_training_data():\n","    train_data = []\n","    for img in os.listdir(Train_DIR):\n","        #print(img)\n","        temp2 = img.replace('.png','')\n","        label = label_img(temp2)\n","        #print(label)\n","        path = os.path.join(Train_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img2= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        #path3= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/training_441_segmented_polar_A1_A2_A4'\n","        resized_img1 = cv2.resize(img2, (224,224), interpolation = cv2.INTER_AREA)\n","        #cv2.imwrite(path3+ '/' + temp2 , resized_img1 )\n","        train_data.append([np.array(resized_img1), label])\n","\n","    shuffle(train_data)\n","    return train_data\n","\n","\n","train_data = load_training_data()\n","\n","trainImages = np.array([i[0] for i in train_data])#.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","trainLabels = np.array([i[1] for i in train_data])\n","# print(trainLabels[5])\n","#print(trainLabels)\n","trainLabels = to_categorical(trainLabels)\n","# #trainLabels = keras.utils.to_categorical(trainLabels, num_classes)\n","# print('now')\n","# print(trainLabels[5])\n","\n","\n","\n","def load_test_data():\n","    test_data = []\n","    for img in os.listdir(TEST_DIR):\n","        temp1 = img.replace('.png','')\n","        label = label_img(temp1)\n","        path = os.path.join(TEST_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img1= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        resized_img2 = cv2.resize(img1, (224,224), interpolation = cv2.INTER_AREA)\n","        #path2= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/testing folder_segmented_resized'\n","        #cv2.imwrite(path2+ '/' + temp1 , resized_img2 )\n","        test_data.append([np.array(resized_img2), label,temp1])\n","    shuffle(test_data)\n","    return test_data\n","\n","\n","test_data = load_test_data()\n","#plt.imshow(test_data[10][0], cmap = 'gist_gray')\n","\n","testImages = np.array([i[0] for i in test_data])#.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","testLabels = np.array([i[1] for i in test_data])\n","test_img_name = np.array([i[2] for i in test_data])\n","#testLabels = keras.utils.to_categorical(testLabels, num_classes)\n","\n","# Input image dimensions.\n","input_shape = trainImages.shape[1:]\n","\n","# Normalize data.\n","##trainImages = trainImages.astype('float32') / 255\n","##testImages = testImages.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","##if subtract_pixel_mean:\n","    ##trainImages_mean = np.mean(trainImages, axis=0)\n","    ##trainImages -= trainImages_mean\n","    ##testImages -= trainImages_mean\n","\n","#print('x_train shape:', x_train.shape)\n","#print(x_train.shape[0], 'train samples')\n","#print(x_test.shape[0], 'test samples')\n","#print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","#y_train = keras.utils.to_categorical(y_train, num_classes)\n","#y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","def resnet_layer(inputs,\n","                 num_filters=16,\n","                 kernel_size=3,\n","                 strides=1,\n","                 activation='relu',\n","                 batch_normalization=True,\n","                 conv_first=True):\n","    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n","\n","    # Arguments\n","        inputs (tensor): input tensor from input image or previous layer\n","        num_filters (int): Conv2D number of filters\n","        kernel_size (int): Conv2D square kernel dimensions\n","        strides (int): Conv2D square stride dimensions\n","        activation (string): activation name\n","        batch_normalization (bool): whether to include batch normalization\n","        conv_first (bool): conv-bn-activation (True) or\n","            bn-activation-conv (False)\n","\n","    # Returns\n","        x (tensor): tensor as input to the next layer\n","    \"\"\"\n","    conv = Conv2D(num_filters,\n","                  kernel_size=kernel_size,\n","                  strides=strides,\n","                  padding='same',\n","                  kernel_initializer='he_normal',\n","                  kernel_regularizer=l2(1e-4))\n","\n","    x = inputs\n","    if conv_first:\n","        x = conv(x)\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","    else:\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","        x = conv(x)\n","    return x\n","\n","\n","\n","\n","\n","def resnet_v2(input_shape, depth, num_classes):\n","    \"\"\"ResNet Version 2 Model builder [b]\n","\n","    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n","    bottleneck layer\n","    First shortcut connection per layer is 1 x 1 Conv2D.\n","    Second and onwards shortcut connection is identity.\n","    At the beginning of each stage, the feature map size is halved (downsampled)\n","    by a convolutional layer with strides=2, while the number of filter maps is\n","    doubled. Within each stage, the layers have the same number filters and the\n","    same filter map sizes.\n","    Features maps sizes:\n","    conv1  : 32x32,  16\n","    stage 0: 32x32,  64\n","    stage 1: 16x16, 128\n","    stage 2:  8x8,  256\n","\n","    # Arguments\n","        input_shape (tensor): shape of input image tensor\n","        depth (int): number of core convolutional layers\n","        num_classes (int): number of classes (CIFAR10 has 10)\n","\n","    # Returns\n","        model (Model): Keras model instance\n","    \"\"\"\n","    if (depth - 2) % 9 != 0:\n","        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n","    # Start model definition.\n","    num_filters_in = 16\n","    num_res_blocks = int((depth - 2) / 9)\n","\n","    inputs = Input(shape=input_shape)\n","    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n","    x = resnet_layer(inputs=inputs,\n","                     num_filters=num_filters_in,\n","                     conv_first=True)\n","\n","    # Instantiate the stack of residual units\n","    for stage in range(3):\n","        for res_block in range(num_res_blocks):\n","            activation = 'relu'\n","            batch_normalization = True\n","            strides = 1\n","            if stage == 0:\n","                num_filters_out = num_filters_in * 4\n","                if res_block == 0:  # first layer and first stage\n","                    activation = None\n","                    batch_normalization = False\n","            else:\n","                num_filters_out = num_filters_in * 2\n","                if res_block == 0:  # first layer but not first stage\n","                    strides = 2    # downsample\n","\n","            # bottleneck residual unit\n","            y = resnet_layer(inputs=x,\n","                             num_filters=num_filters_in,\n","                             kernel_size=1,\n","                             strides=strides,\n","                             activation=activation,\n","                             batch_normalization=batch_normalization,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_in,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_out,\n","                             kernel_size=1,\n","                             conv_first=False)\n","            if res_block == 0:\n","                # linear projection residual shortcut connection to match\n","                # changed dims\n","                x = resnet_layer(inputs=x,\n","                                 num_filters=num_filters_out,\n","                                 kernel_size=1,\n","                                 strides=strides,\n","                                 activation=None,\n","                                 batch_normalization=False)\n","            x = keras.layers.add([x, y])\n","\n","        num_filters_in = num_filters_out\n","\n","    # Add classifier on top.\n","    # v2 has BN-ReLU before Pooling\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = AveragePooling2D(pool_size=8)(x)\n","    y = Flatten()(x)\n","    outputs = Dense(num_classes,\n","                    activation='softmax',\n","                    kernel_initializer='he_normal')(y)\n","\n","    # Instantiate model.\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","model = resnet_v2(input_shape=input_shape, depth=depth, num_classes= num_classes)\n","# if version == 2:\n","#     model = resnet_v2(input_shape=input_shape, depth=depth, num_classes= num_classes)\n","# else:\n","#     model = resnet_v1(input_shape=input_shape, depth=depth)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(learning_rate=lr_schedule(0)),\n","              metrics=['accuracy'])\n","model.summary()\n","print(model_type)\n","\n","# Prepare model model saving directory.\n","save_dir = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model'\n","model_name = 'Resnet_model_fNet_batch10_epoch100.h5'\n","saved_model= save_dir + '/' + model_name\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n","mc = ModelCheckpoint(saved_model , monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","\n","\n","\n","model.fit(trainImages, trainLabels, batch_size= batch_size, epochs = epochs, verbose=1, validation_split=0.2, shuffle=True, callbacks=[es, mc])\n","\n","\n","saved_model_final = load_model(saved_model)\n","\n","# Score trained model.\n","preds = saved_model_final.predict(testImages)\n","print(preds)\n","\n","end1= time.time()\n","time_taken= end1- start1\n","\n","workbook = xlsxwriter.Workbook(result_excel_link)\n","worksheet = workbook.add_worksheet()\n","\n","for i in range(0,test_img_num):\n","        #wb1 = load_workbook('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        #ws1 = wb1.active\n","        #a = ws1.cell(row= i+1, column=1)\n","        #a.value = testLabels[i]\n","        #b = ws1.cell(row= i+1, column=2)\n","        #b.value = np.argmax(preds [i])\n","        #ws1.save('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        name = test_img_name[i]\n","        worksheet.write(i+1, 0 , name)\n","        a = testLabels[i]\n","        worksheet.write(i+1, 1 , a)\n","        b = np.argmax(preds[i])\n","        worksheet.write(i+1, 2 , b)\n","        if a==b:\n","             worksheet.write(i+1, 3 , 1)\n","        else:\n","             worksheet.write(i+1, 3 , 0)\n","\n","# worksheet.write(1, 4 , time_taken)\n","\n","workbook.close()"]},{"cell_type":"markdown","metadata":{},"source":["Epoch 150"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":500},"executionInfo":{"elapsed":11535368,"status":"error","timestamp":1697839562237,"user":{"displayName":"Sayantan Sikdar","userId":"16660529842037260833"},"user_tz":-330},"id":"qoBNqXkkYDHH","outputId":"cf2f7036-9408-45ca-96f5-4abbc43d48e2"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Learning rate:  0.001\n","Model: \"model_4\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_5 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n","                                                                                                  \n"," conv2d_52 (Conv2D)          (None, 224, 224, 16)         448       ['input_5[0][0]']             \n","                                                                                                  \n"," batch_normalization_40 (Ba  (None, 224, 224, 16)         64        ['conv2d_52[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_40 (Activation)  (None, 224, 224, 16)         0         ['batch_normalization_40[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_53 (Conv2D)          (None, 224, 224, 16)         272       ['activation_40[0][0]']       \n","                                                                                                  \n"," batch_normalization_41 (Ba  (None, 224, 224, 16)         64        ['conv2d_53[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_41 (Activation)  (None, 224, 224, 16)         0         ['batch_normalization_41[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_54 (Conv2D)          (None, 224, 224, 16)         2320      ['activation_41[0][0]']       \n","                                                                                                  \n"," batch_normalization_42 (Ba  (None, 224, 224, 16)         64        ['conv2d_54[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_42 (Activation)  (None, 224, 224, 16)         0         ['batch_normalization_42[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_56 (Conv2D)          (None, 224, 224, 64)         1088      ['activation_40[0][0]']       \n","                                                                                                  \n"," conv2d_55 (Conv2D)          (None, 224, 224, 64)         1088      ['activation_42[0][0]']       \n","                                                                                                  \n"," add_12 (Add)                (None, 224, 224, 64)         0         ['conv2d_56[0][0]',           \n","                                                                     'conv2d_55[0][0]']           \n","                                                                                                  \n"," batch_normalization_43 (Ba  (None, 224, 224, 64)         256       ['add_12[0][0]']              \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_43 (Activation)  (None, 224, 224, 64)         0         ['batch_normalization_43[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_57 (Conv2D)          (None, 112, 112, 64)         4160      ['activation_43[0][0]']       \n","                                                                                                  \n"," batch_normalization_44 (Ba  (None, 112, 112, 64)         256       ['conv2d_57[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_44 (Activation)  (None, 112, 112, 64)         0         ['batch_normalization_44[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_58 (Conv2D)          (None, 112, 112, 64)         36928     ['activation_44[0][0]']       \n","                                                                                                  \n"," batch_normalization_45 (Ba  (None, 112, 112, 64)         256       ['conv2d_58[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_45 (Activation)  (None, 112, 112, 64)         0         ['batch_normalization_45[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_60 (Conv2D)          (None, 112, 112, 128)        8320      ['add_12[0][0]']              \n","                                                                                                  \n"," conv2d_59 (Conv2D)          (None, 112, 112, 128)        8320      ['activation_45[0][0]']       \n","                                                                                                  \n"," add_13 (Add)                (None, 112, 112, 128)        0         ['conv2d_60[0][0]',           \n","                                                                     'conv2d_59[0][0]']           \n","                                                                                                  \n"," batch_normalization_46 (Ba  (None, 112, 112, 128)        512       ['add_13[0][0]']              \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_46 (Activation)  (None, 112, 112, 128)        0         ['batch_normalization_46[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_61 (Conv2D)          (None, 56, 56, 128)          16512     ['activation_46[0][0]']       \n","                                                                                                  \n"," batch_normalization_47 (Ba  (None, 56, 56, 128)          512       ['conv2d_61[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_47 (Activation)  (None, 56, 56, 128)          0         ['batch_normalization_47[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_62 (Conv2D)          (None, 56, 56, 128)          147584    ['activation_47[0][0]']       \n","                                                                                                  \n"," batch_normalization_48 (Ba  (None, 56, 56, 128)          512       ['conv2d_62[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_48 (Activation)  (None, 56, 56, 128)          0         ['batch_normalization_48[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_64 (Conv2D)          (None, 56, 56, 256)          33024     ['add_13[0][0]']              \n","                                                                                                  \n"," conv2d_63 (Conv2D)          (None, 56, 56, 256)          33024     ['activation_48[0][0]']       \n","                                                                                                  \n"," add_14 (Add)                (None, 56, 56, 256)          0         ['conv2d_64[0][0]',           \n","                                                                     'conv2d_63[0][0]']           \n","                                                                                                  \n"," batch_normalization_49 (Ba  (None, 56, 56, 256)          1024      ['add_14[0][0]']              \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_49 (Activation)  (None, 56, 56, 256)          0         ['batch_normalization_49[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," average_pooling2d_4 (Avera  (None, 7, 7, 256)            0         ['activation_49[0][0]']       \n"," gePooling2D)                                                                                     \n","                                                                                                  \n"," flatten_4 (Flatten)         (None, 12544)                0         ['average_pooling2d_4[0][0]'] \n","                                                                                                  \n"," dense_4 (Dense)             (None, 12)                   150540    ['flatten_4[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 447148 (1.71 MB)\n","Trainable params: 445388 (1.70 MB)\n","Non-trainable params: 1760 (6.88 KB)\n","__________________________________________________________________________________________________\n","ResNet11v2\n","Epoch 1/150\n","772/772 [==============================] - ETA: 0s - loss: 1.2042 - accuracy: 0.7261\n","Epoch 1: val_accuracy improved from -inf to 0.83005, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_6fruit_batch10_epoch150.h5\n","772/772 [==============================] - 2280s 3s/step - loss: 1.2042 - accuracy: 0.7261 - val_loss: 0.8246 - val_accuracy: 0.8301\n","Epoch 2/150\n","772/772 [==============================] - ETA: 0s - loss: 0.7163 - accuracy: 0.8488\n","Epoch 2: val_accuracy improved from 0.83005 to 0.86684, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_6fruit_batch10_epoch150.h5\n","772/772 [==============================] - 2235s 3s/step - loss: 0.7163 - accuracy: 0.8488 - val_loss: 0.6350 - val_accuracy: 0.8668\n","Epoch 3/150\n","772/772 [==============================] - ETA: 0s - loss: 0.5517 - accuracy: 0.8874\n","Epoch 3: val_accuracy did not improve from 0.86684\n","772/772 [==============================] - 2224s 3s/step - loss: 0.5517 - accuracy: 0.8874 - val_loss: 0.8457 - val_accuracy: 0.8228\n","Epoch 4/150\n","772/772 [==============================] - ETA: 0s - loss: 0.4814 - accuracy: 0.9061\n","Epoch 4: val_accuracy improved from 0.86684 to 0.87254, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_6fruit_batch10_epoch150.h5\n","772/772 [==============================] - 2242s 3s/step - loss: 0.4814 - accuracy: 0.9061 - val_loss: 0.6052 - val_accuracy: 0.8725\n","Epoch 5/150\n","772/772 [==============================] - ETA: 0s - loss: 0.4154 - accuracy: 0.9224\n","Epoch 5: val_accuracy did not improve from 0.87254\n","772/772 [==============================] - 2230s 3s/step - loss: 0.4154 - accuracy: 0.9224 - val_loss: 0.6284 - val_accuracy: 0.8642\n","Epoch 6/150\n","772/772 [==============================] - ETA: 0s - loss: 0.3816 - accuracy: 0.9290\n","Epoch 6: val_accuracy improved from 0.87254 to 0.90881, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_6fruit_batch10_epoch150.h5\n","772/772 [==============================] - 2211s 3s/step - loss: 0.3816 - accuracy: 0.9290 - val_loss: 0.4673 - val_accuracy: 0.9088\n","Epoch 7/150\n","772/772 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.9304\n","Epoch 7: val_accuracy improved from 0.90881 to 0.91088, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_6fruit_batch10_epoch150.h5\n","772/772 [==============================] - 2191s 3s/step - loss: 0.3527 - accuracy: 0.9304 - val_loss: 0.4641 - val_accuracy: 0.9109\n","Epoch 8/150\n","772/772 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.9399\n","Epoch 8: val_accuracy improved from 0.91088 to 0.91347, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_6fruit_batch10_epoch150.h5\n","772/772 [==============================] - 2200s 3s/step - loss: 0.3208 - accuracy: 0.9399 - val_loss: 0.4446 - val_accuracy: 0.9135\n","Epoch 9/150\n","772/772 [==============================] - ETA: 0s - loss: 0.2893 - accuracy: 0.9481\n","Epoch 9: val_accuracy improved from 0.91347 to 0.92591, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_6fruit_batch10_epoch150.h5\n","772/772 [==============================] - 2205s 3s/step - loss: 0.2893 - accuracy: 0.9481 - val_loss: 0.3751 - val_accuracy: 0.9259\n","Epoch 10/150\n","772/772 [==============================] - ETA: 0s - loss: 0.2752 - accuracy: 0.9477\n","Epoch 10: val_accuracy did not improve from 0.92591\n","772/772 [==============================] - 2186s 3s/step - loss: 0.2752 - accuracy: 0.9477 - val_loss: 0.4198 - val_accuracy: 0.9207\n","Epoch 11/150\n","772/772 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9487\n","Epoch 11: val_accuracy did not improve from 0.92591\n","772/772 [==============================] - 2190s 3s/step - loss: 0.2699 - accuracy: 0.9487 - val_loss: 0.3953 - val_accuracy: 0.9244\n","Epoch 12/150\n","772/772 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9614\n","Epoch 12: val_accuracy improved from 0.92591 to 0.93472, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_6fruit_batch10_epoch150.h5\n","772/772 [==============================] - 2210s 3s/step - loss: 0.2297 - accuracy: 0.9614 - val_loss: 0.3761 - val_accuracy: 0.9347\n","Epoch 13/150\n","772/772 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.9627\n","Epoch 13: val_accuracy did not improve from 0.93472\n","772/772 [==============================] - 2221s 3s/step - loss: 0.2239 - accuracy: 0.9627 - val_loss: 0.3919 - val_accuracy: 0.9223\n","Epoch 14/150\n","772/772 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9611\n","Epoch 14: val_accuracy did not improve from 0.93472\n","772/772 [==============================] - 2215s 3s/step - loss: 0.2218 - accuracy: 0.9611 - val_loss: 0.4177 - val_accuracy: 0.9254\n","Epoch 15/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1895 - accuracy: 0.9689\n","Epoch 15: val_accuracy did not improve from 0.93472\n","772/772 [==============================] - 2221s 3s/step - loss: 0.1895 - accuracy: 0.9689 - val_loss: 0.4613 - val_accuracy: 0.9145\n","Epoch 16/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9694\n","Epoch 16: val_accuracy improved from 0.93472 to 0.93679, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_6fruit_batch10_epoch150.h5\n","772/772 [==============================] - 2203s 3s/step - loss: 0.1927 - accuracy: 0.9694 - val_loss: 0.3673 - val_accuracy: 0.9368\n","Epoch 17/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9681\n","Epoch 17: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 2219s 3s/step - loss: 0.1874 - accuracy: 0.9681 - val_loss: 0.4107 - val_accuracy: 0.9233\n","Epoch 18/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1764 - accuracy: 0.9705\n","Epoch 18: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 2226s 3s/step - loss: 0.1764 - accuracy: 0.9705 - val_loss: 0.5027 - val_accuracy: 0.9130\n","Epoch 19/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9737\n","Epoch 19: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 2191s 3s/step - loss: 0.1704 - accuracy: 0.9737 - val_loss: 0.4807 - val_accuracy: 0.9088\n","Epoch 20/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.9635\n","Epoch 20: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 2215s 3s/step - loss: 0.1948 - accuracy: 0.9635 - val_loss: 0.3985 - val_accuracy: 0.9326\n","Epoch 21/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1489 - accuracy: 0.9798\n","Epoch 21: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 2223s 3s/step - loss: 0.1489 - accuracy: 0.9798 - val_loss: 0.5601 - val_accuracy: 0.9073\n","Epoch 22/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9743\n","Epoch 22: val_accuracy did not improve from 0.93679\n","772/772 [==============================] - 2225s 3s/step - loss: 0.1608 - accuracy: 0.9743 - val_loss: 0.4634 - val_accuracy: 0.9269\n","Epoch 23/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9806\n","Epoch 23: val_accuracy improved from 0.93679 to 0.93834, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_6fruit_batch10_epoch150.h5\n","772/772 [==============================] - 2221s 3s/step - loss: 0.1406 - accuracy: 0.9806 - val_loss: 0.3813 - val_accuracy: 0.9383\n","Epoch 24/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9773\n","Epoch 24: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2220s 3s/step - loss: 0.1515 - accuracy: 0.9773 - val_loss: 0.8337 - val_accuracy: 0.8617\n","Epoch 25/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1476 - accuracy: 0.9747\n","Epoch 25: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2236s 3s/step - loss: 0.1476 - accuracy: 0.9747 - val_loss: 0.3656 - val_accuracy: 0.9383\n","Epoch 26/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9823\n","Epoch 26: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2219s 3s/step - loss: 0.1296 - accuracy: 0.9823 - val_loss: 0.3797 - val_accuracy: 0.9316\n","Epoch 27/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1409 - accuracy: 0.9775\n","Epoch 27: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2210s 3s/step - loss: 0.1409 - accuracy: 0.9775 - val_loss: 0.4666 - val_accuracy: 0.9192\n","Epoch 28/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9798\n","Epoch 28: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2215s 3s/step - loss: 0.1331 - accuracy: 0.9798 - val_loss: 0.4068 - val_accuracy: 0.9269\n","Epoch 29/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1220 - accuracy: 0.9826\n","Epoch 29: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2215s 3s/step - loss: 0.1220 - accuracy: 0.9826 - val_loss: 0.4124 - val_accuracy: 0.9358\n","Epoch 30/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9780\n","Epoch 30: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2207s 3s/step - loss: 0.1351 - accuracy: 0.9780 - val_loss: 0.4638 - val_accuracy: 0.9171\n","Epoch 31/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9824\n","Epoch 31: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2207s 3s/step - loss: 0.1185 - accuracy: 0.9824 - val_loss: 0.4043 - val_accuracy: 0.9290\n","Epoch 32/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.9817\n","Epoch 32: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2243s 3s/step - loss: 0.1226 - accuracy: 0.9817 - val_loss: 0.4945 - val_accuracy: 0.9192\n","Epoch 33/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9826\n","Epoch 33: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2245s 3s/step - loss: 0.1200 - accuracy: 0.9826 - val_loss: 0.4526 - val_accuracy: 0.9326\n","Epoch 34/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9860\n","Epoch 34: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2232s 3s/step - loss: 0.1091 - accuracy: 0.9860 - val_loss: 0.4072 - val_accuracy: 0.9352\n","Epoch 35/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9824\n","Epoch 35: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2216s 3s/step - loss: 0.1170 - accuracy: 0.9824 - val_loss: 0.3831 - val_accuracy: 0.9321\n","Epoch 36/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1150 - accuracy: 0.9837\n","Epoch 36: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2234s 3s/step - loss: 0.1150 - accuracy: 0.9837 - val_loss: 0.3701 - val_accuracy: 0.9342\n","Epoch 37/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9868\n","Epoch 37: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 2006s 3s/step - loss: 0.1085 - accuracy: 0.9868 - val_loss: 0.4472 - val_accuracy: 0.9301\n","Epoch 38/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9854\n","Epoch 38: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 1417s 2s/step - loss: 0.1043 - accuracy: 0.9854 - val_loss: 0.4145 - val_accuracy: 0.9290\n","Epoch 39/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9802\n","Epoch 39: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 1414s 2s/step - loss: 0.1253 - accuracy: 0.9802 - val_loss: 0.4004 - val_accuracy: 0.9311\n","Epoch 40/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9861\n","Epoch 40: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 1429s 2s/step - loss: 0.1018 - accuracy: 0.9861 - val_loss: 0.4361 - val_accuracy: 0.9269\n","Epoch 41/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9873\n","Epoch 41: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 1425s 2s/step - loss: 0.0997 - accuracy: 0.9873 - val_loss: 0.4021 - val_accuracy: 0.9337\n","Epoch 42/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9899\n","Epoch 42: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 1441s 2s/step - loss: 0.0913 - accuracy: 0.9899 - val_loss: 0.5655 - val_accuracy: 0.9109\n","Epoch 43/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9863\n","Epoch 43: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 1416s 2s/step - loss: 0.1005 - accuracy: 0.9863 - val_loss: 0.4073 - val_accuracy: 0.9352\n","Epoch 44/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9846\n","Epoch 44: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 1407s 2s/step - loss: 0.1014 - accuracy: 0.9846 - val_loss: 0.4706 - val_accuracy: 0.9342\n","Epoch 45/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9841\n","Epoch 45: val_accuracy did not improve from 0.93834\n","772/772 [==============================] - 1409s 2s/step - loss: 0.1077 - accuracy: 0.9841 - val_loss: 0.4180 - val_accuracy: 0.9378\n","Epoch 46/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9877\n","Epoch 46: val_accuracy improved from 0.93834 to 0.94663, saving model to /Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model/Resnet_model_fNet_6fruit_batch10_epoch150.h5\n","772/772 [==============================] - 1402s 2s/step - loss: 0.0930 - accuracy: 0.9877 - val_loss: 0.3706 - val_accuracy: 0.9466\n","Epoch 47/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0954 - accuracy: 0.9877\n","Epoch 47: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1399s 2s/step - loss: 0.0954 - accuracy: 0.9877 - val_loss: 0.5071 - val_accuracy: 0.9244\n","Epoch 48/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9907\n","Epoch 48: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1400s 2s/step - loss: 0.0849 - accuracy: 0.9907 - val_loss: 0.5601 - val_accuracy: 0.8984\n","Epoch 49/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9873\n","Epoch 49: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1389s 2s/step - loss: 0.0983 - accuracy: 0.9873 - val_loss: 0.4129 - val_accuracy: 0.9326\n","Epoch 50/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9896\n","Epoch 50: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1403s 2s/step - loss: 0.0887 - accuracy: 0.9896 - val_loss: 0.4694 - val_accuracy: 0.9254\n","Epoch 51/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9833\n","Epoch 51: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1384s 2s/step - loss: 0.1080 - accuracy: 0.9833 - val_loss: 0.4540 - val_accuracy: 0.9290\n","Epoch 52/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9905\n","Epoch 52: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1387s 2s/step - loss: 0.0823 - accuracy: 0.9905 - val_loss: 0.3805 - val_accuracy: 0.9399\n","Epoch 53/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9895\n","Epoch 53: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1395s 2s/step - loss: 0.0833 - accuracy: 0.9895 - val_loss: 5.8887 - val_accuracy: 0.6321\n","Epoch 54/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9852\n","Epoch 54: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1390s 2s/step - loss: 0.0971 - accuracy: 0.9852 - val_loss: 0.4376 - val_accuracy: 0.9264\n","Epoch 55/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9909\n","Epoch 55: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1388s 2s/step - loss: 0.0838 - accuracy: 0.9909 - val_loss: 0.3978 - val_accuracy: 0.9332\n","Epoch 56/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9911\n","Epoch 56: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1397s 2s/step - loss: 0.0805 - accuracy: 0.9911 - val_loss: 0.4512 - val_accuracy: 0.9311\n","Epoch 57/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9869\n","Epoch 57: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1390s 2s/step - loss: 0.0921 - accuracy: 0.9869 - val_loss: 0.4536 - val_accuracy: 0.9316\n","Epoch 58/150\n","772/772 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9861\n","Epoch 58: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1402s 2s/step - loss: 0.1004 - accuracy: 0.9861 - val_loss: 0.3899 - val_accuracy: 0.9420\n","Epoch 59/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9937\n","Epoch 59: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1510s 2s/step - loss: 0.0746 - accuracy: 0.9937 - val_loss: 0.4021 - val_accuracy: 0.9373\n","Epoch 60/150\n","772/772 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9887\n","Epoch 60: val_accuracy did not improve from 0.94663\n","772/772 [==============================] - 1300s 2s/step - loss: 0.0868 - accuracy: 0.9887 - val_loss: 0.4052 - val_accuracy: 0.9342\n","Epoch 61/150\n","763/772 [============================>.] - ETA: 16s - loss: 0.0776 - accuracy: 0.9906"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"resnet_overfitting_included.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1x-dq99w3wIjofB1NVMmbTqd-4Eh7O4OE\n","\"\"\"\n","\n","from __future__ import print_function\n","import keras\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import os\n","import time\n","from openpyxl import load_workbook\n","\n","import xlsxwriter\n","import cv2\n","from random import shuffle\n","\n","\n","\n","\n","start1= time.time()\n","\n","# Training parameters\n","batch_size = 10  # orig paper trained all networks with batch_size=128\n","epochs = 150\n","data_augmentation = False\n","num_classes = 12\n","test_img_num = 2363\n","result_excel_link = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Result/fruit_batch10_epoch150.xlsx'\n","Train_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TRAIN'\n","TEST_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TEST'\n","label_link= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL_encoded_file.xlsx'\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 2\n","n = 1\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'ResNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","##(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","\n","\n","def label_img(name):\n","    #print(name)\n","    filename1 = os.fsdecode(name)\n","    #print(filename1)\n","    s=[]\n","    s= filename1.split('_')\n","    #print(int(s[0]))\n","    #word_label = name.split('-')[0]\n","    #if word_label == 'golden_retriever': return np.array([1, 0])\n","    #elif word_label == 'shetland_sheepdog' : return np.array([0, 1])\n","    wb = load_workbook(label_link)\n","    ws = wb.active\n","    #for cell in ws.columns[1]:  #here column 9 is value is what i am testing which is Column X of my example.\n","    for x in range(2, 18454):\n","        #for y in range(1,):\n","        cell= ws.cell(row=x, column=1)\n","        #print(cell.value)\n","        if cell.value == int(s[0]) :\n","               #print ws.cell(column=12).value\n","                #a= ws.columns(row).value\n","                #b= ws.columns(col).value\n","                cell2= ws.cell(row=x, column=2)\n","\n","                label1= cell2.value\n","                #print(label1)\n","                break\n","        #else:\n","                #print('hi')\n","    return label1\n","\n","def load_training_data():\n","    train_data = []\n","    for img in os.listdir(Train_DIR):\n","        #print(img)\n","        temp2 = img.replace('.png','')\n","        label = label_img(temp2)\n","        #print(label)\n","        path = os.path.join(Train_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img2= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        #path3= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/training_441_segmented_polar_A1_A2_A4'\n","        resized_img1 = cv2.resize(img2, (224,224), interpolation = cv2.INTER_AREA)\n","        #cv2.imwrite(path3+ '/' + temp2 , resized_img1 )\n","        train_data.append([np.array(resized_img1), label])\n","\n","    shuffle(train_data)\n","    return train_data\n","\n","\n","train_data = load_training_data()\n","\n","trainImages = np.array([i[0] for i in train_data])#.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","trainLabels = np.array([i[1] for i in train_data])\n","# print(trainLabels[5])\n","#print(trainLabels)\n","trainLabels = to_categorical(trainLabels)\n","# #trainLabels = keras.utils.to_categorical(trainLabels, num_classes)\n","# print('now')\n","# print(trainLabels[5])\n","\n","\n","\n","def load_test_data():\n","    test_data = []\n","    for img in os.listdir(TEST_DIR):\n","        temp1 = img.replace('.png','')\n","        label = label_img(temp1)\n","        path = os.path.join(TEST_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img1= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        resized_img2 = cv2.resize(img1, (224,224), interpolation = cv2.INTER_AREA)\n","        #path2= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/testing folder_segmented_resized'\n","        #cv2.imwrite(path2+ '/' + temp1 , resized_img2 )\n","        test_data.append([np.array(resized_img2), label,temp1])\n","    shuffle(test_data)\n","    return test_data\n","\n","\n","test_data = load_test_data()\n","#plt.imshow(test_data[10][0], cmap = 'gist_gray')\n","\n","testImages = np.array([i[0] for i in test_data])#.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","testLabels = np.array([i[1] for i in test_data])\n","test_img_name = np.array([i[2] for i in test_data])\n","#testLabels = keras.utils.to_categorical(testLabels, num_classes)\n","\n","# Input image dimensions.\n","input_shape = trainImages.shape[1:]\n","\n","# Normalize data.\n","##trainImages = trainImages.astype('float32') / 255\n","##testImages = testImages.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","##if subtract_pixel_mean:\n","    ##trainImages_mean = np.mean(trainImages, axis=0)\n","    ##trainImages -= trainImages_mean\n","    ##testImages -= trainImages_mean\n","\n","#print('x_train shape:', x_train.shape)\n","#print(x_train.shape[0], 'train samples')\n","#print(x_test.shape[0], 'test samples')\n","#print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","#y_train = keras.utils.to_categorical(y_train, num_classes)\n","#y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","def resnet_layer(inputs,\n","                 num_filters=16,\n","                 kernel_size=3,\n","                 strides=1,\n","                 activation='relu',\n","                 batch_normalization=True,\n","                 conv_first=True):\n","    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n","\n","    # Arguments\n","        inputs (tensor): input tensor from input image or previous layer\n","        num_filters (int): Conv2D number of filters\n","        kernel_size (int): Conv2D square kernel dimensions\n","        strides (int): Conv2D square stride dimensions\n","        activation (string): activation name\n","        batch_normalization (bool): whether to include batch normalization\n","        conv_first (bool): conv-bn-activation (True) or\n","            bn-activation-conv (False)\n","\n","    # Returns\n","        x (tensor): tensor as input to the next layer\n","    \"\"\"\n","    conv = Conv2D(num_filters,\n","                  kernel_size=kernel_size,\n","                  strides=strides,\n","                  padding='same',\n","                  kernel_initializer='he_normal',\n","                  kernel_regularizer=l2(1e-4))\n","\n","    x = inputs\n","    if conv_first:\n","        x = conv(x)\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","    else:\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","        x = conv(x)\n","    return x\n","\n","\n","\n","\n","\n","def resnet_v2(input_shape, depth, num_classes):\n","    \"\"\"ResNet Version 2 Model builder [b]\n","\n","    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n","    bottleneck layer\n","    First shortcut connection per layer is 1 x 1 Conv2D.\n","    Second and onwards shortcut connection is identity.\n","    At the beginning of each stage, the feature map size is halved (downsampled)\n","    by a convolutional layer with strides=2, while the number of filter maps is\n","    doubled. Within each stage, the layers have the same number filters and the\n","    same filter map sizes.\n","    Features maps sizes:\n","    conv1  : 32x32,  16\n","    stage 0: 32x32,  64\n","    stage 1: 16x16, 128\n","    stage 2:  8x8,  256\n","\n","    # Arguments\n","        input_shape (tensor): shape of input image tensor\n","        depth (int): number of core convolutional layers\n","        num_classes (int): number of classes (CIFAR10 has 10)\n","\n","    # Returns\n","        model (Model): Keras model instance\n","    \"\"\"\n","    if (depth - 2) % 9 != 0:\n","        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n","    # Start model definition.\n","    num_filters_in = 16\n","    num_res_blocks = int((depth - 2) / 9)\n","\n","    inputs = Input(shape=input_shape)\n","    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n","    x = resnet_layer(inputs=inputs,\n","                     num_filters=num_filters_in,\n","                     conv_first=True)\n","\n","    # Instantiate the stack of residual units\n","    for stage in range(3):\n","        for res_block in range(num_res_blocks):\n","            activation = 'relu'\n","            batch_normalization = True\n","            strides = 1\n","            if stage == 0:\n","                num_filters_out = num_filters_in * 4\n","                if res_block == 0:  # first layer and first stage\n","                    activation = None\n","                    batch_normalization = False\n","            else:\n","                num_filters_out = num_filters_in * 2\n","                if res_block == 0:  # first layer but not first stage\n","                    strides = 2    # downsample\n","\n","            # bottleneck residual unit\n","            y = resnet_layer(inputs=x,\n","                             num_filters=num_filters_in,\n","                             kernel_size=1,\n","                             strides=strides,\n","                             activation=activation,\n","                             batch_normalization=batch_normalization,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_in,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_out,\n","                             kernel_size=1,\n","                             conv_first=False)\n","            if res_block == 0:\n","                # linear projection residual shortcut connection to match\n","                # changed dims\n","                x = resnet_layer(inputs=x,\n","                                 num_filters=num_filters_out,\n","                                 kernel_size=1,\n","                                 strides=strides,\n","                                 activation=None,\n","                                 batch_normalization=False)\n","            x = keras.layers.add([x, y])\n","\n","        num_filters_in = num_filters_out\n","\n","    # Add classifier on top.\n","    # v2 has BN-ReLU before Pooling\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = AveragePooling2D(pool_size=8)(x)\n","    y = Flatten()(x)\n","    outputs = Dense(num_classes,\n","                    activation='softmax',\n","                    kernel_initializer='he_normal')(y)\n","\n","    # Instantiate model.\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","model = resnet_v2(input_shape=input_shape, depth=depth, num_classes= num_classes)\n","# if version == 2:\n","#     model = resnet_v2(input_shape=input_shape, depth=depth, num_classes= num_classes)\n","# else:\n","#     model = resnet_v1(input_shape=input_shape, depth=depth)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(learning_rate=lr_schedule(0)),\n","              metrics=['accuracy'])\n","model.summary()\n","print(model_type)\n","\n","# Prepare model model saving directory.\n","save_dir = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model'\n","model_name = 'Resnet_model_fNet_6fruit_batch10_epoch150.h5'\n","saved_model= save_dir + '/' + model_name\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n","mc = ModelCheckpoint(saved_model , monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","\n","\n","\n","model.fit(trainImages, trainLabels, batch_size= batch_size, epochs = epochs, verbose=1, validation_split=0.2, shuffle=True, callbacks=[es, mc])\n","\n","\n","saved_model_final = load_model(saved_model)\n","\n","# Score trained model.\n","preds = saved_model_final.predict(testImages)\n","print(preds)\n","\n","end1= time.time()\n","time_taken= end1- start1\n","\n","workbook = xlsxwriter.Workbook(result_excel_link)\n","worksheet = workbook.add_worksheet()\n","\n","for i in range(0,test_img_num):\n","        #wb1 = load_workbook('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        #ws1 = wb1.active\n","        #a = ws1.cell(row= i+1, column=1)\n","        #a.value = testLabels[i]\n","        #b = ws1.cell(row= i+1, column=2)\n","        #b.value = np.argmax(preds [i])\n","        #ws1.save('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        name = test_img_name[i]\n","        worksheet.write(i+1, 0 , name)\n","        a = testLabels[i]\n","        worksheet.write(i+1, 1 , a)\n","        b = np.argmax(preds[i])\n","        worksheet.write(i+1, 2 , b)\n","        if a==b:\n","             worksheet.write(i+1, 3 , 1)\n","        else:\n","             worksheet.write(i+1, 3 , 0)\n","\n","# worksheet.write(1, 4 , time_taken)\n","\n","workbook.close()"]},{"cell_type":"markdown","metadata":{},"source":["Epoch : 200"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"resnet_overfitting_included.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1x-dq99w3wIjofB1NVMmbTqd-4Eh7O4OE\n","\"\"\"\n","\n","from __future__ import print_function\n","import keras\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import os\n","import time\n","from openpyxl import load_workbook\n","\n","import xlsxwriter\n","import cv2\n","from random import shuffle\n","\n","\n","\n","\n","start1= time.time()\n","\n","# Training parameters\n","batch_size = 10  # orig paper trained all networks with batch_size=128\n","epochs = 200\n","data_augmentation = False\n","num_classes = 12\n","test_img_num = 2083\n","result_excel_link = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Result/fruit_batch10_epoch200.xlsx'\n","Train_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TRAIN'\n","TEST_DIR= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL DATA_Fruit/TEST'\n","label_link= '/Users/sayantansikdar/Developer/FRUITS_MASTER/FINAL_encoded_file.xlsx'\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 2\n","n = 1\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'ResNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","##(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","\n","\n","def label_img(name):\n","    #print(name)\n","    filename1 = os.fsdecode(name)\n","    #print(filename1)\n","    s=[]\n","    s= filename1.split('_')\n","    #print(int(s[0]))\n","    #word_label = name.split('-')[0]\n","    #if word_label == 'golden_retriever': return np.array([1, 0])\n","    #elif word_label == 'shetland_sheepdog' : return np.array([0, 1])\n","    wb = load_workbook(label_link)\n","    ws = wb.active\n","    #for cell in ws.columns[1]:  #here column 9 is value is what i am testing which is Column X of my example.\n","    for x in range(2, 18454):\n","        #for y in range(1,):\n","        cell= ws.cell(row=x, column=1)\n","        #print(cell.value)\n","        if cell.value == int(s[0]) :\n","               #print ws.cell(column=12).value\n","                #a= ws.columns(row).value\n","                #b= ws.columns(col).value\n","                cell2= ws.cell(row=x, column=2)\n","\n","                label1= cell2.value\n","                #print(label1)\n","                break\n","        #else:\n","                #print('hi')\n","    return label1\n","\n","def load_training_data():\n","    train_data = []\n","    for img in os.listdir(Train_DIR):\n","        #print(img)\n","        temp2 = img.replace('.png','')\n","        label = label_img(temp2)\n","        #print(label)\n","        path = os.path.join(Train_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img2= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        #path3= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/training_441_segmented_polar_A1_A2_A4'\n","        resized_img1 = cv2.resize(img2, (224,224), interpolation = cv2.INTER_AREA)\n","        #cv2.imwrite(path3+ '/' + temp2 , resized_img1 )\n","        train_data.append([np.array(resized_img1), label])\n","\n","    shuffle(train_data)\n","    return train_data\n","\n","\n","train_data = load_training_data()\n","\n","trainImages = np.array([i[0] for i in train_data])#.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","trainLabels = np.array([i[1] for i in train_data])\n","# print(trainLabels[5])\n","#print(trainLabels)\n","trainLabels = to_categorical(trainLabels)\n","# #trainLabels = keras.utils.to_categorical(trainLabels, num_classes)\n","# print('now')\n","# print(trainLabels[5])\n","\n","\n","\n","def load_test_data():\n","    test_data = []\n","    for img in os.listdir(TEST_DIR):\n","        temp1 = img.replace('.png','')\n","        label = label_img(temp1)\n","        path = os.path.join(TEST_DIR, img)\n","        #if \"DS_Store\" not in path:\n","        #img = Image.open(path)\n","        img1= cv2.imread(path)\n","        #img = img.convert('L')\n","        #img = img.resize((IMG_SIZE, IMG_SIZE, 3), Image.ANTIALIAS)\n","        resized_img2 = cv2.resize(img1, (224,224), interpolation = cv2.INTER_AREA)\n","        #path2= 'D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/testing folder_segmented_resized'\n","        #cv2.imwrite(path2+ '/' + temp1 , resized_img2 )\n","        test_data.append([np.array(resized_img2), label,temp1])\n","    shuffle(test_data)\n","    return test_data\n","\n","\n","test_data = load_test_data()\n","#plt.imshow(test_data[10][0], cmap = 'gist_gray')\n","\n","testImages = np.array([i[0] for i in test_data])#.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","testLabels = np.array([i[1] for i in test_data])\n","test_img_name = np.array([i[2] for i in test_data])\n","#testLabels = keras.utils.to_categorical(testLabels, num_classes)\n","\n","# Input image dimensions.\n","input_shape = trainImages.shape[1:]\n","\n","# Normalize data.\n","##trainImages = trainImages.astype('float32') / 255\n","##testImages = testImages.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","##if subtract_pixel_mean:\n","    ##trainImages_mean = np.mean(trainImages, axis=0)\n","    ##trainImages -= trainImages_mean\n","    ##testImages -= trainImages_mean\n","\n","#print('x_train shape:', x_train.shape)\n","#print(x_train.shape[0], 'train samples')\n","#print(x_test.shape[0], 'test samples')\n","#print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","#y_train = keras.utils.to_categorical(y_train, num_classes)\n","#y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","def resnet_layer(inputs,\n","                 num_filters=16,\n","                 kernel_size=3,\n","                 strides=1,\n","                 activation='relu',\n","                 batch_normalization=True,\n","                 conv_first=True):\n","    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n","\n","    # Arguments\n","        inputs (tensor): input tensor from input image or previous layer\n","        num_filters (int): Conv2D number of filters\n","        kernel_size (int): Conv2D square kernel dimensions\n","        strides (int): Conv2D square stride dimensions\n","        activation (string): activation name\n","        batch_normalization (bool): whether to include batch normalization\n","        conv_first (bool): conv-bn-activation (True) or\n","            bn-activation-conv (False)\n","\n","    # Returns\n","        x (tensor): tensor as input to the next layer\n","    \"\"\"\n","    conv = Conv2D(num_filters,\n","                  kernel_size=kernel_size,\n","                  strides=strides,\n","                  padding='same',\n","                  kernel_initializer='he_normal',\n","                  kernel_regularizer=l2(1e-4))\n","\n","    x = inputs\n","    if conv_first:\n","        x = conv(x)\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","    else:\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","        x = conv(x)\n","    return x\n","\n","\n","\n","\n","\n","def resnet_v2(input_shape, depth, num_classes):\n","    \"\"\"ResNet Version 2 Model builder [b]\n","\n","    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n","    bottleneck layer\n","    First shortcut connection per layer is 1 x 1 Conv2D.\n","    Second and onwards shortcut connection is identity.\n","    At the beginning of each stage, the feature map size is halved (downsampled)\n","    by a convolutional layer with strides=2, while the number of filter maps is\n","    doubled. Within each stage, the layers have the same number filters and the\n","    same filter map sizes.\n","    Features maps sizes:\n","    conv1  : 32x32,  16\n","    stage 0: 32x32,  64\n","    stage 1: 16x16, 128\n","    stage 2:  8x8,  256\n","\n","    # Arguments\n","        input_shape (tensor): shape of input image tensor\n","        depth (int): number of core convolutional layers\n","        num_classes (int): number of classes (CIFAR10 has 10)\n","\n","    # Returns\n","        model (Model): Keras model instance\n","    \"\"\"\n","    if (depth - 2) % 9 != 0:\n","        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n","    # Start model definition.\n","    num_filters_in = 16\n","    num_res_blocks = int((depth - 2) / 9)\n","\n","    inputs = Input(shape=input_shape)\n","    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n","    x = resnet_layer(inputs=inputs,\n","                     num_filters=num_filters_in,\n","                     conv_first=True)\n","\n","    # Instantiate the stack of residual units\n","    for stage in range(3):\n","        for res_block in range(num_res_blocks):\n","            activation = 'relu'\n","            batch_normalization = True\n","            strides = 1\n","            if stage == 0:\n","                num_filters_out = num_filters_in * 4\n","                if res_block == 0:  # first layer and first stage\n","                    activation = None\n","                    batch_normalization = False\n","            else:\n","                num_filters_out = num_filters_in * 2\n","                if res_block == 0:  # first layer but not first stage\n","                    strides = 2    # downsample\n","\n","            # bottleneck residual unit\n","            y = resnet_layer(inputs=x,\n","                             num_filters=num_filters_in,\n","                             kernel_size=1,\n","                             strides=strides,\n","                             activation=activation,\n","                             batch_normalization=batch_normalization,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_in,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_out,\n","                             kernel_size=1,\n","                             conv_first=False)\n","            if res_block == 0:\n","                # linear projection residual shortcut connection to match\n","                # changed dims\n","                x = resnet_layer(inputs=x,\n","                                 num_filters=num_filters_out,\n","                                 kernel_size=1,\n","                                 strides=strides,\n","                                 activation=None,\n","                                 batch_normalization=False)\n","            x = keras.layers.add([x, y])\n","\n","        num_filters_in = num_filters_out\n","\n","    # Add classifier on top.\n","    # v2 has BN-ReLU before Pooling\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = AveragePooling2D(pool_size=8)(x)\n","    y = Flatten()(x)\n","    outputs = Dense(num_classes,\n","                    activation='softmax',\n","                    kernel_initializer='he_normal')(y)\n","\n","    # Instantiate model.\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","model = resnet_v2(input_shape=input_shape, depth=depth, num_classes= num_classes)\n","# if version == 2:\n","#     model = resnet_v2(input_shape=input_shape, depth=depth, num_classes= num_classes)\n","# else:\n","#     model = resnet_v1(input_shape=input_shape, depth=depth)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(learning_rate=lr_schedule(0)),\n","              metrics=['accuracy'])\n","model.summary()\n","print(model_type)\n","\n","# Prepare model model saving directory.\n","save_dir = '/Users/sayantansikdar/Developer/FRUITS_MASTER/Resnet_Model'\n","model_name = 'Resnet_model_fNet_batch10_epoch200.h5'\n","saved_model= save_dir + '/' + model_name\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n","mc = ModelCheckpoint(saved_model , monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","\n","\n","\n","model.fit(trainImages, trainLabels, batch_size= batch_size, epochs = epochs, verbose=1, validation_split=0.2, shuffle=True, callbacks=[es, mc])\n","\n","\n","saved_model_final = load_model(saved_model)\n","\n","# Score trained model.\n","preds = saved_model_final.predict(testImages)\n","print(preds)\n","\n","end1= time.time()\n","time_taken= end1- start1\n","\n","workbook = xlsxwriter.Workbook(result_excel_link)\n","worksheet = workbook.add_worksheet()\n","\n","for i in range(0,test_img_num):\n","        #wb1 = load_workbook('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        #ws1 = wb1.active\n","        #a = ws1.cell(row= i+1, column=1)\n","        #a.value = testLabels[i]\n","        #b = ws1.cell(row= i+1, column=2)\n","        #b.value = np.argmax(preds [i])\n","        #ws1.save('D:/research phd/agriculture research/2nd phase of research/leaf phenotyping/label/results.xlsx')\n","        name = test_img_name[i]\n","        worksheet.write(i+1, 0 , name)\n","        a = testLabels[i]\n","        worksheet.write(i+1, 1 , a)\n","        b = np.argmax(preds[i])\n","        worksheet.write(i+1, 2 , b)\n","        if a==b:\n","             worksheet.write(i+1, 3 , 1)\n","        else:\n","             worksheet.write(i+1, 3 , 0)\n","\n","# worksheet.write(1, 4 , time_taken)\n","\n","workbook.close()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPRe5jCrkjAwOh+WAoBbRAL","mount_file_id":"1_y4b61UvJXmEn--Uxx2ASvtBVyDj4XR4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
